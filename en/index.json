[{"authors":["admin"],"categories":null,"content":"I was born in Beijing, and now I’m 23 years old. I obtained my bachelor’s degree in Automation from Shandong University, and I’m studying for my doctor’s degree supervised by prof. Dai @BIT.\n","date":1580809024,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1580809024,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://leidawt.github.io/en/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/authors/admin/","section":"authors","summary":"I was born in Beijing, and now I’m 23 years old. I obtained my bachelor’s degree in Automation from Shandong University, and I’m studying for my doctor’s degree supervised by prof. Dai @BIT.","tags":null,"title":"Xiaohan Wang","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://leidawt.github.io/en/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/en/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://leidawt.github.io/en/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://leidawt.github.io/en/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":["Pfrommer Bernd"],"categories":[],"content":"","date":1580815981,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580815981,"objectID":"f4b210109686229ce8507f2e0278a8f5","permalink":"https://leidawt.github.io/en/publication/paper1/","publishdate":"2020-02-04T19:33:01+08:00","relpermalink":"/en/publication/paper1/","section":"publication","summary":"TagSLAM provides a convenient, ﬂexible, and robust way of performing Simultaneous Localization and Mapping (SLAM) with AprilTag ﬁducial markers. By leveraging a few simple abstractions (bodies, tags, cameras), TagSLAM provides a front end to the GTSAM factor graph optimizer that makes it possible to rapidly design a range of experiments that are based on tags: full SLAM, extrinsic camera calibration with non-overlapping views, visual localization for ground truth, loop closure for odometry, pose estimation etc. We discuss in detail how TagSLAM initializes the factor graph in a robust way, and present loop closure as an application example. TagSLAM is a ROS based open source package and can be found at https://berndpfrommer.github.io/tagslam_web.","tags":[],"title":"TagSLAM: Robust SLAM with Fiducial Markers","type":"publication"},{"authors":["Xiaohan Wang"],"categories":["open-source"],"content":"","date":1580809024,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580809024,"objectID":"db3e6f5f4b276d58fb71feb5f336ba91","permalink":"https://leidawt.github.io/en/project/blog_transformer-tool-for-blog-migrate/","publishdate":"2020-02-04T17:37:04+08:00","relpermalink":"/en/project/blog_transformer-tool-for-blog-migrate/","section":"project","summary":"This is a python-based markdown blog migration tool","tags":["util"],"title":"blog_transformer--tool for blog migrate","type":"project"},{"authors":["Xiaohan Wang"],"categories":[],"content":"@[TOC] Hexo+github.io是当前最广为人知的个人网站搭建方法，但Hexo的主题一般只适合于写博客，想构建个包含随笔，论文，代码，教程，博客等多重内容的个人网站并不很方便。我们经常可看到学术大牛们都会有个个人网站介绍自己的论文、团队、简历、博客等等的内容，比如\r这个\r，实现这样的网站使用hexo上的各种主题配合插件自己折腾就稍显麻烦了，因此我们介绍Hugo+Academic主题+github.io构建复合型个人网站的方法。Hugo和Hexo很相似，都是静态网页生成器，Hugo基于go语言编写，速度飞快，配合异常好用的Academic主题，可方便的构建网站。\n1. 准备 首先准备Hugo环境，可参阅官方文档\rInstall Hugo\r进行安装。对于windows系统，我们只需在\rgithub release\r中下载预编译程序即可。注意Windows平台的一定要下载带有hugo_extended_xxx 的版本。下载zip中解压到喜欢的地方，如C:\\Program Files\\Hugo，然后将路径添加到环境变量即可。在命令行中执行hugo version可检查安装是否正确。 接下来下载Academic。只需在自己github中fork一下模板项目\racademic-kickstart\r，git clone到本地，进入文件夹（我们下面称为网站文件夹），执行\ngit submodule update --init --recursive  上面语句目的是将仓库所有子模块更新到最新版本。关于git子模块的知识建议查阅\rhere\r。简言之，git子模块是为一个Git仓库中添加其他Git 仓库的场景设计的机制。 上面的仓库用于存放所有网站源码，接下来我们再在github上创建一个托管网站内容文件的仓库。建立一个名字为\u0026lt;你的github用户名\u0026gt;.github.io的仓库（注：这是github page要求的形式），然后拉到本地：进入刚才的网站文件夹执行\ngit submodule add -f -b master https://github.com/\u0026lt;你的github用户名\u0026gt;/\u0026lt;你的github用户名\u0026gt;.github.io.git public  克隆到网站文件夹的public文件夹中。 完成后应得到如下结构 \r\r\r之后，我们将域名填入配置文件：打开 \u0026lt;网站文件夹\u0026gt;\\config_default\\config.toml，将baseurl配置项写入\u0026lt;你的github用户名\u0026gt;.github.io \r\r\r最后我们进行commit，分两步，一是提交整个网站文件夹用以备份，二是提交到github.io的内容仓库。 在网站文件夹执行以下内容\ngit add . git commit -m \u0026quot;Initial commit\u0026quot; git push -u origin master hugo cd public git add . git commit -m \u0026quot;Build website\u0026quot; git push origin master cd ..  hugo命令会从源码生成静态网站文件到public文件夹。第二个push会要求输入github账号和密码。 至此整个准备和部署已经完成，我们建立好了两个仓库，现在可从 https://\u0026lt;你的github用户名\u0026gt;.github.io 看到初始网页了。\n2.基本使用及配置 开启本地测试服务器 输入hugo server来启动测试服务器，@http://localhost:1313 hugo server会自动侦测源文件变动自动刷新页面，调试十分方便。 个性化配置 Academic主题的\r官方文档\r极为清晰，这里只做下文档导读 我们需要动的内容都集中在confg和content下面，Academic的配置文件采用toml，一个改进了yaml的新的文档格式，并不复杂，配置项的注释里都写明了文档链接，顺次捋一遍按自己需求修改即可。和hexo类似，Academic的内容由markdown文件表达，前面部分用 +++ 包起来的是用于指挥渲染的头信息，后面是正常的markdown内容。home文件夹下是首页各个组件的.md文件，我们可以调整各个组件.md文件中的active配置项来决定是否使用组件。 开启中英双语言 幸运的是hugo的多语言支持相当不错。首先修改配置文件，对config_default\\languages.toml修改如下：\n# Languages # Create a `[X]` block for each language you want, where X is the language ID. # Refer to https://sourcethemes.com/academic/docs/language/ # Configure the English version of the site. [en] languageCode = \u0026quot;en-us\u0026quot; contentDir = \u0026quot;content/en\u0026quot; # Uncomment for multi-lingual sites, and move English content into `en` sub-folder. # Uncomment the lines below to configure your website in a second language. [zh] languageCode = \u0026quot;zh-Hans\u0026quot; contentDir = \u0026quot;content/zh\u0026quot; title = \u0026quot;Academic\u0026quot; [zh.params] description = \u0026quot;Site description in Chinese...\u0026quot; [[zh.menu.main]] name = \u0026quot;主页\u0026quot; url = \u0026quot;#about\u0026quot; weight = 10 [[zh.menu.main]] name = \u0026quot;文章\u0026quot; url = \u0026quot;#posts\u0026quot; weight = 20 [[zh.menu.main]] name = \u0026quot;项目\u0026quot; url = \u0026quot;#projects\u0026quot; weight = 30 [[zh.menu.main]] name = \u0026quot;论文\u0026quot; url = \u0026quot;#featured\u0026quot; weight = 40 [[zh.menu.main]] name = \u0026quot;联系我\u0026quot; url = \u0026quot;#contact\u0026quot; weight = 60 [[zh.menu.main]] name = \u0026quot;教程\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 80  后面的一大堆是用来汉化主页的bar的，至于为什么Academic本身的汉化包不含这些的原因是因为Academic支持自己在首页定义新的组件，因此bar的内容不定，需要自己汉化。之后把content文件夹的所有内容移动到content\\en中，并将content\\en文件夹copy一份重命名为content\\zh \r\r\r至此我们就完成了多语言配置，可在网页右上角看到切换选项 \r\r\r开启后两种语言的内容间完全解耦，不存在任何相互关系，需各自单独维护！ Academic为不同内容提供了方便的样式模板，其中最常用的是博客（post）和论文（publication）。 新建博客（post） Academic原生完美支持$\\LaTeX$公式渲染，内建的甘特图等图表渲染也十分漂亮，如下： \r\r\r\r\r\r详情可参考\r官方文档\r新建一篇博客文章可执行：\nhugo new --kind post post/my-article-name  该命令的效果实际会建立\\content\\en\\post\\my-article-name文件夹，编辑其中的_index.md即可。hugo new命令并没有考虑多语言，要想建立其中文版，我们需要手工复制一份到\\content\\zh\\post\\里去。文章写好后我们只需执行hexo命令进行渲染，之后用git提交到远程库即可。 新建论文（publication） 新建一篇博客文章可执行：\nhugo new --kind publication publication/\u0026lt;my-publication\u0026gt;  论文类型对论文有专门的排版优化，很适合用于介绍自己的文章。官方文档介绍的python工具可通过bib文件自动生成对应的markdown，但问题很多，当前不建议使用，最好手工编写。论文的显示效果如下： 在首页： \r\r\r文章： \r\r\r除了最常用的博客和论文样式，Academic还提供了slide page等丰富的内容，详情可参阅文档。\n3. 优秀参考范例 https://skyao.io/ https://sourcethemes.com/academic/ https://hughandbecky.us/Hugh-CV/ http://cicl.stanford.edu/\n","date":1580788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580788800,"objectID":"794cf4e804dae1947674bf7060b2603d","permalink":"https://leidawt.github.io/en/post/%E5%80%9F%E5%8A%A9hugo%E5%92%8Cacademic%E4%B8%BB%E9%A2%98%E5%9C%A8github/","publishdate":"2020-02-04T12:00:00+08:00","relpermalink":"/en/post/%E5%80%9F%E5%8A%A9hugo%E5%92%8Cacademic%E4%B8%BB%E9%A2%98%E5%9C%A8github/","section":"post","summary":"@[TOC] Hexo+github.io是当前最广为人知的个人网站搭建方法，但Hexo的主题一般只适合于写博客，想构建个包含随笔，论文，代码，教程，博","tags":[],"title":"借助Hugo和Academic主题在github","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"@[TOC](python package打包制作)\n1. python package层次结构 根据navdeep-G大神提供的最佳实践（项目模板可从\r这里\r下载），一个典型python工程项目包应具有如下结构： \r\r\r其中mypackage是自己要写的包，里面放上__init__.py文件声明该文件夹构成python package，__init__.py可以是个空文件，亦可包含一些import操作，具体取决于我们希望呈现给使用者的使用形式。 当留空__init__.py文件时，我们若想使用a.py 中的a()函数，只需以\nfrom mypackage.a import a  方式引入。 而很多时候我们希望能一次引入所有模块，形如我们使用numpy包时先import numpy as np 之后直接按np.xxx() np.random.xxx() 这种方式调用想要的函数，这可通过在__init__.py文件中写入import来实现。如numpy项目在其在顶层__init__.py文件中写入了如下内容： \r\r\r同时，对应于\nfrom mypackage import *  的写法，我们可在__init__.py中写入\n__all__ = ['module1', 'module2', 'module1']  这个列表，这会指明import *的内容 综上，mypackage的__init__.py文件可如下编写：\n__all__ = ['core', 'a', 'b'] from . import core from . import a from . import b  MANIFEST.in文件是一个清单模板，用于指定要在python源代码分发中分发的其他文件。默认情况下，当实际打包python代码（使用，比方说python setup.py sdist）创建用于分发的打包是，打包程序将仅在包存档中包含一组特定文件（例如，python代码本身）。如果存储库中包含文本文件（例如，模板）或图形（用于您的文档），该怎么办？默认情况下，打包程序不会在归档中包含这些文件，故我们的打包将不够完整，MANIFEST.in 允许覆盖默认值，准确指定打包的文件以供分发。如在上面的模板项目的MANIFEST.in内容为\ninclude README.md LICENSE  表示把说明文件README.md和开源协议LICENSE文件一并打包。更多细节可参看\r这里\r。协议文件可参看\r这里\r来加入。 更进一步的，更为规范的项目还会加入单元测试部分和文档部分，文档的自动生成可见\r这里\r，对使用Sphinx进行文档生成和使用reStructuredText进行文档发布进行了详尽介绍。在构建规范的python包方面，一个十分值得学习的库是\rhowdoit\r，十分规范易懂。此外也可看一下\r最佳实践\r。 最后还有一个setup.py文件是打包的关键，下面予以详细讨论。\n2. python package打包，分发与安装 对于只由单一文件组成的纯python库，我们只需要将python文件拷贝到python安装位置中（如ubuntu一般为/usr/lib/python3.x），即可通过import来找到。 但对于更复杂的由多个文件组成的python库或是含有其他语言编写的库（主要是c,c++），则更推荐使用setuptools提供的工具链打包发布成whl等格式。.whl既是pip管理工具使用的标准库格式，我们可借助pip很方便的部署和管理这个新打包的库，分发也更为容易。 关于setuptools 和 setup.py的详细说明可查阅\rcnblogs\r和\rthis guide\r，通常我们只要套用\rsetup.py\r提供的模板即可，如下所示，我们只需要填入meta-data中包名，版本号等信息，并在REQUIRED 中写入依赖的python包即可，这里写入依赖包后，pip install 的时候便会自动检查和安装依赖\n#!/usr/bin/env python # -*- coding: utf-8 -*- # Note: To use the 'upload' functionality of this file, you must: # $ pipenv install twine --dev import io import os import sys from shutil import rmtree from setuptools import find_packages, setup, Command # Package meta-data. NAME = 'mypackage' DESCRIPTION = 'My short description for my project.' URL = 'https://github.com/me/myproject' EMAIL = 'me@example.com' AUTHOR = 'Awesome Soul' REQUIRES_PYTHON = '\u0026gt;=3.6.0' VERSION = '0.1.0' # What packages are required for this module to be executed? REQUIRED = [ # 'requests', 'maya', 'records', ] # What packages are optional? EXTRAS = { # 'fancy feature': ['django'], } # The rest you shouldn't have to touch too much :) # ------------------------------------------------ # Except, perhaps the License and Trove Classifiers! # If you do change the License, remember to change the Trove Classifier for that! here = os.path.abspath(os.path.dirname(__file__)) # Import the README and use it as the long-description. # Note: this will only work if 'README.md' is present in your MANIFEST.in file! try: with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f: long_description = '\\n' + f.read() except FileNotFoundError: long_description = DESCRIPTION # Load the package's __version__.py module as a dictionary. about = {} if not VERSION: project_slug = NAME.lower().replace(\u0026quot;-\u0026quot;, \u0026quot;_\u0026quot;).replace(\u0026quot; \u0026quot;, \u0026quot;_\u0026quot;) with open(os.path.join(here, project_slug, '__version__.py')) as f: exec(f.read(), about) else: about['__version__'] = VERSION class UploadCommand(Command): \u0026quot;\u0026quot;\u0026quot;Support setup.py upload.\u0026quot;\u0026quot;\u0026quot; description = 'Build and publish the package.' user_options = [] @staticmethod def status(s): \u0026quot;\u0026quot;\u0026quot;Prints things in bold.\u0026quot;\u0026quot;\u0026quot; print('\\033[1m{0}\\033[0m'.format(s)) def initialize_options(self): pass def finalize_options(self): pass def run(self): try: self.status('Removing previous builds…') rmtree(os.path.join(here, 'dist')) except OSError: pass self.status('Building Source and Wheel (universal) distribution…') os.system('{0} setup.py sdist bdist_wheel --universal'.format(sys.executable)) self.status('Uploading the package to PyPI via Twine…') os.system('twine upload dist/*') self.status('Pushing git tags…') os.system('git tag v{0}'.format(about['__version__'])) os.system('git push --tags') sys.exit() # Where the magic happens: setup( name=NAME, version=about['__version__'], description=DESCRIPTION, long_description=long_description, long_description_content_type='text/markdown', author=AUTHOR, author_email=EMAIL, python_requires=REQUIRES_PYTHON, url=URL, packages=find_packages(exclude=[\u0026quot;tests\u0026quot;, \u0026quot;*.tests\u0026quot;, \u0026quot;*.tests.*\u0026quot;, \u0026quot;tests.*\u0026quot;]), # If your package is a single module, use this instead of 'packages': # py_modules=['mypackage'], # entry_points={ # 'console_scripts': ['mycli=mymodule:cli'], # }, install_requires=REQUIRED, extras_require=EXTRAS, include_package_data=True, license='MIT', classifiers=[ # Trove classifiers # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers 'License :: OSI Approved :: MIT License', 'Programming Language :: Python', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.6', 'Programming Language :: Python :: Implementation :: CPython', 'Programming Language :: Python :: Implementation :: PyPy' ], # $ setup.py publish support. cmdclass={ 'upload': UploadCommand, }, )  若想推送自己的包到PyPI，则需先注册PyPI账号然后执行python3 setup.py upload来上传。 写好后执行如下命令编译打包为.whl\npython3 setup.py bdist_wheel  也可打包为.tar.gz，均可直接由pip工具安装\npython3 setup.py sdist  执行setup.py后的目录结构如下： \r\r\r安装：\npip3 install ./dist/foo-1.0-py3-none-any.whl #安装包  注意，在ubuntu系统中以远程登录方式非root方式执行install时会默认启用\u0026ndash;user选项，即会把包安装到用户的~/.local/lib/python3.6/site-packages目录下 \r\r\r而若使用\nsudo pip3 install ./dist/foo-1.0-py3-none-any.whl  则会安装到/usr/local/lib/python3.6/dist-packages中去，使用 pip3 show \u0026lt;packange-name\u0026gt; 可以看到安装信息\n","date":1580529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580529600,"objectID":"3b61b1f649b0bcf21ef97261f0e4a61f","permalink":"https://leidawt.github.io/en/post/python-package%E6%89%93%E5%8C%85%E5%88%B6%E4%BD%9C/","publishdate":"2020-02-01T12:00:00+08:00","relpermalink":"/en/post/python-package%E6%89%93%E5%8C%85%E5%88%B6%E4%BD%9C/","section":"post","summary":"@[TOC](python package打包制作) 1. python package层次结构 根据navdeep-G大神提供的最佳实践（项目模板可从 这里 下载），一个典型python工","tags":[],"title":"python package打包制作","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"自matlab2016a版本以来，matlab多了创建实时脚本的功能。在未来版本中作为mupad的替代。其思想与mathcad相似，企图将文档与程序合二为一。就是在原有m文件上加了交互式图标，富文本功能和控件。格式为mlx。2016a以上版本都可打开，还可以输出为pdf等用于分享。 \r\r\r创建实时脚本很简单，help里有详细记述，这里记录下实时脚本的功能和技巧。 首先是交互式输入公式的能力，如下，有强大的公式录入功能，还支持latex \r\r\r\r\r\r其次，绘图结果可以进行交互式操作，并生成对应操作的代码。如下： \r\r\r还可以插入箭头，加网格，图例等，比用代码写要更方便快捷，只需之后操作生成的代码更新到code即可 \r\r\r此外一个十分重要特性是对公式结果的展示，比以前的pretty更加优秀的是，其可进行手写化展示。以后利用matlab进行符号推导更加舒服了。 \r\r\r自2018a开始，又增加了控件功能，可嵌入代码之中，通过拖动和下拉菜单选项进行取值更改，之后会自动刷新和执行一遍程序。 \r\r\r","date":1569556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569556800,"objectID":"cbdb13bbcc5ecb887e30ad3e92ebfc80","permalink":"https://leidawt.github.io/en/post/matlab%E5%AE%9E%E6%97%B6%E8%84%9A%E6%9C%AC/","publishdate":"2019-09-27T12:00:00+08:00","relpermalink":"/en/post/matlab%E5%AE%9E%E6%97%B6%E8%84%9A%E6%9C%AC/","section":"post","summary":"自matlab2016a版本以来，matlab多了创建实时脚本的功能。在未来版本中作为mupad的替代。其思想与mathcad相似，企图将文","tags":[],"title":"MATLAB实时脚本","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"sutton RL an introduction 2nd CH5例子 ref: https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/edit/master/chapter05/blackjack.py 本文解读整理上述示例代码\n规则 21点（blackjack）是经典赌场游戏，玩的是在牌面和不超过21点的情况下尽可能大。牌面规定：Ace可以是1 或 11， J,Q,K均为10，无大小王。具体规则有很多种，书中规定如下：\n The object of the popular casino card game of blackjack is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player. One of the dealer’s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a natural. He then wins unless the dealer also has a natural, in which case the game is a draw. If the player does not have a natural, then he can request additional cards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust ). If he goes bust, he loses; if he sticks, then it becomes the dealer’s turn. The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome—win, lose, or draw—is determined by whose final sum is closer to 21.\n 首先是21点游戏逻辑的定义和一些预设policy。 作为玩家，只有hit 和 stand两种action 其进行决策只需考虑如下三点：\n usable_ace手头是否有ace牌，且能叫为11点而不爆牌 手头牌面值和（12-21）0-11不需考虑，因为无论抽到什么牌怎么都不可能爆牌，故一定是hit 庄家的明牌。(1,\u0026hellip;10)（J Q K都是10） 所以共有2*10*10 200个state，故policy表和value表就是2*10*10的  import numpy as np import matplotlib matplotlib.use('Agg') import matplotlib.pyplot as plt import seaborn as sns from tqdm import tqdm # actions: hit or stand ACTION_HIT = 0 ACTION_STAND = 1 # \u0026quot;strike\u0026quot; in the book ACTIONS = [ACTION_HIT, ACTION_STAND] # policy for player # 这是书中规定的一个policy，用于演示MC policy evaluation， # 其很朴素，只在自己牌面和为 20 or 21 时stand，其余一律hit，不考虑其他因素 POLICY_PLAYER = np.zeros(22, dtype=np.int) for i in range(12, 20): POLICY_PLAYER[i] = ACTION_HIT POLICY_PLAYER[20] = ACTION_STAND POLICY_PLAYER[21] = ACTION_STAND ###########这俩policy是为off-policy算法准备 # function form of target policy of player def target_policy_player(usable_ace_player, player_sum, dealer_card): return POLICY_PLAYER[player_sum] # function form of behavior policy of player def behavior_policy_player(usable_ace_player, player_sum, dealer_card): if np.random.binomial(1, 0.5) == 1: return ACTION_STAND return ACTION_HIT ########### # policy for dealer # 21点游戏规则规定的庄家policy，即持续hit直至\u0026gt;=17 POLICY_DEALER = np.zeros(22) for i in range(12, 17): POLICY_DEALER[i] = ACTION_HIT for i in range(17, 22): POLICY_DEALER[i] = ACTION_STAND # get a new card def get_card(): card = np.random.randint(1, 14)# [1,14) card = min(card, 10)#（J Q K都是10） return card # get the value of a card (11 for ace). def card_value(card_id): return 11 if card_id == 1 else card_id # play a game 环境交互核心函数，返回： #（state, reward, player_trajectory） # 其中state = [usable_ace_player, player_sum, dealer_card1] # reward 是+1 或-1或0 # player_trajectory [(usable_ace_player, player_sum, dealer_card1), action])的 # 序列 # @policy_player: specify policy for player # @initial_state: [whether player has a usable Ace, sum of player's cards, one card of dealer] # @initial_action: the initial action def play(policy_player, initial_state=None, initial_action=None): # player status # sum of player player_sum = 0 # trajectory of player player_trajectory = [] # whether player uses Ace as 11 usable_ace_player = False # dealer status dealer_card1 = 0 dealer_card2 = 0 usable_ace_dealer = False if initial_state is None: # generate a random initial state while player_sum \u0026lt; 12: # if sum of player is less than 12, always hit card = get_card() player_sum += card_value(card) # If the player's sum is larger than 21, he may hold one or two aces. if player_sum \u0026gt; 21: assert player_sum == 22 # last card must be ace player_sum -= 10 else: usable_ace_player |= (1 == card) # initialize cards of dealer, suppose dealer will show the first card he gets dealer_card1 = get_card() dealer_card2 = get_card() else: # use specified initial state usable_ace_player, player_sum, dealer_card1 = initial_state dealer_card2 = get_card() # initial state of the game state = [usable_ace_player, player_sum, dealer_card1] # initialize dealer's sum dealer_sum = card_value(dealer_card1) + card_value(dealer_card2) usable_ace_dealer = 1 in (dealer_card1, dealer_card2) # if the dealer's sum is larger than 21, he must hold two aces. if dealer_sum \u0026gt; 21: assert dealer_sum == 22 # use one Ace as 1 rather than 11 dealer_sum -= 10 assert dealer_sum \u0026lt;= 21 assert player_sum \u0026lt;= 21 # game starts! # player's turn while True: if initial_action is not None: action = initial_action initial_action = None else: # get action based on current sum action = policy_player(usable_ace_player, player_sum, dealer_card1) # track player's trajectory for importance sampling player_trajectory.append([(usable_ace_player, player_sum, dealer_card1), action]) if action == ACTION_STAND: break # if hit, get new card card = get_card() # Keep track of the ace count. the usable_ace_player flag is insufficient alone as it cannot # distinguish between having one ace or two. ace_count = int(usable_ace_player) if card == 1: ace_count += 1 player_sum += card_value(card) # If the player has a usable ace, use it as 1 to avoid busting and continue. while player_sum \u0026gt; 21 and ace_count: player_sum -= 10 ace_count -= 1 # player busts if player_sum \u0026gt; 21: return state, -1, player_trajectory assert player_sum \u0026lt;= 21 usable_ace_player = (ace_count == 1) # dealer's turn while True: # get action based on current sum action = POLICY_DEALER[dealer_sum] if action == ACTION_STAND: break # if hit, get a new card new_card = get_card() ace_count = int(usable_ace_dealer) if new_card == 1: ace_count += 1 dealer_sum += card_value(new_card) # If the dealer has a usable ace, use it as 1 to avoid busting and continue. while dealer_sum \u0026gt; 21 and ace_count: dealer_sum -= 10 ace_count -= 1 # dealer busts if dealer_sum \u0026gt; 21: return state, 1, player_trajectory usable_ace_dealer = (ace_count == 1) # compare the sum between player and dealer assert player_sum \u0026lt;= 21 and dealer_sum \u0026lt;= 21 if player_sum \u0026gt; dealer_sum: return state, 1, player_trajectory elif player_sum == dealer_sum: return state, 0, player_trajectory else: return state, -1, player_trajectory  蒙特卡洛 policy 评估 \r\r\r\r该方法是对无完整过程模型p，无法使用DP，而仅从交互序列中进行值函数v(s)估计的方法。可分为first-visit 和 every-visit两种，其区别在于first-visit仅处理每一个交互序列中某state的第一次出现，而every-visit对每一个交互序列中某state的每次出现一视同仁。 具体做法可由代码进行理解：\n# Monte Carlo Sample with On-Policy def monte_carlo_on_policy(episodes): states_usable_ace = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided states_usable_ace_count = np.ones((10, 10)) states_no_usable_ace = np.zeros((10, 10)) # initialze counts to 1 to avoid 0 being divided states_no_usable_ace_count = np.ones((10, 10)) for i in tqdm(range(0, episodes)): _, reward, player_trajectory = play(target_policy_player) for (usable_ace, player_sum, dealer_card), _ in player_trajectory: player_sum -= 12 # 因为value表的第一个index是12 dealer_card -= 1 # 同样是index问题 if usable_ace: states_usable_ace_count[player_sum, dealer_card] += 1 states_usable_ace[player_sum, dealer_card] += reward else: states_no_usable_ace_count[player_sum, dealer_card] += 1 states_no_usable_ace[player_sum, dealer_card] += reward return states_usable_ace / states_usable_ace_count, states_no_usable_ace / states_no_usable_ace_count  该函数对前面定义的简单policy进行v(s)评估。将200个state按有无ace分两类分别返回（100+100）。此函数使用every-visit，实践中绝大部分都是every-visit，因为实现更方便，不用验证是不是first-visit。拿到一个交互序列后，遍历其中每一step，将交互序列的reward对应加到各个state上。整个过程重复episodes次，最后对value表用state count求平均。 按此方法即可在未知过程模型的情况下，仅用policy 与 环境的交互结果对policy对应的v(s)进行估计。\nMonte Carlo policy iteration with Exploring Starts 上部分主要目的是阐述Monte Carlo 估计RL问题的基本方法。我们要想将Monte Carlo应用到DP中的 policy iteration算法上求optimal policy，需要估计state_action_values 即q(a,s)而非v(s)，这是因为没有过程模型p时，仅有v(s)是无法求optimal action的。（DP中可以）。下面的算法采用greedy policy，并用Exploring Starts弥补探索的缺失。所谓Exploring Starts就是随机选取交互的init，这样在当进行的episodes足够多的时候，就可以保证每个state都被探索到了。显然Exploring Starts在很多实际问题中并不现实，因为init态很多时候是定死的，导致Exploring Starts无法进行，之后会讨论其他保证探索的方法。\n# Monte Carlo policy iteration with Exploring Starts def monte_carlo_es(episodes): # (playerSum, dealerCard, usableAce, action) state_action_values = np.zeros((10, 10, 2, 2)) # initialze counts to 1 to avoid division by 0 state_action_pair_count = np.ones((10, 10, 2, 2)) # behavior policy is greedy # 如遇到value的action则随机选取 def behavior_policy(usable_ace, player_sum, dealer_card): usable_ace = int(usable_ace) player_sum -= 12 dealer_card -= 1 # get argmax of the average returns(s, a) values_ = state_action_values[player_sum, dealer_card, usable_ace, :] / \\ state_action_pair_count[player_sum, dealer_card, usable_ace, :] return np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)]) # play for several episodes for episode in tqdm(range(episodes)): # for each episode, use a randomly initialized state and action # 即Exploring Starts # x下面更新q(s,a)表 initial_state = [bool(np.random.choice([0, 1])), np.random.choice(range(12, 22)), np.random.choice(range(1, 11))] initial_action = np.random.choice(ACTIONS) current_policy = behavior_policy if episode else target_policy_player _, reward, trajectory = play(current_policy, initial_state, initial_action) for (usable_ace, player_sum, dealer_card), action in trajectory: usable_ace = int(usable_ace) player_sum -= 12 dealer_card -= 1 # update values of state-action pairs state_action_values[player_sum, dealer_card, usable_ace, action] += reward state_action_pair_count[player_sum, dealer_card, usable_ace, action] += 1 return state_action_values / state_action_pair_count  On-Policy 和 Off-Policy 为了能不用Exploring Starts并保证探索，可以采用Chapter2中的e-greedy策略，但这种方法最后收敛得到的value* π*是对e-greedy这个policy的，只是近似最优，为了解决此问题，又发展了一类方法叫off-policy。其使用两个policy，一个target_policy作为目标policy作为最终的policy（一般就是按value greedy的），另一个带探索的behavior_policy用于产生交互序列。off-policy名称的含义即指这种用其他policy的交互数据对本policy进行提升和评估的一类方法。与之相对的on-policy就是形如上面Monte Carlo policy iteration with Exploring Starts的一类方法，仅有一个policy自给自足。 off-policy为了能利用behavior_policy的交互数据为target_policy进行action_value表估计，需要引入importance sampling来进行补偿一面产生biased的估计。 某个policy交互出某一action state序列的概率可表示为： \r\r\r忽略环境p的部分，可推知其importance-sampling-ratio应该是 \r\r\rimportance-sampling进一步分为ordinary importance sampling\r\r\r\r和 weighted importance sampling \r\r\r实践上weighted importance sampling更常用。下面的函数同时计算了两类方法。注意importance-sampling-ratio的计算，一旦序列中action出现不同，就会给0，终止计算。（因为错误的action后，p(s|s,a)就会产生出0，既拐不回去了。。）\n# Monte Carlo Sample with Off-Policy def monte_carlo_off_policy(episodes): initial_state = [True, 13, 2] rhos = [] returns = [] for i in range(0, episodes): _, reward, player_trajectory = play(behavior_policy_player, initial_state=initial_state) # get the importance ratio numerator = 1.0 denominator = 1.0 for (usable_ace, player_sum, dealer_card), action in player_trajectory: if action == target_policy_player(usable_ace, player_sum, dealer_card): denominator *= 0.5 else: numerator = 0.0 break rho = numerator / denominator rhos.append(rho) returns.append(reward) rhos = np.asarray(rhos) returns = np.asarray(returns) weighted_returns = rhos * returns # 这里算accumulate是方便后面的误差统计，无关紧要 weighted_returns = np.add.accumulate(weighted_returns) rhos = np.add.accumulate(rhos) ordinary_sampling = weighted_returns / np.arange(1, episodes + 1) with np.errstate(divide='ignore', invalid='ignore'): weighted_sampling = np.where(rhos != 0, weighted_returns / rhos, 0) return ordinary_sampling, weighted_sampling  ","date":1563508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563508800,"objectID":"949adc54fe88ac8c244d83550ee25170","permalink":"https://leidawt.github.io/en/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%8E%A921%E7%82%B9/","publishdate":"2019-07-19T12:00:00+08:00","relpermalink":"/en/post/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%8E%A921%E7%82%B9/","section":"post","summary":"sutton RL an introduction 2nd CH5例子 ref: https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/edit/master/chapter05/blackjack.py 本文解读整理上述示例代码 规则 21点（blackjack）是经典赌场游戏，玩的是在牌面和不超过21点的情况下尽可能大。","tags":[],"title":"强化学习玩21点","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"本文总结了强化学习中的经典Policy Iteration方法，在一个租车问题背景之下使用python实现，踩了一下python多进程的坑。。 主要仿写： https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/blob/master/chapter04/car_rental_synchronous.py\n背景问题描述 假设租车公司有两个场地 A,B 最大车辆数20，借出车辆收益10，在两地之间调运车辆收益-2，每天最多移动5辆。假设A,B两地的借还数服从参数分别为3 3 4 2 的泊松分布。问题的目标就是找到最佳的调运方案使得总收益最大。\nPolicy Iteration Policy Iteration是交替使用policy evaluation和policy improvement的方法\n首先先定义分布函数，由于调用量大，以lru_cache进行加速\nimport numpy as np import matplotlib.pyplot as plt from scipy.stats import poisson import functools import multiprocessing as mp import itertools import time @functools.lru_cache() def poi(n, lam): return poisson.pmf(n, lam)  定义问题的常量，其中TRUNCATE = 9表明只考虑泊松分布的截断，以便遍历借还情况\n############# PROBLEM SPECIFIC CONSTANTS ####################### MAX_CARS = 20 MAX_MOVE = 5 MOVE_COST = -2 RENT_REWARD = 10 # expectation for rental requests in first location RENTAL_REQUEST_FIRST_LOC = 3 # expectation for rental requests in second location RENTAL_REQUEST_SECOND_LOC = 4 # expectation for # of cars returned in first location RETURNS_FIRST_LOC = 3 # expectation for # of cars returned in second location RETURNS_SECOND_LOC = 2 # 限制泊松分布最大取值，否则会有无限多种state TRUNCATE = 9 # bellman方程的GAMMA GAMMA = 0.9 # 并行进程数 MP_PROCESS_NUM = 8 ################################################################  定义问题的贝尔曼方程，根据给定的参数，遍历借还情况，给出expected_return。这个函数的计算量很大，有O(N^4)\ndef bellman(values, state, action): expected_return = 0 # 先减掉移动车辆的开销 因为是固定的 expected_return += MOVE_COST*abs(action) for req1, req2 in itertools.product(range(TRUNCATE), range(TRUNCATE)): # 遍历两个地区每一组可能的借车请求 # 按action在两地间移动车辆 # 确保够移在policy_improvement 中实现 # 确保不超过两地最多车数限制，多了认为是移动到了别的场地 num_of_cars_loc1 = int(min(state[0]-action, MAX_CARS)) num_of_cars_loc2 = int(min(state[1]+action, MAX_CARS)) # 实际借车数量 real_rent_loc1 = min(num_of_cars_loc1, req1) real_rent_loc2 = min(num_of_cars_loc2, req2) num_of_cars_loc1 -= real_rent_loc1 num_of_cars_loc2 -= real_rent_loc2 # 借出受益 reward = (real_rent_loc1+real_rent_loc2)*RENT_REWARD # 本state的可能性 prob = poi(req1, RENTAL_REQUEST_FIRST_LOC) * \\ poi(req2, RENTAL_REQUEST_SECOND_LOC) # 还车 for ret1, ret2 in itertools.product(range(TRUNCATE), range(TRUNCATE)): # 按照题目意思，多还不考虑 num_of_cars_loc1_ = int(min(num_of_cars_loc1+ret1, MAX_CARS)) num_of_cars_loc2_ = int(min(num_of_cars_loc2+ret2, MAX_CARS)) prob_ = poi(ret1, RETURNS_FIRST_LOC) * \\ poi(ret2, RETURNS_SECOND_LOC)*prob # 计算经典贝尔曼方程，其中prob_就是p(s',r|a,s)其中s'对应 # (num_of_cars_loc1_,num_of_cars_loc2_) expected_return += prob_ * \\ (reward+GAMMA*values[num_of_cars_loc1_, num_of_cars_loc2_]) return expected_return  之后policy evaluation 就很好实现了 采用mutiprocessing来对每个state并行加快速度 注意policy_evaluation_helper需要是global函数，不能在policy_evaluation中定义。另需注意states迭代器每次需要重新做一下，mp.Pool不会自动重置迭代器。\ndef policy_evaluation_helper(state, values, policy): action = policy[state[0], state[1]] expected_return = bellman(values, state, action) return expected_return, state def policy_evaluation(values, policy): # 并行的遍历更新values表,返回新values表 # 此辅助函数返回给定state的expected_return while True: k = np.arange(MAX_CARS + 1) states = ((i, j) for i, j in itertools.product(k, k)) new_values = np.copy(values) # 用于比对以判断退出迭代 results = [] with mp.Pool(processes=MP_PROCESS_NUM) as p: f = functools.partial(policy_evaluation_helper, values=values, policy=policy) results = p.map(f, states) for v, (i, j) in results: new_values[i, j] = v diff = np.max(np.abs(values-new_values)) print('diff in policy_evaluation:{}'.format(diff)) values = new_values if diff \u0026lt;= 1e-1: print('Values are converged!') return values  policy_improvement的实现，将新policy赋为value最大的action。同时返回policy的变化数目，以判断收敛。\ndef policy_improvement_helper(state, values, actions): # 此辅助函数返回给定state的最优action # 不够移的action给-inf v_max = -float('inf') best_action = 0 for action in actions: if ((action \u0026gt;= 0 and state[0] \u0026gt;= action) or (action \u0026lt; 0 and state[1] \u0026gt;= abs(action))) == False: v = -float('inf') else: v = bellman(values, state, action) if v \u0026gt;= v_max: v_max = v best_action = action return best_action, state def policy_improvement(actions, values, policy): # 并行的更新policy表 并返回新policy表 new_policy = np.copy(policy) results = [] with mp.Pool(processes=MP_PROCESS_NUM) as p: k = np.arange(MAX_CARS + 1) states = ((i, j) for i, j in itertools.product(k, k)) f = functools.partial(policy_improvement_helper, values=values, actions=actions) results = p.map(f, states) for a, (i, j) in results: new_policy[i, j] = a policy_change = np.sum(new_policy != policy) return new_policy, policy_change  最后是solve函数，其中values 和 policy 表都以0作为初值\ndef solve(): # 初始化值函数和策略函数表，action表 values = np.zeros((MAX_CARS + 1, MAX_CARS + 1)) policy = np.zeros_like(values, dtype=np.int) actions = np.arange(-MAX_MOVE, MAX_MOVE + 1) # [-5,-4 ... 4,5] iteration_count = 0 print('Solving...') while True: start_time = time.time() print('#'*10) print('Runnning policy_evaluation...') values = policy_evaluation(values, policy) print('Running policy_improvement...') policy, policy_change = policy_improvement(actions, values, policy) #print(policy, policy_change) #assert False iteration_count += 1 print('iter {} costs {}'.format(iteration_count, time.time()-start_time)) print('policy_change:{}'.format(policy_change)) # policy不再变化时终止更新 if policy_change == 0: print('Done!') return values, policy  最后需如下执行，必需有__main__以防mutiprocessing报错\ndef main(): values, policy = solve() plot(policy) if __name__ == '__main__': main()  此程序在i5-8500执行需要约300s。\n","date":1563249600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563249600,"objectID":"82c37711546e16b38a421e3095c28bfd","permalink":"https://leidawt.github.io/en/post/%E7%BB%8F%E5%85%B8policy-iteration%E5%AE%9E%E7%8E%B0/","publishdate":"2019-07-16T12:00:00+08:00","relpermalink":"/en/post/%E7%BB%8F%E5%85%B8policy-iteration%E5%AE%9E%E7%8E%B0/","section":"post","summary":"本文总结了强化学习中的经典Policy Iteration方法，在一个租车问题背景之下使用python实现，踩了一下python多进程的坑。。","tags":[],"title":"经典Policy Iteration实现","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"ref: https://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf MCMC（Markov chain Monte Carlo）是一类采样方法，起源与1930年代的研究。MCMC模拟是解决某些高维困难问题的唯一有效方法，通过选择统计样本来近似困难组合问题是现代MCMC模拟的核心。 MCMC计技术经常被用于解决高维空间下的积分和优化问题，比如求期望，求目标函数极值等。MCMC方法从目标分布p(x) iid的采样N个样点，那么便可由大数定律保证如下的近似： \r\r\r但有时候p(x)并不是像高斯分布那样易于采样的形式，此时需要一些更复杂的技术，如拒绝采样，重要性采样和MCMC。 拒绝采样算法如下所示： \r\r\r\r\r\r其中q分布一般取好采样的分布比如均匀分布和高斯分布，称参考分布，显然p q若比较接近那么采样效率会比较好，之后乘系数M使得参考分布能完全包住目标分布，之后的采样操作很巧妙，可以看做是以正比于p的概率接收样点。此方法的一个问题是为满足约束M有时会被迫取的很大，从而导致： \r\r\r这一问题使得拒绝采样在高维场景下不佳。 https://blog.csdn.net/jteng/article/details/54344766 重要性采样： \r\r\rhttps://www.jianshu.com/p/3d30070932a8\nMCMC采样： 马尔科夫链（MC）精髓之一在于定义状态转移的概率只依赖于前一个状态。MC有一个有名的收敛定理指出不管链的初始状态如何，最终状态都将收敛到一个固定的终止分布。（需满足以下条件：可能的状态数是有限的，转移概率固定不变，从任意状态能够转变到任意其他状态） https://www.cnblogs.com/pinard/p/6632399.html https://cosx.org/2013/01/lda-math-mcmc-and-gibbs-sampling/ MCMC方法利用了MC链会收敛到确定分布的性质，即如果我们能构造一个转移矩阵为P的马氏链，使得该马氏链的平稳分布恰好是p(x), 那么我们从任何一个初始状态x0出发沿着马氏链转移, 得到一个转移序列 x0,x1,x2,⋯xn,xn+1⋯,， 如果马氏链在第n步已经收敛了，于是我们就得到了p(x)的样本xn,xn+1⋯！！！ 上述方法起源于Metropolis，经一些优化后成为了经典的MH-MCMC算法。 关于MH-MCMC的原理详见： https://zhuanlan.zhihu.com/p/37121528 https://cosx.org/2013/01/lda-math-mcmc-and-gibbs-sampling/ 简言之，MH-MCMC目标是设计能收敛到目标分布的MC链的巧妙方法。直接找到理想转移矩阵是几乎不可行的，MH方法引入接受率概念对不完美的转移矩阵进行修正，使得满足收敛条件。 上面资料主要以离散形式进行说明，在连续问题上，MC链的状态转移如下计算，\r\r\r\r原来的转移矩阵变为了积分核K，经常为高斯分布。这种形式的MCMC在实践中更为多见，积分核K的具体设计是个关键问题，很多MCMC的改进变种都针对积分核K进行(比如著名的吉布斯采样，进来还有用深度学习来做核的)。下图为使用不同方差高斯积分核K的采样结果，展示了因核不佳造成采样效率低和视野窄的问题，这也是设计新核的最主要关注点和最期望改善的问题。 \r\r\rhttps://zhuanlan.zhihu.com/p/67691581 简介了一种重要的改进算法：HMC，HMC是很多概率建模软件如pymc stan等的默认MCMC方法，工程上使用非常普遍。\n","date":1561003200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561003200,"objectID":"6a2ac4a3043da8c5c5c5b7d4e3a8c02b","permalink":"https://leidawt.github.io/en/post/mcmc%E9%87%87%E6%A0%B7%E7%AE%97%E6%B3%95/","publishdate":"2019-06-20T12:00:00+08:00","relpermalink":"/en/post/mcmc%E9%87%87%E6%A0%B7%E7%AE%97%E6%B3%95/","section":"post","summary":"ref: https://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf MCMC（Markov chain Monte Carlo）是一类采样方法，起源与1930年代的研究。MCMC模拟是解决某些高维困难问题的唯一有效方法，通过选","tags":[],"title":"MCMC采样算法","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"此书从实践角度讲了卡尔曼等一系列贝叶斯滤波器，没有从线控视角入手，提供了大量直观解读和代码实例，看着玩玩摘录些重点 @[TOC]\n1.g-h滤波器 又称Alpha beta filter，f-g filter，是一类融合观测和估计的滤波器中形式上最朴素的。原书表达不清晰，下为wiki的。g-h滤波器只考虑 x x\u0026rsquo; 作为状态变量 \r\r\r其中$x_k$是观测，此算法很简单，（1）（2）按模型更新状态，（3）步融合观测来得到残差，（4）（5）根据残差修订状态。显然其中两个参数是决定的是融合比例。 参数的选择 g的影响 \r\r\rh的影响 \r\r\r2. 离散贝叶斯滤波器 这是卡尔曼滤波，例子滤波等一类滤波器的基本框架 \r\r\r核心思想如下 $$\\begin{aligned} \\bar {\\mathbf x} \u0026amp;= \\mathbf x \\ast f_{\\mathbf x}(\\bullet), , \u0026amp;\\text{Predict Step} \\\n\\mathbf x \u0026amp;= |\\mathcal L \\cdot \\bar{\\mathbf x}|, , \u0026amp;\\text{Update Step}\\end{aligned}$$ $\\mathcal L$是似然函数。$||$符号表示取范数。我们需要将概率与先验的乘积标准化以确保$x$是一个和为1的概率分布。 流程伪代码如下：\n初始化 初始化我们对状态的信念。 预测 根据系统行为，预测下一步的状态 调整信念，以解释预测中的不确定性 更新 获取测量并给一个对其精度的信念 计算测量值对每个状态匹配的程度（likelihood） 用这种可能性更新状态信念\n这种形式的算法有时也叫预测校正。 例子： 背景陈述：假设追踪狗在走廊里的位置，为了方便，离散化为10个位置，其中三个位置由门，其他位置没有。我们对狗的行为一无所知，故采用平坦先验，即认为狗在任意位置出现等可能。不过我们能获得狗身上声呐传感器的数据，故可以知道狗是否在有门的位置。 先看主函数逻辑：\ndef discrete_bayes_sim(prior, kernel, measurements, z_prob, hallway): posterior = np.array([.1]*10) priors, posteriors = [], [] for i, z in enumerate(measurements): prior = predict(posterior, 1, kernel) priors.append(prior) likelihood = lh_hallway(hallway, z, z_prob) posterior = update(likelihood, prior) posteriors.append(posterior) return priors, posteriors hallway = np.array([1, 0, 1, 0, 0]*2) kernel = (.1, .8, .1) prior = np.array([.1] * 10) zs = [1, 0, 1, 0, 0, 1] z_prob = 0.75 priors, posteriors = discrete_bayes_sim(prior, kernel, zs, z_prob, hallway) interact(animate_discrete_bayes, step=IntSlider(value=12, max=len(zs)*2));  predict函数根据上次位置预测结果为先验进行预测，预测方法是假设其以kernel（）的概率向左或向右移动1个位置。为了计算方便，是通过卷积实现的，是个小trick convolve(np.roll(pdf, offset), kernel, mode='wrap\u0026rsquo;) 此步骤实际是全概率公式的应用：$$P(X_i^t) = \\sum_j P(X_j^{t-1}) P(x_i | x_j)$$\n得到新的prior 后，计算观测的likelihood（对每个点）。 这个玩意在不同具体任务中计算方式不同，本问题中算法很朴素：\ntry: scale = z_prob / (1. - z_prob) except ZeroDivisionError: scale = 1e8 likelihood = np.ones(len(hall)) likelihood[hall==z] *= scale return likelihood  z_prob 表示相信传感器的程度。例子中何为1表示完全相信，故当z=1时强烈加大在三个有门位置的likelihood，z=0时表示一定没门，强烈加大其余位置likelihood。 最后的update步骤实为应用下面的贝叶斯公式产生新息，其将likelihood和prior相乘并归一化 $$p(x_i \\mid z) = \\frac{p(z \\mid x_i) p(x_i)}{p(z)}$$ 其中p(x)即prior，p(z|x)即算得的likelihood。分母仅仅是个norm项，顾不必计算，只需归一化一下即可。\n最后运行起来如下： \r\r\r可以看到两件事，一是因为predict的不确定，分布在预测阶段发散，但由于观测的帮助，弥补了纯预测的信度损失。 此方法几大缺陷： 1.此filter是多峰的（multimodal），不能给出唯一确定的答案，不过这在很多场景下反而不是问题 2.需要直接测量状态变化\n3.概率，高斯和贝叶斯 俩高斯分布相乘:\n$$\\begin{aligned}\\mu \u0026amp;=\\frac{\\sigma_1^2\\mu_2 + \\sigma_2^2\\mu_1}{\\sigma_1^2+\\sigma_2^2}\\\n\\sigma^2 \u0026amp;=\\frac{\\sigma_1^2\\sigma_2^2}{\\sigma_1^2+\\sigma_2^2} \\end{aligned}$$\n俩高斯分布相加\n$$\\begin{gathered}\\mu = \\mu_1 + \\mu_2 \\\n\\sigma^2 = \\sigma^2_1 + \\sigma^2_2 \\end{gathered}$$\n在前一章的离散分布情形下，我们使用了 element-wise成来updata后验分布，计算比较麻烦，如果分布写作高斯分布形式，即可用上面的公式大大简单计算。\n4. 一维卡尔曼滤波 背景问题描述：狗位置预测，传感器返回狗位置，带高斯噪声。 我们以高斯建模传感器观测，套用离散贝叶斯滤波器的基本形式如下\n$$\\begin{array}{l|l|c} \\text{discrete Bayes} \u0026amp; \\text{Gaussian} \u0026amp; \\text{Step}\\\n\\hline \\bar {\\mathbf x} = \\mathbf x \\ast f(\\mathbf x) \u0026amp; \\bar {x}\\mathcal{N} = x\\mathcal{N} , \\oplus , f_{x_\\mathcal{N}}(\\bullet) \u0026amp; \\text{Predict} \\\n\\mathbf x = |\\mathcal L \\bar{\\mathbf x}| \u0026amp; x_\\mathcal{N} = L , \\otimes , \\bar{x}_\\mathcal{N} \u0026amp; \\text{Update} \\end{array}$$\n具体predict方法 $$ \\begin{aligned}\\bar{x}k \u0026amp;= x{k-1} + v_k \\Delta t \\\n\u0026amp;= x_{k-1} + f_x\\end{aligned}$$ 其中$f_x,x$均为高斯分布。编程上用python的具名数组做了一下，比较规范，还改了repr魔法方法\nfrom collections import namedtuple gaussian = namedtuple('Gaussian', ['mean', 'var']) gaussian.__repr__ = lambda s: '?(μ={:.3f}, ?²={:.3f})'.format(s[0], s[1]) ''' g1 = gaussian(3.4, 10.1) g2 = gaussian(mean=4.5, var=0.2**2) print(g1) g1.mean, g1[0], g1[1], g1.var ''' def predict(pos, movement): return gaussian(pos.mean + movement.mean, pos.var + movement.var)  具体update方法 likelihood是给定当前状态下测量的概率，此处likelihood就是我们的measurement，也用高斯表示。 $$\\begin{aligned} \\mathcal N(\\mu, \\sigma^2) \u0026amp;= | prior \\cdot likelihood |\\\n\u0026amp;= | \\mathcal{N}(\\bar\\mu, \\bar\\sigma^2)\\cdot \\mathcal{N}(\\mu_z, \\sigma_z^2) |\\\n\u0026amp;= \\mathcal N(\\frac{\\bar\\sigma^2 \\mu_z + \\sigma_z^2 \\bar\\mu}{\\bar\\sigma^2 + \\sigma_z^2},\\frac{\\bar\\sigma^2\\sigma_z^2}{\\bar\\sigma^2 + \\sigma_z^2}) \\end{aligned}$$\ndef gaussian_multiply(g1, g2): mean = (g1.var * g2.mean + g2.var * g1.mean) / (g1.var + g2.var) variance = (g1.var * g2.var) / (g1.var + g2.var) return gaussian(mean, variance) def update(prior, likelihood): posterior = gaussian_multiply(likelihood, prior) return posterior  人工制造一个观测序列zs，在zs上构建卡尔曼滤波器\n# perform Kalman filter on measurement z for z in zs: prior = predict(x, process_model) likelihood = gaussian(z, sensor_var) x = update(prior, likelihood) kf_internal.print_gh(prior, x, z)  \r\r\r\r开始方差很大是因为设的初始系统方差很大，可以看到收敛非常快。 以上程序就是一维卡尔曼的实现，下面将其数学表达转换为更传统的表达方式，不过此形式下贝叶斯滤波的意思不再那么明显： We see that the filter works. Now let\u0026rsquo;s go back to the math to understand what is happening. The posterior $x$ is computed as the likelihood times the prior ($\\mathcal L \\bar x$), where both are Gaussians.\nTherefore the mean of the posterior is given by:\n$$ \\mu=\\frac{\\bar\\sigma^2, \\mu_z + \\sigma_z^2 , \\bar\\mu} {\\bar\\sigma^2 + \\sigma_z^2} $$\nI use the subscript $z$ to denote the measurement. We can rewrite this as:\n$$\\mu = \\left( \\frac{\\bar\\sigma^2}{\\bar\\sigma^2 + \\sigma_z^2}\\right) \\mu_z + \\left(\\frac{\\sigma_z^2}{\\bar\\sigma^2 + \\sigma_z^2}\\right)\\bar\\mu$$\nIn this form it is easy to see that we are scaling the measurement and the prior by weights:\n$$\\mu = W_1 \\mu_z + W_2 \\bar\\mu$$\nThe weights sum to one because the denominator is a normalization term. We introduce a new term, $K=W_1$, giving us:\n$$\\begin{aligned} \\mu \u0026amp;= K \\mu_z + (1-K) \\bar\\mu\\\n\u0026amp;= \\bar\\mu + K(\\mu_z - \\bar\\mu) \\end{aligned}$$\nwhere\n$$K = \\frac {\\bar\\sigma^2}{\\bar\\sigma^2 + \\sigma_z^2}$$\n$K$ is the Kalman gain. It\u0026rsquo;s the crux of the Kalman filter. It is a scaling term that chooses a value partway between $\\mu_z$ and $\\bar\\mu$.\nLet\u0026rsquo;s work a few examples. If the measurement is nine times more accurate than the prior, then $\\bar\\sigma^2 = 9\\sigma_z^2$, and\n$$\\begin{aligned} \\mu\u0026amp;=\\frac{9 \\sigma_z^2 \\mu_z + \\sigma_z^2, \\bar\\mu} {9 \\sigma_z^2 + \\sigma_\\mathtt{z}^2} \\\n\u0026amp;= \\left(\\frac{9}{10}\\right) \\mu_z + \\left(\\frac{1}{10}\\right) \\bar\\mu \\end{aligned} $$\nHence $K = \\frac 9 {10}$, and to form the posterior we take nine tenths of the measurement and one tenth of the prior.\nIf the measurement and prior are equally accurate, then $\\bar\\sigma^2 = \\sigma_z^2$ and\n$$\\begin{gathered} \\mu=\\frac{\\sigma_z^2, (\\bar\\mu + \\mu_z)}{2\\sigma_\\mathtt{z}^2} \\\n= \\left(\\frac{1}{2}\\right)\\bar\\mu + \\left(\\frac{1}{2}\\right)\\mu_z \\end{gathered}$$\nwhich is the average of the two means. It makes intuitive sense to take the average of two equally accurate values.\nWe can also express the variance in terms of the Kalman gain:\n$$\\begin{aligned} \\sigma^2 \u0026amp;= \\frac{\\bar\\sigma^2 \\sigma_z^2 } {\\bar\\sigma^2 + \\sigma_z^2} \\\n\u0026amp;= K\\sigma_z^2 \\\n\u0026amp;= (1-K)\\bar\\sigma^2 \\end{aligned}$$ \r\r\r按此推导的等价实现如下：\ndef update(prior, measurement): x, P = prior # mean and variance of prior z, R = measurement # mean and variance of measurement y = z - x # residual K = P / (P + R) # Kalman gain x = x + K*y # posterior P = (1 - K) * P # posterior variance return gaussian(x, P) def predict(posterior, movement): x, P = posterior # mean and variance of posterior dx, Q = movement # mean and variance of movement x = x + dx P = P + Q return gaussian(x, P)  总结：\nPredict\n$$\\begin{array}{|l|l|l|}\\\n\\hline\\\n\\text{Equation} \u0026amp; \\text{Implementation} \u0026amp; \\text{Kalman Form}\\\n\\hline\\\n\\bar x = x + f_x \u0026amp; \\bar\\mu = \\mu + \\mu_{f_x} \u0026amp; \\bar x = x + dx\\\n\u0026amp; \\bar\\sigma^2 = \\sigma^2 + \\sigma_{f_x}^2 \u0026amp; \\bar P = P + Q\\\n\\hline\\\n\\end{array}$$\nUpdate\n$$\\begin{array}{|l|l|l|} \\hline \\text{Equation} \u0026amp; \\text{Implementation}\u0026amp; \\text{Kalman Form}\\\n\\hline x = | \\mathcal L\\bar x| \u0026amp; y = z - \\bar\\mu \u0026amp; y = z - \\bar x\\\n\u0026amp; K = \\frac {\\bar\\sigma^2} {\\bar\\sigma^2 + \\sigma_z^2} \u0026amp; K = \\frac {\\bar P}{\\bar P+R}\\\n\u0026amp; \\mu = \\bar \\mu + Ky \u0026amp; x = \\bar x + Ky\\\n\u0026amp; \\sigma^2 = \\frac {\\bar\\sigma^2 \\sigma_z^2} {\\bar\\sigma^2 + \\sigma_z^2} \u0026amp; P = (1-K)\\bar P\\\n\\hline \\end{array}$$\n其他小问题： 1.此方法难以处理非线性 2.在面临受限的硬件时，可以直接将K设为其最后的收敛值来简化计算。\n5.多维高斯 $$ f(\\mathbf{x},, \\mu,,\\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}, \\exp \\Big [{ -\\frac{1}{2}(\\mathbf{x}-\\mu)^\\mathsf{T}\\Sigma^{-1}(\\mathbf{x}-\\mu) \\Big ]} $$ 由于其协方差矩阵包含变量关系的信息，因此对预测是有帮助的 多维高斯相乘： $$\\begin{aligned} \\mu \u0026amp;= \\Sigma_2(\\Sigma_1 + \\Sigma_2)^{-1}\\mu_1 + \\Sigma_1(\\Sigma_1 + \\Sigma_2)^{-1}\\mu_2 \\\n\\Sigma \u0026amp;= \\Sigma_1(\\Sigma_1+\\Sigma_2)^{-1}\\Sigma_2 \\end{aligned}$$\n6.多维卡尔曼滤波 算法：\nPredict\n$\\begin{array}{|l|l|l|} \\hline \\text{Univariate} \u0026amp; \\text{Univariate} \u0026amp; \\text{Multivariate}\\\n\u0026amp; \\text{(Kalman form)} \u0026amp; \\\n\\hline \\bar \\mu = \\mu + \\mu_{f_x} \u0026amp; \\bar x = x + dx \u0026amp; \\bar{\\mathbf x} = \\mathbf{Fx} + \\mathbf{Bu}\\\n\\bar\\sigma^2 = \\sigma_x^2 + \\sigma_{f_x}^2 \u0026amp; \\bar P = P + Q \u0026amp; \\bar{\\mathbf P} = \\mathbf{FPF}^\\mathsf T + \\mathbf Q \\\n\\hline \\end{array}$\n$\\mathbf x,, \\mathbf P$ 是状态均值和方差\n$\\mathbf F$ 是状态转移矩阵\n$\\mathbf Q$ 过程方差\n$\\mathbf B$ ， $\\mathbf u$ 输入矩阵和输入，新加入的\nUpdate\n$\\begin{array}{|l|l|l|} \\hline \\text{Univariate} \u0026amp; \\text{Univariate} \u0026amp; \\text{Multivariate}\\\n\u0026amp; \\text{(Kalman form)} \u0026amp; \\\n\\hline \u0026amp; y = z - \\bar x \u0026amp; \\mathbf y = \\mathbf z - \\mathbf{H\\bar x} \\\n\u0026amp; K = \\frac{\\bar P}{\\bar P+R}\u0026amp; \\mathbf K = \\mathbf{\\bar{P}H}^\\mathsf T (\\mathbf{H\\bar{P}H}^\\mathsf T + \\mathbf R)^{-1} \\\n\\mu=\\frac{\\bar\\sigma^2, \\mu_z + \\sigma_z^2 , \\bar\\mu} {\\bar\\sigma^2 + \\sigma_z^2} \u0026amp; x = \\bar x + Ky \u0026amp; \\mathbf x = \\bar{\\mathbf x} + \\mathbf{Ky} \\\n\\sigma^2 = \\frac{\\sigma_1^2\\sigma_2^2}{\\sigma_1^2+\\sigma_2^2} \u0026amp; P = (1-K)\\bar P \u0026amp; \\mathbf P = (\\mathbf I - \\mathbf{KH})\\mathbf{\\bar{P}} \\\n\\hline \\end{array}$\n$\\mathbf H$ 观测函数\n$\\mathbf z,, \\mathbf R$ 是测量均值和测量方差\n$\\mathbf y$ ， $\\mathbf K$ 残差，卡尔曼增益\n与单变量相比，只是引入了矩阵，其他都一样：\n 使用高斯函数来表示我们对状态和误差的估计 使用高斯函数表示测量值及其误差 使用高斯函数表示过程模型 使用过程模型预测下一个状态(先验) 在测量值和先验值之间形成一个估计 注：细节的一些直观理解 在predict步中，状态方差以$\\mathbf{\\bar P} = \\mathbf{FPF}^\\mathsf T + \\mathbf Q$方式计算得到，而不是简单P+Q，这是因为要映射到相同空间才能运算。$\\mathbf{FPF}^\\mathsf T$将P从t-1状态空间映射变换到t的状态空间。  下面以之前的狗追踪问题举例说明 这次试用位置和速度作为状态变量，其中位置可测，作为观测变量，速度作为隐变量，假设做匀速直线运动，并估测了初试位置和过程方差，如下，过程方差Q还未确定\nfrom filterpy.kalman import predict x = np.array([10.0, 4.5]) P = np.diag([500, 49]) F = np.array([[1, dt], [0, 1]]) # Q is the process noise x, P = predict(x=x, P=P, F=F, Q=0)  预测结果如下： \r\r\r可以看到随着 predict的进行，不确定在增加，同时速度与位置的相关性在增加。 过程噪声设计： Q是表示系统受外界的干扰的，具体细节在下章讨论，现在先使用库函数计算\nfrom filterpy.common import Q_discrete_white_noise Q = Q_discrete_white_noise(dim=2, dt=1., var=2.35) print(Q) ''' [[0.588 1.175] [1.175 2.35 ]] '''  最后，我们加上控制项，这可以让我们得以控制对象。$\\Delta\\mathbf x = \\mathbf{Bu}$，至此，完整的预测方程已经确定，次例子是无控的，故设u=0$$\\mathbf{\\bar x} = \\mathbf{Fx} + \\mathbf{Bu}$$ 之后来看updata步： 为了求观测和预测之间的残差，需要一个观测函数（矩阵）H将状态空间映射到观测空间。本例中为[1,0]，最后设置好观测方差R，整个滤波器就完全确定好了\nfrom filterpy.kalman import update H = np.array([[1., 0.]]) R = np.array([[5.]]) z = 1. x, P = update(x, P, z, R, H)  7. 卡尔曼滤波器数学细节 动态系统的状态空间表达 一般建立系统微分方程后很容易得到连续状态方程$\\dot{\\mathbf x} = \\mathbf{Ax} + \\mathbf{Bu} + w$（w是噪声），接下来的问题是要离散化为$\\mathbf x_k = \\mathbf {Fx}_{k-1}$以让计算机得以运行。离散化实际上就是在解微分方程组，方法很多，工程上常用数值方法。如van Loan\u0026rsquo;s方法可求 $\\mathbf F_k$ 并同时得到 $\\mathbf Q_k$ 过程噪声Q的确定 相似的，我们也需要对噪声w离散化以确定Q，具体要看我们对噪声的假设，典型的有连续白噪声模型和分段白噪声模型，各有利弊，实践上通过实验来确定。其具体计算可调用如下：\nfrom filterpy.common import Q_continuous_white_noise from filterpy.common import Q_discrete_white_noise Q = Q_continuous_white_noise(dim=2, dt=1, spectral_density=1) Q = Q_discrete_white_noise(2, var=1.)  Q可以被简化。当时间步t很小的时候，我们大可以只保留最高阶状态变量对应的值，而将其他设为0。如 $$\\mathbf Q=\\begin{bmatrix}0\u0026amp;0\u0026amp;0\\0\u0026amp;0\u0026amp;0\\0\u0026amp;0\u0026amp;\\sigma^2\\end{bmatrix}$$ 对 $x=\\begin{bmatrix}x \u0026amp; \\dot x \u0026amp; \\ddot{x} \u0026amp; y \u0026amp; \\dot{y} \u0026amp; \\ddot{y}\\end{bmatrix}^\\mathsf{T}$ Q 将为 6x6，这时可以保留$\\ddot{x}$ ， $\\ddot{y}$ ，其他都设为0.\n$\\mathbf P = (\\mathbf I - \\mathbf{KH})\\mathbf{\\bar P}$的计算稳定性问题 由于浮点数的精度损失，此式子在计算的时候可能使得P不再对称，由于P是协方差矩阵，其应该是严格对称的，这会导致滤波器发散等一系列错误。因此实际上使用了Joseph 方程计算P。$$\\mathbf P = (\\mathbf I-\\mathbf {KH})\\mathbf{\\bar P}(\\mathbf I-\\mathbf{KH})^\\mathsf T + \\mathbf{KRK}^\\mathsf T$$ 此方法不仅能解决因为浮点不对称引起的滤波器不稳定，亦可一定程度上解决因模型不精确和非高斯噪声等实际环境引起的滤波器发散问题。\n8.面向实际问题的卡尔曼滤波器的设计 以过滤来自有噪声传感器的(x,y)为例（人工数据，匀速直线运动）。显然可取状态变量和转移矩阵 $$\\mathbf x = \\begin{bmatrix}x \u0026amp; \\dot x \u0026amp; y \u0026amp; \\dot y\\end{bmatrix}^\\mathsf T$$ $$ \\begin{bmatrix}x \\ \\dot x \\ y \\ \\dot y\\end{bmatrix} = \\begin{bmatrix}1\u0026amp; \\Delta t\u0026amp; 0\u0026amp; 0\\0\u0026amp; 1\u0026amp; 0\u0026amp; 0\\0\u0026amp; 0\u0026amp; 1\u0026amp; \\Delta t\\ 0\u0026amp; 0\u0026amp; 0\u0026amp; 1\\end{bmatrix}\\begin{bmatrix}x \\ \\dot x \\ y \\ \\dot y\\end{bmatrix}$$ 之后设计过程噪声，采用离散白噪声模型，并认为x,y直接独立。 系统无控制，故设B=0。 确定观测矩阵：由于我们传感器返回$\\begin{bmatrix}x \u0026amp; y\\end{bmatrix}^\\mathsf T$ ，且单位为英尺，故设: $$\\mathbf H = \\begin{bmatrix} \\frac{1}{0.3048} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\n0 \u0026amp; 0 \u0026amp; \\frac{1}{0.3048} \u0026amp; 0 \\end{bmatrix} $$ 观测噪声矩阵：认为x,y独立，假设高斯噪声，设： $$\\mathbf R = \\begin{bmatrix}\\sigma_x^2 \u0026amp; \\sigma_y\\sigma_x \\ \\sigma_x\\sigma_y \u0026amp; \\sigma_{y}^2\\end{bmatrix} = \\begin{bmatrix}5\u0026amp;0\\0\u0026amp;5\\end{bmatrix}$$ 最后设置下初始值，注意P初值习惯上给的较大。 $$ \\mathbf x = \\begin{bmatrix}0\\0\\0\\0\\end{bmatrix}, , \\mathbf P = \\begin{bmatrix}500\u0026amp;0\u0026amp;0\u0026amp;0\\0\u0026amp;500\u0026amp;0\u0026amp;0\\0\u0026amp;0\u0026amp;500\u0026amp;0\\0\u0026amp;0\u0026amp;0\u0026amp;500\\end{bmatrix}$$\nfrom filterpy.stats import plot_covariance_ellipse from kf_book.book_plots import plot_filter R_std = 0.35 Q_std = 0.04 def tracker1(): tracker = KalmanFilter(dim_x=4, dim_z=2) dt = 1.0 # time step tracker.F = np.array([[1, dt, 0, 0], [0, 1, 0, 0], [0, 0, 1, dt], [0, 0, 0, 1]]) tracker.u = 0. tracker.H = np.array([[1/0.3048, 0, 0, 0], [0, 0, 1/0.3048, 0]]) tracker.R = np.eye(2) * R_std**2 q = Q_discrete_white_noise(dim=2, dt=dt, var=Q_std**2) tracker.Q = block_diag(q, q) tracker.x = np.array([[0, 0, 0, 0]]).T tracker.P = np.eye(4) * 500. return tracker # simulate robot movement N = 30 sensor = PosSensor((0, 0), (2, .2), noise_std=R_std) zs = np.array([sensor.read() for _ in range(N)]) # run filter robot_tracker = tracker1() mu, cov, _, _ = robot_tracker.batch_filter(zs) for x, P in zip(mu, cov): # covariance of x and y cov = np.array([[P[0, 0], P[2, 0]], [P[0, 2], P[2, 2]]]) mean = (x[0, 0], x[2, 0]) plot_covariance_ellipse(mean, cov=cov, fc='g', std=3, alpha=0.5) #plot results zs *= .3048 # convert to meters plot_filter(mu[:, 0], mu[:, 2]) plot_measurements(zs[:, 0], zs[:, 1]) plt.legend(loc=2) plt.xlim(0, 20);  效果尚可 \r\r\r模型阶数选择 具体应该用多少阶的模型来描述实际问题？下面给出0,1,2阶的系统，并对其进行性能分析。具体分析方法是画出真实值与预测值直接的残差，并借助P矩阵的对角线画出置信域，如下是一阶的： \r\r\r当换到二阶滤波器，虽然短期内不是很差，但可观察到长期发散。 \r\r\r高阶数的过程模型很容易过拟合，我们可以选择采用较低阶数，同时适当加大过程噪声Q以达到统样不错的效果。 检测和避免糟糕的测量值 显然实践中由于传感器故障等原因我们偶尔会得到很糟糕的观测值，这个需要被检测和剔除以避免其破坏性影响。一个朴素但有效的做法是gating。可以利用P矩阵中的状态方差来判断观测值的好坏。具体的，可以使用mahalanobis distance。mahalanobis distance判断一个点到分布的距离，其定义如下： $$D_m= \\sqrt{(\\mathbf x-\\mu)^\\mathsf T \\mathbf S^{-1} (\\mathbf x-\\mu)}$$ 其中S是协方差矩阵。 它其实很像欧氏距离: $$D_e= \\sqrt{(\\mathbf x-\\mathbf y)^\\mathsf T (\\mathbf x-\\mathbf y)}$$ 若S是对角阵，马氏距离可简化为： $$D_m = \\sqrt{\\sum_{i-1}^N \\frac{(x_i - \\mu_i)^2}{\\sigma_i}}$$ 就是欧氏距离加了个方差做因子。 **滤波器性能评估** 真实环境下的卡尔曼滤波器设计往往靠式，因此这里提几个滤波器性能的客观量化指标。\n Normalized Estimated Error Squared（NEES） 这是当能获取状态变量ground-truth时的最佳选择。 $$\\tilde{\\mathbf x} = \\mathbf x - \\hat{\\mathbf x} \\ \\epsilon = \\tilde{\\mathbf x}^\\mathsf T\\mathbf P^{-1}\\tilde{\\mathbf x}$$  from filterpy.stats import NEES  事实上这是一个假设检验方法。我们认定当NEES的均值\u0026lt;状态变量维度时是一个好的滤波器。 2. 似然函数 当无法获得ground-truth的时候，可计算似然，即观测符合当前模型假设的程度。 $$\\begin{aligned} \\mathbf y \u0026amp;= \\mathbf z - \\mathbf{H \\bar x}\\\n\\mathbf S \u0026amp;= \\mathbf{H\\bar{P}H}^\\mathsf T + \\mathbf R\\\n\\mathcal{L} = \\frac{1}{\\sqrt{2\\pi S}}\\exp [-\\frac{1}{2}\\mathbf y^\\mathsf T\\mathbf S^{-1}\\mathbf y] \\end{aligned} $$ 这个也很适合用于剔除坏的测量，如下 \r\r\r9.非线性滤波 原始的卡尔曼滤波器是线性的，我们生活在非线性的世界，需要一些非线性的技术。 非线性 数学上的线性要求以下两点：\n additivity: $f(x+y) = f(x) + f(y)$ homogeneity: $f(ax) = af(x)$  非线性的效果 考虑一个有噪声雷达跟踪系统，它向我们报告物体距离50km，方位90度，那么我们直觉上可能认为其绝对坐标应该是（50，0）。我们设距离方差为0.4，角度方差为0.35，并采样3000点如下，计算3000点的均值标记为星。 \r\r\r可见由于非线性的缘故，我们的直觉失效了。 下图绘出了分析函数对高斯分布的影响，可以看到虽然f(x)看起来还是挺线性的，但是影响巨大，这对卡尔曼滤波器的假设具有破坏性打击。 \r\r\rUKF,EKF,例子滤波 是几种处理非线性滤波的有效手段，尤其UKF和粒子滤波器在实践中取得了非常好的效果。\n9. 无迹卡尔曼滤波器UKF 该技术与1997年提出，相对于EKF等算是比较新的。有趣的是其名称中的unscented一词并没有任何含义，而是源于发明者开的玩笑。。。 上一章的例子使我们了解到由于非线性函数的存在，新状态已经难以解析的计算得到，因此需要引入采样方法进行数值求解。单纯使用大量随机采样（蒙特卡洛思路）是最朴素的方法，效果不错，能很好的近似原分布经过非线性函数后的均值和方差，但考虑到这个采样在每个时间步update中都要进行，计算量实在太大。更好的方法是使用加权采样。选择的采样点称之为sigma points，记为$\\boldsymbol{\\chi}$。当我们通过一些采样方法得到$\\boldsymbol{\\chi}$后，并可进行如下的计算 $$\\boldsymbol{\\mathcal{Y}} = f(\\boldsymbol{\\chi})$$ 将$\\boldsymbol{\\chi}$通过非线性函数计算其对应的映射之后，计算均值和方差作为新的估计，拿到这些估计值后我们便可与标准的线性卡尔曼滤波方法接轨了。故UKF方法的精髓就是以加权采样近似估计状态在非线性变换之后的均值方差，借助于较好的采样算法，这个估计一般是不错的，故此方法可以在非线性系统中取得较好的性能。 具体算法 $$\\begin{array}{l|l} \\textrm{Kalman Filter} \u0026amp; \\textrm{Unscented Kalman Filter} \\\n\\hline \u0026amp; \\boldsymbol{\\mathcal Y} = f(\\boldsymbol\\chi) \\\n\\mathbf{\\bar x} = \\mathbf{Fx} \u0026amp; \\mathbf{\\bar x} = \\sum w^m\\boldsymbol{\\mathcal Y} \\\n\\mathbf{\\bar P} = \\mathbf{FPF}^\\mathsf T+\\mathbf Q \u0026amp; \\mathbf{\\bar P} = \\sum w^c({\\boldsymbol{\\mathcal Y} - \\mathbf{\\bar x})(\\boldsymbol{\\mathcal Y} - \\mathbf{\\bar x})^\\mathsf T}+\\mathbf Q \\\n\\hline \u0026amp; \\boldsymbol{\\mathcal Z} = h(\\boldsymbol{\\mathcal{Y}}) \\\n\u0026amp; \\boldsymbol\\mu_z = \\sum w^m\\boldsymbol{\\mathcal{Z}} \\\n\\mathbf y = \\mathbf z - \\mathbf{Hx} \u0026amp; \\mathbf y = \\mathbf z - \\boldsymbol\\mu_z \\\n\\mathbf S = \\mathbf{H\\bar PH}^\\mathsf{T} + \\mathbf R \u0026amp; \\mathbf P_z = \\sum w^c{(\\boldsymbol{\\mathcal Z}-\\boldsymbol\\mu_z)(\\boldsymbol{\\mathcal{Z}}-\\boldsymbol\\mu_z)^\\mathsf{T}} + \\mathbf R \\ \\mathbf K = \\mathbf{\\bar PH}^\\mathsf T \\mathbf S^{-1} \u0026amp; \\mathbf K = \\left[\\sum w^c(\\boldsymbol{\\mathcal Y}-\\bar{\\mathbf x})(\\boldsymbol{\\mathcal{Z}}-\\boldsymbol\\mu_z)^\\mathsf{T}\\right] \\mathbf P_z^{-1} \\\n\\mathbf x = \\mathbf{\\bar x} + \\mathbf{Ky} \u0026amp; \\mathbf x = \\mathbf{\\bar x} + \\mathbf{Ky}\\\n\\mathbf P = (\\mathbf{I}-\\mathbf{KH})\\mathbf{\\bar P} \u0026amp; \\mathbf P = \\bar{\\mathbf P} - \\mathbf{KP_z}\\mathbf{K}^\\mathsf{T} \\end{array}$$ 可以看到仅是进行一些调整，其他完全符合卡尔曼滤波的思想。 首先，由于非线性系统难以用状态方程描述，这里转而使用非线性函数$f$描述$F$。相应的，观测矩阵$H$也替换为了$h$函数。$w^m,w^c$分别是sigma points的均值权重和方差权重，在构造sigma points时生成。 Sigma Point的采样方法 几十年来大家水了很多采样方法，目前大家用的最多的是Van der Merwe的方法，其能很好的平衡计算复杂度和精度。该方法由$\\alpha$, $\\beta$, $\\kappa$三个参数控制。 为了方便我们记$\\lambda = \\alpha^2(n+\\kappa)-n$ 先看点的位置如何得到： 首先一个特殊的采样点是 $$ \\mathcal{X}0 = \\mu$$ 之后其余点的计算 $$ \\boldsymbol{\\chi}i = \\begin{cases} \\mu + \\left[ \\sqrt{(n+\\lambda)\\Sigma}\\right ]{i}\u0026amp; \\text{for i=1 .. n} \\\n\\mu - \\left[ \\sqrt{(n+\\lambda)\\Sigma}\\right]{i-n} \u0026amp;\\text{for i=(n+1) .. 2n}\\end{cases} $$ 其中n是状态向量x的维数，$\\Sigma$是协方差矩阵P。+-的形式保证了对称性。下标$i$表示取矩阵的第$i$列。此式子通俗理解是以$\\mu$为中心，扩散$\\Sigma$的一个系数大小的的值。这里的矩阵开方是个很神奇的东西，其定义不唯一，此处使用如下定义：若$\\Sigma = \\mathbf{SS}^\\mathsf T$则 $S = \\sqrt{\\Sigma}$。具体的，使用$\\mathbf S = \\text{cholesky}(\\mathbf P)$ 作为$\\mathbf P$的开方。其中cholesky是cholesky分解。 之后看权重的计算： $$W^m_0 = \\frac{\\lambda}{n+\\lambda} \\\nW^c_0 = \\frac{\\lambda}{n+\\lambda} + 1 -\\alpha^2 + \\beta \\\nW^m_i = W^c_i = \\frac{1}{2(n+\\lambda)};;;i=1..2n$$ 此方法的采样结形如下图： \r\r\r至于三个参数的选择，可如下进行： $\\beta=2$ is a good choice for Gaussian problems, $\\kappa=3-n$ where $n$ is the dimension of $\\mathbf x$ is a good choice for $\\kappa$, and $0 \\le \\alpha \\le 1$ is an appropriate choice for $\\alpha$, where a larger value for $\\alpha$ spreads the sigma points further from the mean. 一个例子：航班追踪 \r\r\r此任务的目标是由雷达给出的距离和角度的观测，得到（x，y）。 我们采用匀速直线运动假设： $$\\mathbf x = \\begin{bmatrix}\\mathtt{distance} \\\\mathtt{velocity}\\ \\mathtt{altitude}\\end{bmatrix}= \\begin{bmatrix}x \\ \\dot x\\ y\\end{bmatrix}$$ 显然转移矩阵是线性的： $$\\mathbf{\\bar x} = \\begin{bmatrix} 1 \u0026amp; \\Delta t \u0026amp; 0 \\ 0\u0026amp; 1\u0026amp; 0 \\ 0\u0026amp;0\u0026amp;1\\end{bmatrix} \\begin{bmatrix}x \\ \\dot x\\ y\\end{bmatrix} $$ 但本例中的观测函数就是非线性的了： $$\\text{r} = \\sqrt{(x_\\text{ac} - x_\\text{radar})^2 + (y_\\text{ac} - y_\\mathtt{radar})^2}$$\n$$\\epsilon = \\tan^{-1}{\\frac{y_\\mathtt{ac} - y_\\text{radar}}{x_\\text{ac} - x_\\text{radar}}}$$ 代码实现如下： 首先创造仿真环境：飞机和雷达站\nfrom numpy.linalg import norm from math import atan2 class RadarStation(object): def __init__(self, pos, range_std, elev_angle_std): self.pos = np.asarray(pos) self.range_std = range_std self.elev_angle_std = elev_angle_std def reading_of(self, ac_pos): \u0026quot;\u0026quot;\u0026quot; Returns (range, elevation angle) to aircraft. Elevation angle is in radians. \u0026quot;\u0026quot;\u0026quot; diff = np.subtract(ac_pos, self.pos) rng = norm(diff) brg = atan2(diff[1], diff[0]) return rng, brg def noisy_reading(self, ac_pos): \u0026quot;\u0026quot;\u0026quot; Compute range and elevation angle to aircraft with simulated noise\u0026quot;\u0026quot;\u0026quot; rng, brg = self.reading_of(ac_pos) rng += randn() * self.range_std brg += randn() * self.elev_angle_std return rng, brg class ACSim(object): def __init__(self, pos, vel, vel_std): self.pos = np.asarray(pos, dtype=float) self.vel = np.asarray(vel, dtype=float) self.vel_std = vel_std def update(self, dt): \u0026quot;\u0026quot;\u0026quot; Compute and returns next position. Incorporates random variation in velocity. \u0026quot;\u0026quot;\u0026quot; dx = self.vel*dt + (randn() * self.vel_std) * dt self.pos += dx return self.pos  之后便可实现借助filterpy库实现UKF：\ndef f_radar(x, dt): \u0026quot;\u0026quot;\u0026quot; state transition function for a constant velocity aircraft with state vector [x, velocity, altitude]'\u0026quot;\u0026quot;\u0026quot; F = np.array([[1, dt, 0], [0, 1, 0], [0, 0, 1]], dtype=float) return np.dot(F, x) def h_radar(x): dx = x[0] - h_radar.radar_pos[0] dy = x[2] - h_radar.radar_pos[1] slant_range = math.sqrt(dx**2 + dy**2) elevation_angle = math.atan2(dy, dx) return [slant_range, elevation_angle] import math from kf_book.ukf_internal import plot_radar dt = 3. # 12 seconds between readings range_std = 5 # meters elevation_angle_std = math.radians(0.5) ac_pos = (0., 1000.) ac_vel = (100., 0.) radar_pos = (0., 0.) h_radar.radar_pos = radar_pos points = MerweScaledSigmaPoints(n=3, alpha=.1, beta=2., kappa=0.) kf = UKF(3, 2, dt, fx=f_radar, hx=h_radar, points=points) kf.Q[0:2, 0:2] = Q_discrete_white_noise(2, dt=dt, var=0.1) kf.Q[2,2] = 0.1 kf.R = np.diag([range_std**2, elevation_angle_std**2]) kf.x = np.array([0., 90., 1100.]) kf.P = np.diag([300**2, 30**2, 150**2]) np.random.seed(200) pos = (0, 0) radar = RadarStation(pos, range_std, elevation_angle_std) ac = ACSim(ac_pos, (100, 0), 0.02) time = np.arange(0, 360 + dt, dt) xs = [] for _ in time: ac.update(dt) r = radar.noisy_reading(ac.pos) kf.predict() kf.update([r[0], r[1]]) xs.append(kf.x) plot_radar(xs, time)  最后，呈现一个较复杂的例子 此例子处理机器人定位问题。我们有一个机器人，它需要用传感器感知地标来定位和导航。 机器人模型采用经典的自行车模型来建模。 $$\\begin{aligned} \\bar x \u0026amp;= x - R\\sin(\\theta) + R\\sin(\\theta + \\beta) \\\n\\bar y \u0026amp;= y + R\\cos(\\theta) - R\\cos(\\theta + \\beta) \\\n\\bar \\theta \u0026amp;= \\theta + \\beta \\end{aligned} $$ 其中$\\beta = \\frac{d}{w} \\tan{(\\alpha)}$ ，$R = \\frac{d}{\\beta}$ 选定状态变量$\\mathbf x = \\begin{bmatrix}x \u0026amp; y \u0026amp; \\theta\\end{bmatrix}^\\mathsf{T}$ 选定控制变量$\\mathbf{u} = \\begin{bmatrix}v \u0026amp; \\alpha\\end{bmatrix}^\\mathsf{T}$（α是前轮转向角） 明确地标p的观测函数： $$\\begin{aligned} \\mathbf{z}\u0026amp; = h(\\mathbf x, \\mathbf P) \u0026amp;+ \\mathcal{N}(0, R)\\\n\u0026amp;= \\begin{bmatrix} \\sqrt{(p_x - x)^2 + (p_y - y)^2} \\\n\\tan^{-1}(\\frac{p_y - y}{p_x - x}) - \\theta \\end{bmatrix} \u0026amp;+ \\mathcal{N}(0, R) \\end{aligned}$$ 这里需要小心角度相减的计算问题，即0°附近的离散性，显然可以取模处理之。 此外还需要注意的是如何对角度求平均？显然359°和1°的平均是0°，但其数值均值却为180°。因此我们需要特别考虑此问题，使用下面的式子计算角度均值： $$\\bar{\\theta} = atan2\\left(\\frac{\\sum_{i=1}^n \\sin\\theta_i}{n}, \\frac{\\sum_{i=1}^n \\cos\\theta_i}{n}\\right)$$ 规定的这些均值计算方法可以传入UKF类的初始化函数。 具体代码见原书，不赘述，具有很好参考价值。 实验： 给一组手工设定的控制序列，撒一些地标，运行如下。可以看到不确定度很小。 \r\r\r删去一些地标，误差显著增大。 \r\r\r11. 扩展卡尔曼滤波器EKF EKF算法在卡尔曼发表线性卡尔曼滤波器后很快被提出，其应对非线性的能力相对较差，同时理论比较繁琐，故暂略\n12. 粒子滤波器PF 之前的技术不能处理多目标，强非线性，强非高斯噪声，无过程模型的场景，这些场景下可以使用PF。 粒子滤波器是一类滤波器，通用的粒子滤波算法框架如下：\n  随机产生一群粒子 粒子可以有位置、方向 和/或 任何其他需要估计的状态变量。 每一个粒子都有一个权值(概率)，表示它与系统实际状态匹配的可能性。 用相同的权重初始化每个对象。\n  预测粒子的下一个状态 根据你对真实系统行为的预测来移动粒子。\n  更新 根据测量结果更新颗粒的权重。 与测量值紧密相称的粒子的权重要高于与测量值不匹配的粒子。\n  重采样 抛弃非常不可能的粒子，用更可能的粒子的拷贝替换它们。\n  计算估计 （可选）计算一组粒子的加权平均值和协方差，得到状态估计。\n  以车辆追踪问题为例，我们选定（x,y,θ）为粒子的变量 初始化阶段，随机初始化粒子状态，基于1/N的均等权重 预测阶段我们让例子有噪声地按控制变量移动 更新阶段我们计算每个例子对观测的似然度，以似然度更新例子权重 重采样阶段我们按例子权重随机采样，防止退化。具体操作方法如下： 对归一化权重[0.0625, 0.125 , 0.1875, 0.25 , 0.125 , 0.1875, 0.0625]，计算累积和序列得[0.0625, 0.1875, 0.375 , 0.625 , 0.75 , 0.9375, 1. ]以便画出如下的图。之后制造0-1的随机数，按照几何概型，其落在各个色块的概率就是权重，进行N次后就可达到完整的重采样结果。 \r\r\r重采样不是每次更新都进行的，没有新息注入时重采样并无意义。我们使用一下指标决定何时进行重采样： $$\\hat{N}_\\text{eff} = \\frac{1}{\\sum w^2}$$（w是粒子权重） 重采样在此值小于每一给定的阈值（比如N/2，其中N为粒子数）后进行。此值是对当前有用粒子数量的估计。这种策略称Sampling Importance Resampling (SIR)。\n在上面的预测和更新两步中我们按输入u移动例子，并使用观测更新其对应权重，这种方法的理论依据正是著名的重要性采样（统计学通用方法，也被openai用在PPO中用于构建off-policy learning）： 使用重要性采样可以更换采样分布，将无法计算的 $$\\mathbb{E}\\big[f(x)\\big] = \\int f(x)\\pi(x), dx$$ 转为 $$\\mathbb{E}\\big[f(x)\\big] = \\int f(x)q(x), , \\cdot , \\frac{\\pi(x)}{q(x)}, dx$$ 在上面的机器人追踪问题中，分布π即机器人状态的分布，这是不一定对的（因为是推断的结果），而q分布即新获得的测量的分布，可以认为是准确的。因此自然的利用重要性采样公式换到q上采样会得到更精准的值。在具体实施中，我们计算的是$\\mu = \\sum\\limits_{i=1}^N x^iw^i$，即加权平均，其中权重w就是重要性采样公式中的π/q。 **粒子滤波中不同的重采样方法** 重采样在粒子滤波中起到防止粒子退化的作用，有重要意义。除了上面的采样方法，还有残差重采样（Residual Resampling），分层重采样（Stratified Resampling），系统重采样（Systematic Resampling）。作者认为其中比较好的是系统重采样，这在采样量小的时候比较明显（所谓好指采样结果符合权重的程度好坏，好的采样应该很好符合权重规定的比例关系），但个人感觉在点稍多的时候这并不是个问题。不过系统重采样有O(N)的复杂度，这比上面的O(Nlog(N))好。 **总结** 在线性和高斯噪声的假设下，卡尔曼滤波是最优估计，而粒子滤波则可处理非线性和无模型系统，不需要高斯和线性假设。但是粒子滤波的性能分析，稳定性分析都很困难，计算量也大，不过这也是处理和非线性有关问题时经常见到的。KF，UKF，EKF已经在工业上获得了很多实时在线应用的机会。随着计算成本的不断下降，未来粒子滤波类可能也会有不错的应用。\n13. 平滑器 卡尔曼滤波器是在线滤波方法，之利用历史信息，而对于离线滤波场景，使用平滑器可扩展到使用整个序列信息进行滤波，效果会更好。 固定区间平滑（Fixed-Interval Smoothing） 这类平滑器在接受到一个完整的区间后一并进行估计操作。RTS是这类中最主要的方法。RTS先运行标准卡尔曼滤波，得到所有的估计值X和对应系统协方差P。之后用X和P，按下面算法从第K步预测到第1步（即反向预测）： 预测： $$\\begin{aligned} \\mathbf{P} \u0026amp;= \\mathbf{FP}_k\\mathbf{F}^\\mathsf{T} + \\mathbf{Q } \\end{aligned}$$ 更新： $$\\begin{aligned} \\mathbf{K}_k \u0026amp;= \\mathbf{P}_k\\mathbf{F}^\\mathsf{T}\\mathbf{P}^{-1} \\\n\\mathbf{x}_k \u0026amp;= \\mathbf{x}_k + \\mathbf{K}k(\\mathbf{x}{k+1} - \\mathbf{Fx}_k) \\\n\\mathbf{P}_k \u0026amp;= \\mathbf{P}_k + \\mathbf{K}k(\\mathbf{P}{k+1} - \\mathbf{P})\\mathbf{K}_k^\\mathsf{T} \\end{aligned}$$ 效果显著： \r\r\r14. 自适应滤波 Adaptive Filtering旨在处理模型不匹配的情况，其检查到有当前过程模型无法描述的动态行为时可以自适应的调整。（回想在之前非自适应的滤波器中，我们只能增大Q以权衡模型的问题，但大的Q会使我们的滤波器不再进行最优估计） 我们建立一个ManeuveringTarget类来产生人工数据。ManeuveringTarget模拟了二维平面的小车，可以接受转向和加减速指令。另外设计个模拟传感器，吐出加噪声后的（x，y）。给一个转向的数据实例如下： \r\r\r下面我们尝试以匀速直线运动模型的卡尔曼滤波器对齐进行滤波，为方便起见只做x坐标。结果如下，可见由于转向导致的与模型不符，有了很大的估计误差，不过误差在观测的纠正下逐渐变小： \r\r\r为了解决这个，我们可以大幅加大Q，但这也导致了估计质量的下降： \r\r\r或，我们可以增大模型阶数，改为横加速度直线运动模型，同样可以有效解决滞后的问题，但依旧顾此失彼，估计噪声明显大了： \r\r\r因此，若我们能设计一种机制动态的改变滤波器模型，比如在恒速运动阶段使用低阶模型降低噪声，在转向等阶段使用高阶模型保证响应则有望较好的平衡矛盾。 我们可以观察滤波器里的残差来得知目标运动状态何时发生变化，如下： \r\r\r自适应思路1：可调过程噪声 第一种方法是使用低阶模型，并根据机动是否发生来调整过程噪声。当残差变得“大”时，我们将增加过程噪声。这将导致滤波器更倾向于测量而不是过程预测，并且滤波器将密切跟踪信号。当残差很小时，我们将缩小过程噪声。 一种连续调整的方法计算残差的归一化值$\\epsilon = \\mathbf{y^\\mathsf{T}S}^{-1}\\mathbf{y}$，其中S是过程噪声：$\\mathbf{S} = \\mathbf{HPH^\\mathsf{T}} + \\mathbf{R}$。我们可以设一个$\\epsilon_{max}$（可通过实验选择，一个不错的经验值是$\\epsilon$ 的4~5倍标准差），当$\\epsilon \\gt \\epsilon_{max}$时将Q矩阵成一个扩大系数$q_{factor}$即可。下图$\\epsilon_{max}=4， q_{factor}=1000$，可见显著的性能提升。 \r\r\r此外，还可计算$std = \\sqrt{\\mathbf{HPH}^\\mathsf{T} + \\mathbf{R}}$ 之后采用如下策略改变Q：当残差\u0026gt;std的某一倍数时，增大Q **自适应思路2：渐消记忆滤波器（Fading memory filters）** 此类方法优势不被归类为自适应滤波方法。此方法的关注点在卡尔曼滤波器的记忆性上。标准卡尔曼滤波使用了1~k-1的所有信息，但这可能导致过大的“惯性”，我们加入一个微小的修改，将原方法中$\\bar{\\mathbf P} = \\mathbf{FPF}^\\mathsf T + \\mathbf Q$更改为$\\tilde{\\mathbf P} = \\alpha^2\\mathbf{FPF}^\\mathsf T + \\mathbf Q$。其中α\u0026gt;1，但通常很接近1，比如1.02。这样做的考虑是若我们增加估计误差协方差，滤波器对估计的不确定性就会增加，因此它给测量增加了权重，从而减小了滤波器的惯性。此法的改动微乎其微，实现很方便，值得一试，虽然效果不是很惊艳。 \r\r\r**自适应思路3：多模型估计** 这是一种集成（ensemble ）的思路。最朴素的想法是搞一把模型，然后看情况切换，当然也可进行UKF,KF的集成或对不同的状态变量使用不同的模型或算法。 我们很容易实现的方法是对残差进行阈值判断，以此来切换模型，但这种硬切换显然会导致估计结果不连续的跳跃。因此，我们采用多模型自适应估计技术（MMAE），该方法按模型likelihoods给出融合结果。其中似然函数已在前述章节有过讨论：$\\mathcal{L} = \\frac{1}{\\sqrt{2\\pi S}}\\exp [-\\frac{1}{2}\\mathbf{y}^\\mathsf{T}\\mathbf{S}^{-1}\\mathbf{y}]$，其中y是残差，S是系统不确定性。之后按$p_k^i = \\frac{\\mathcal{L}_k^ip_{k-1}^i}{\\sum\\limits_{j=1}^N \\mathcal{L}_k^jp_{k-1}^j}$分配即可。此方法的一个缺陷是算法可能收敛到只信任某一最可能算法上去，这需要被注意，可以通过重新初始化选择权重进行修正。\n此外还有交互式多模型技术 IMM（Interacting Multiple Model），利用贝叶斯方法在多个模型间转移，但为了交互，其要求模型必需有相同的维数，这是个比较大的限制。\n完。\n","date":1559880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559880000,"objectID":"b3c791974c00e8584adc893749df2a59","permalink":"https://leidawt.github.io/en/post/kalman-and-bayesian-filters-in-python-%E5%8D%A1%E5%B0%94%E6%9B%BC%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%BB%A4%E6%B3%A2%E5%99%A8%E7%AC%94%E8%AE%B0/","publishdate":"2019-06-07T12:00:00+08:00","relpermalink":"/en/post/kalman-and-bayesian-filters-in-python-%E5%8D%A1%E5%B0%94%E6%9B%BC%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%BB%A4%E6%B3%A2%E5%99%A8%E7%AC%94%E8%AE%B0/","section":"post","summary":"此书从实践角度讲了卡尔曼等一系列贝叶斯滤波器，没有从线控视角入手，提供了大量直观解读和代码实例，看着玩玩摘录些重点 @[TOC] 1.g-h滤波器 又称Al","tags":[],"title":"Kalman and Bayesian Filters in Python （卡尔曼与贝叶斯滤波器）笔记","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"这本开源书从实践角度初步入门概率编程。值得学习的有： 1.大佬优秀的可视化技巧 2.TFP包基础 3.概率编程和贝叶斯思想\n书包含使用不同框架的版本，这里用TFP的版本 https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers\n一些util函数 此段作者定义了一些实用的函数并配置了绘图风格，应当首先执行\n#@title Imports and Global Variables (run this cell first) { display-mode: \u0026quot;form\u0026quot; } \u0026quot;\u0026quot;\u0026quot; The book uses a custom matplotlibrc file, which provides the unique styles for matplotlib plots. If executing this book, and you wish to use the book's styling, provided are two options: 1. Overwrite your own matplotlibrc file with the rc-file provided in the book's styles/ dir. See http://matplotlib.org/users/customizing.html 2. Also in the styles is bmh_matplotlibrc.json file. This can be used to update the styles in only this notebook. Try running the following code: import json s = json.load(open(\u0026quot;../styles/bmh_matplotlibrc.json\u0026quot;)) matplotlib.rcParams.update(s) \u0026quot;\u0026quot;\u0026quot; #!pip3 install -q wget from __future__ import absolute_import, division, print_function #@markdown This sets the warning status (default is `ignore`, since this notebook runs correctly) warning_status = \u0026quot;ignore\u0026quot; #@param [\u0026quot;ignore\u0026quot;, \u0026quot;always\u0026quot;, \u0026quot;module\u0026quot;, \u0026quot;once\u0026quot;, \u0026quot;default\u0026quot;, \u0026quot;error\u0026quot;] import warnings warnings.filterwarnings(warning_status) with warnings.catch_warnings(): warnings.filterwarnings(warning_status, category=DeprecationWarning) warnings.filterwarnings(warning_status, category=UserWarning) import numpy as np import os #@markdown This sets the styles of the plotting (default is styled like plots from [FiveThirtyeight.com](https://fivethirtyeight.com/)) matplotlib_style = 'fivethirtyeight' #@param ['fivethirtyeight', 'bmh', 'ggplot', 'seaborn', 'default', 'Solarize_Light2', 'classic', 'dark_background', 'seaborn-colorblind', 'seaborn-notebook'] import matplotlib.pyplot as plt plt.style.use(matplotlib_style) import matplotlib.axes as axes from matplotlib.patches import Ellipse %matplotlib inline import seaborn as sns; sns.set_context('notebook') from IPython.core.pylabtools import figsize #@markdown This sets the resolution of the plot outputs (`retina` is the highest resolution) notebook_screen_res = 'retina' #@param ['retina', 'png', 'jpeg', 'svg', 'pdf'] %config InlineBackend.figure_format = notebook_screen_res import tensorflow as tf tfe = tf.contrib.eager # Eager Execution #@markdown Check the box below if you want to use [Eager Execution](https://www.tensorflow.org/guide/eager) #@markdown Eager execution provides An intuitive interface, Easier debugging, and a control flow comparable to Numpy. You can read more about it on the [Google AI Blog](https://ai.googleblog.com/2017/10/eager-execution-imperative-define-by.html) use_tf_eager = False #@param {type:\u0026quot;boolean\u0026quot;} # Use try/except so we can easily re-execute the whole notebook. if use_tf_eager: try: tf.enable_eager_execution() except: pass import tensorflow_probability as tfp tfd = tfp.distributions tfb = tfp.bijectors def evaluate(tensors): \u0026quot;\u0026quot;\u0026quot;Evaluates Tensor or EagerTensor to Numpy `ndarray`s. Args: tensors: Object of `Tensor` or EagerTensor`s; can be `list`, `tuple`, `namedtuple` or combinations thereof. Returns: ndarrays: Object with same structure as `tensors` except with `Tensor` or `EagerTensor`s replaced by Numpy `ndarray`s. \u0026quot;\u0026quot;\u0026quot; if tf.executing_eagerly(): return tf.contrib.framework.nest.pack_sequence_as( tensors, [t.numpy() if tf.contrib.framework.is_tensor(t) else t for t in tf.contrib.framework.nest.flatten(tensors)]) return sess.run(tensors) class _TFColor(object): \u0026quot;\u0026quot;\u0026quot;Enum of colors used in TF docs.\u0026quot;\u0026quot;\u0026quot; red = '#F15854' blue = '#5DA5DA' orange = '#FAA43A' green = '#60BD68' pink = '#F17CB0' brown = '#B2912F' purple = '#B276B2' yellow = '#DECF3F' gray = '#4D4D4D' def __getitem__(self, i): return [ self.red, self.orange, self.green, self.blue, self.pink, self.brown, self.purple, self.yellow, self.gray, ][i % 9] TFColor = _TFColor() def session_options(enable_gpu_ram_resizing=True, enable_xla=True): \u0026quot;\u0026quot;\u0026quot; Allowing the notebook to make use of GPUs if they're available. XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that optimizes TensorFlow computations. \u0026quot;\u0026quot;\u0026quot; config = tf.ConfigProto() config.log_device_placement = True if enable_gpu_ram_resizing: # `allow_growth=True` makes it possible to connect multiple colabs to your # GPU. Otherwise the colab malloc's all GPU ram. config.gpu_options.allow_growth = True if enable_xla: # Enable on XLA. https://www.tensorflow.org/performance/xla/. config.graph_options.optimizer_options.global_jit_level = ( tf.OptimizerOptions.ON_1) return config def reset_sess(config=None): \u0026quot;\u0026quot;\u0026quot; Convenience function to create the TF graph \u0026amp; session or reset them. \u0026quot;\u0026quot;\u0026quot; if config is None: config = session_options() global sess tf.reset_default_graph() try: sess.close() except: pass sess = tf.InteractiveSession(config=config) reset_sess()  其中reset_sess()和evaluate()尤为关键，封装了sess操作便于直接得到结果 例：\n''' evaluate函数使用方法（其将tf_tensor执行session以转换到numpy格式） 注：采用以下的命名约定减小混乱：下划线结尾的变量表示转换到numpy的版本 ''' reset_sess()#构建或重启全局session parameter = tfd.Exponential(rate=1., name=\u0026quot;poisson_param\u0026quot;).sample()#从指数分布采样一个值（tensor） [ parameter_ ] = evaluate([ parameter ])#转换 print(\u0026quot;Sample from exponential distribution before evaluation: \u0026quot;, parameter) print(\u0026quot;Evaluated sample from exponential distribution: \u0026quot;, parameter_)  Sample from exponential distribution before evaluation: Tensor(\u0026ldquo;poisson_param/sample/Reshape:0\u0026rdquo;, shape=(), dtype=float32) Evaluated sample from exponential distribution: 0.3230632\nChapter 1 引言 概率领域有两大学派：频率派和贝叶斯派，其对概率的观点有所不同 频率派视概率是事件的长期频率。这在解释多次事件（如飞机坠毁）在中是没有问题的，但处理单次事件（如xxx竞选成功的概率）会面临问题。 贝叶斯派视概率为对事件发生信心的度量。这种观点解释单次事件就很直观和清晰了（如xxx竞选成功的概率就是对xxx成功竞选的信心）。 故实际上贝叶斯派对概率的看法是很符合人最初的直觉的，频率派的思想来自后期训练。 通常的，记我们对某事件的信度为P(A)，称先验概率（prior probability），观测到一些证据X后，看法会改变，形成P(A|X)，称后验概率（posterior probability）。有时候先验可能很错误或不包含什么有用的东西，不过通过观测证据的修正，我们得到的后验会变得更加正确。 贝叶斯推断和频率推断回答问题的方式有很大的不同，以bug测试问题举例说明： 1.频率派 问：我的代码通过了所有的X测试;我的代码没有bug吗? 答：没错误 2.贝叶斯派 问：我的代码经常有错，我的代码通过了所有的X测试;我的代码没有bug吗? 答：有80%的可能性没错误 这里有两大区别：一是后者回答可能性，前者回答估计结果。二是贝叶斯派还在体温在给出了可选的先验，让贝叶斯推断同时考虑一下“我的代码经常有错”这个先验条件。此处贝叶斯的优势在于其保持了在给出的证据不充分时回答的不确定性。 贝叶斯公式： \r\r\r这个简单的式子是贝叶斯推理的核心，其重要意义是构建了先验和后验之间的关系。 其中P(X|A)又称先验A的似然度，也写作L(A|X)。分母实则是个归一化常数，所以贝叶斯公式也经常写作$P(A|X)\\propto P(X|A)P(A)$。 贝叶斯推理简单实例 现在要从下图所示数据中分析出来人们的编码习惯是否与时间相关。可以注意到贝叶斯推理将参数视作随机变量的观点。 \r\r\r鉴于泊松分布适合于描述单位时间内随机事件发生的次数的概率分布，如某一服务设施在一定时间内受到的服务请求的次数，电话交换机接到呼叫的次数、汽车站台的候客人数、机器出现的故障数。故可以记第i天消息数 $C_i \\sim \\text{Poisson}(\\lambda)$。这里参数 $\\lambda$ 代表独立实事件的发生概率，是未知的。 可以从数据观察到后期消息数明显有提高，因此考虑如下假设： $$\\lambda = \\begin{cases} \\lambda_1 ; \\text{if } t \\lt \\tau \\cr \\lambda_2 ; \\text{if } t \\ge \\tau \\end{cases} $$ 即认为$\\lambda$是时变的。继续为 $\\lambda_1$ ，$\\lambda_2$建模。考虑到其连续性，使用了指数分布，$\\alpha$作为超参数（超参数准确含义是影响其他参数的参数）。 $$ \\lambda_1 \\sim \\text{Exp}( \\alpha ) \\\n\\lambda_2 \\sim \\text{Exp}( \\alpha ) $$ 最后由于不知道$\\tau$的任何先验知识，只好以均匀分布建模。即$\\tau \\sim \\text{DiscreteUniform(1,70) }$。至此模型假设已经全部定义完毕（通过指定先验分布）。接下来是求解，由于较为复杂，使用概率编程库来计算之。 概率编程库\u0026ndash;TensorFlow Probability 作为TensorFlow 的子项目，TFP的核心优势是数据计算性能的高度优化。TFP建模的关键在于定义描述模型结构的joint_log_prob函数，该函数以数据和欲求的模型参数为输入，返回联合概率的对数（即返回参数在数据中的似然度），这个似然度会用于后续采样算法。 上面定义的模型归纳如下： \r\r\r\r\r\rjoint_log_prob定义为：\ndef joint_log_prob(count_data, lambda_1, lambda_2, tau): tfd = tfp.distributions #指定超参数 alpha = np.array(1. / count_data.mean(), np.float32) #指明参数的分布函数 rv_lambda_1 = tfd.Exponential(rate=alpha) rv_lambda_2 = tfd.Exponential(rate=alpha) rv_tau = tfd.Uniform() lambda_ = tf.gather( [lambda_1, lambda_2], indices=tf.to_int32(tau * count_data.size \u0026lt;= np.arange(count_data.size))) rv_observation = tfd.Poisson(rate=lambda_) # 返回joint prob 因为取了log，故乘变加 return ( rv_lambda_1.log_prob(lambda_1) + rv_lambda_2.log_prob(lambda_2) + rv_tau.log_prob(tau) + tf.reduce_sum(rv_observation.log_prob(count_data)) )  然后经过MCMC采样算法（后续章节说明）的迭代计算，欲求参数的后验分布就能得到了，如下所示： \r\r\r可以观察到后验分布的结果保留了不确定性。\nChapter 2 TFP包 TensorFlow新版中加入了eager模式增加了对立即计算的支持。此书支持eager和非eager的模式，统一使用evaluate函数得到结果。\n分布函数 tfp.distributions包含大量常用分布类。https://www.tensorflow.org/probability/api_docs/python/tfp/distributions 这些分布类的关键概念是： Event shape：多变量概率分布的维数，5维就是[5] Batch shape：独立不同分布的个数 建立随机变量可用\nvar= tfd.Exponential(rate=1., name=\u0026quot;poisson_param\u0026quot;)  建立确定性变量可用\nnew_deterministic_variable = tfd.Deterministic(name=\u0026quot;deterministic_variable\u0026quot;, loc=var.sample())  所谓确定性变量既是取值固定的随机变量，为了抽象和统一接口方便\n贝叶斯A/B测试 这是一种类似本科概率论中假设检验的方法。下面以一个例子说明。 例：假设$P_A$表示某购物网站的 浏览-\u0026gt;购买 比例，$P_B$表示另一购物网站的 浏览-\u0026gt;购买 比例。现在希望从在两个网站观察到的 购买数 来估计真实概率$P_A$，$P_B$ 并推测谁更高。\n可以通过贝叶斯推理来估计$P_A$,$P_B$ 首先制作人工数据。\nreset_sess() #set constants prob_true = 0.05 # 指定P_A N = 1500 # 总浏览数（采样数） # 以 Ber(0.05) 为采样分布 occurrences = tfd.Bernoulli(probs=prob_true).sample(sample_shape=N, seed=10) print(evaluate(occurrences))  造出数据后可用joint_log_prob来表达数据和参数的匹配程度，如下：\ndef joint_log_prob(data,A_guess): rv_A_guess=tfd.Uniform(low=0.,high=1.) rv_data=tfd.Bernoulli(probs=A_guess) return ( rv_A_guess.log_prob(A_guess)+ tf.reduce_sum(rv_data.log_prob(data)) )  之后通过MCMC采样来从造出来的数据估计真实$P_A$。 MCMC采样方法不停尝试不同的A_guess，并调用joint_log_prob判断猜测的好坏最终得到$P_A$的后验估计。如下： \r\r\r可用看到中心在真实值0.05处，拖尾是由于不确定造成的。$P_B$的处理方法相同。 令$delta=P_A-P_B$。通过MCMC一并得到他们的后验估计结果： \r\r\r注：由于设置了B站的N为A的一半，故B的拖尾更大一些。\n最后，求np.mean(delta\u0026gt; 0))即可得到如下的推理结果：有0.706的可能A好于B。这便是A/B实验的基本方法。\n实例：挑战者号爆炸 数据如下： \r\r\r数据描述o型圈失效与温度的关系。 观察其形状，可考虑带偏置的logistic 函数描述数据： $$p(t) = \\frac{1}{ 1 + e^{ ;\\beta t + \\alpha } } $$ 接下来开始建模，目标是确定 $\\beta, \\alpha$ 。 可以用正态分布$N( \\mu, 1/\\tau)$ 作为 $\\beta, \\alpha$的先验分布。（原作者选了N(0,1/1000)，个人认为此次使用uniform（-100,100）可能更有效，N(0,1/1000)引入的先验过多且无依据）。 最后的数据预测可使用伯努利分布来做$$ \\text{Defect Incident, }D_i \\sim \\text{Ber}( ;p(t_i); ), ;; i=1..N$$ 综上，失效与否是以时变的伯努利分布确定的，其参数是以带偏置的logistic 函数确定的，logistic 的参数是求解目标，且以N(0,1/1000)为先验。 之后使用MCMC求解即可。下图画出了由 $\\beta, \\alpha$的后验确定的logistic函数。紫色区域是$\\beta, \\alpha$的后验分布的95%置信区间。 \r\r\r坠毁当天的t=31，用的到的推理结果进行预测，结果显示出问题几乎是必然的：\nplt.figure(figsize(12.5, 3)) prob_31 = logistic(31, posterior_beta_, posterior_alpha_) [ prob_31_ ] = evaluate([ prob_31 ]) plt.xlim(0.98, 1) plt.hist(prob_31_, bins=10, density=True, histtype='stepfilled') plt.title(\u0026quot;Posterior distribution of probability of defect, given $t = 31$\u0026quot;) plt.xlabel(\u0026quot;probability of defect occurring in O-ring\u0026quot;);  \r\r\r\r一个十分值得关注的问题是如何评价模型十分很适合数据？一个不错的方法是从模型采样人工数据并与真实数据对比。接下来作者提出了一种可视化这种对比的不错的新方法。 首先从模型求一下对应数据点的后验（左为后验，右为真实数据） posterior prob of defect | realized defect 0.47 | 0 0.20 | 1 0.25 | 0 0.31 | 0 \u0026hellip; 之后按后验概率进行排序 probb | defect 0.01 | 0 0.01 | 0 0.02 | 0 0.03 | 0 \u0026hellip; 最后调用如下的函数绘制\nimport matplotlib.pyplot as plt def separation_plot( p, y, **kwargs ): \u0026quot;\u0026quot;\u0026quot; This function creates a separation plot for logistic and probit classification. See http://mdwardlab.com/sites/default/files/GreenhillWardSacks.pdf p: The proportions/probabilities, can be a nxM matrix which represents M models. y: the 0-1 response variables. \u0026quot;\u0026quot;\u0026quot; assert p.shape[0] == y.shape[0], \u0026quot;p.shape[0] != y.shape[0]\u0026quot; n = p.shape[0] try: M = p.shape[1] except: p = p.reshape( n, 1 ) M = p.shape[1] colors_bmh = np.array( [\u0026quot;#eeeeee\u0026quot;, \u0026quot;#348ABD\u0026quot;] ) fig = plt.figure( ) for i in range(M): ax = fig.add_subplot(M, 1, i+1) ix = np.argsort( p[:,i] ) #plot the different bars bars = ax.bar( np.arange(n), np.ones(n), width=1., color = colors_bmh[ y[ix].astype(int) ], edgecolor = 'none') ax.plot( np.arange(n+1), np.append(p[ix,i], p[ix,i][-1]), \u0026quot;k\u0026quot;, linewidth = 1.,drawstyle=\u0026quot;steps-post\u0026quot; ) #create expected value bar. ax.vlines( [(1-p[ix,i]).sum()], [0], [1] ) plt.xlim( 0, n) plt.tight_layout() return plt.figure(figsize(11., 3)) separation_plot(posterior_probability_, D_)  结果如下： \r\r\r蛇形线表示排序后的概率，蓝色条表示缺陷，而空白表示非缺陷。随着概率的增加，我们看到越来越多的缺陷发生。在右侧，图中显示，当后验概率较大(直线接近1)时，缺陷会更多。这是良好的行为。理想情况下，所有的蓝条都应该靠近右边，偏离右边反映了预测失误。 使用这样的图标可以比较直观的评价模型，如： \r\r\r\r\r\rChapter 3 MCMC 马尔科夫链蒙特卡洛采样（MCMC）方法是用来在概率空间，通过随机采样估算兴趣参数的后验分布的。https://zhuanlan.zhihu.com/p/37121528。 其余的采用方法还有拉普拉斯近似和VAE等。\nMCMC的思路大致如下： 1.从当前位置开始。 2.考虑移动到一个新的位置(探索附近的点)。 3.根据位置对数据和先验分布的遵从程度接受/拒绝新位置。 4.如果接受了，那就移动到新位置。回到步骤1。 否则:不移动到新位置。回到步骤1。 5. 在大量迭代之后，返回所有接受的位置。\n一个使用混合模型的无监督聚类例子 目标：对形如下图的双峰数据进行无监督聚类。 \r\r\r拟使用的混合模型是$P(x|\\theta) = \\sum_{k=1}^{K}{\\alpha_{k}\\phi(x|\\theta_{k})}$。即k个分布的加权相加。当每个分布是高斯分布时候，就是常用的gmm（高斯混合模型）。 先制作形如上图的人工数据，方法如下： 设点属于$C_1$类的概率为p，$C_2$类的1-p，$p \\sim U(0,1)$。 设$\\sigma_1,\\sigma_2\\sim U(0,100);\\mu_0\\sim N(120,10);\\mu_1\\sim N(190,10)$ 其中120,190是目视观察的估计值。 模型的待求参数是p和两个高斯分布的均值方差\n此模型的联合概率函数定义如下，其中MixtureSameFamily函数是tfp提供加权和的函数，换为$p*N(120,10)+(1-p)*N(190,10)$意思一样。\ndef joint_log_prob(data_, sample_prob_1, sample_centers, sample_sds): \u0026quot;\u0026quot;\u0026quot; Joint log probability optimization function. Args: data: tensor array representation of original data sample_prob_1: Scalar representing probability (out of 1.0) of assignment being 0 sample_sds: 2d vector containing standard deviations for both normal dists in model sample_centers: 2d vector containing centers for both normal dists in model Returns: Joint log probability optimization function. \u0026quot;\u0026quot;\u0026quot; ### Create a mixture of two scalar Gaussians: rv_prob = tfd.Uniform(name='rv_prob', low=0., high=1.) sample_prob_2 = 1. - sample_prob_1 rv_assignments = tfd.Categorical(probs=tf.stack([sample_prob_1, sample_prob_2])) rv_sds = tfd.Uniform(name=\u0026quot;rv_sds\u0026quot;, low=[0., 0.], high=[100., 100.]) rv_centers = tfd.Normal(name=\u0026quot;rv_centers\u0026quot;, loc=[120., 190.], scale=[10., 10.]) rv_observations = tfd.MixtureSameFamily( mixture_distribution=rv_assignments, components_distribution=tfd.Normal( loc=sample_centers, # One for each component. scale=sample_sds)) # And same here. return ( rv_prob.log_prob(sample_prob_1) + rv_prob.log_prob(sample_prob_2) + tf.reduce_sum(rv_observations.log_prob(data_)) # Sum over samples. + tf.reduce_sum(rv_centers.log_prob(sample_centers)) # Sum over components. + tf.reduce_sum(rv_sds.log_prob(sample_sds)) # Sum over components. ) sum_log_prob = tf.reduce_sum(tf.concat(log_prob_parts, axis=-1), axis=-1) # Note: for easy debugging, uncomment the following: return sum_log_prob  接下来是重要的MCMC部分：\nnumber_of_steps=25000 #采用次数 burnin=1000 #前期丢弃样本数（因前期不准） # Set the chain's start state. initial_chain_state = [ tf.constant(0.5, name='init_probs'), tf.constant([120., 190.], name='init_centers'), tf.constant([10., 10.], name='init_sds') ] # Since MCMC operates over unconstrained space, we need to transform the # samples so they live in real-space. unconstraining_bijectors = [ tfp.bijectors.Identity(), # Maps R to R. tfp.bijectors.Identity(), # Maps R to R. tfp.bijectors.Identity(), # Maps R to R. ] # Define a closure over our joint_log_prob. unnormalized_posterior_log_prob = lambda *args: joint_log_prob(data_, *args) # Initialize the step_size. (It will be automatically adapted.) with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE): step_size = tf.get_variable( name='step_size', initializer=tf.constant(0.5, dtype=tf.float32), trainable=False, use_resource=True ) # Defining the HMC hmc=tfp.mcmc.TransformedTransitionKernel( inner_kernel=tfp.mcmc.HamiltonianMonteCarlo( target_log_prob_fn=unnormalized_posterior_log_prob, num_leapfrog_steps=2, step_size=step_size, step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(), state_gradients_are_stopped=True), bijector=unconstraining_bijectors) # Sample from the chain. [ posterior_prob, posterior_centers, posterior_sds ], kernel_results = tfp.mcmc.sample_chain( num_results=number_of_steps, num_burnin_steps=burnin, current_state=initial_chain_state, kernel=hmc) # Initialize any created variables. init_g = tf.global_variables_initializer() init_l = tf.local_variables_initializer() evaluate(init_g) evaluate(init_l) [ posterior_prob_, posterior_centers_, posterior_sds_, kernel_results_ ] = evaluate([ posterior_prob, posterior_centers, posterior_sds, kernel_results ]) new_step_size_initializer_ = kernel_results_.inner_results.is_accepted.mean() print(\u0026quot;acceptance rate: {}\u0026quot;.format( new_step_size_initializer_)) new_step_size_initializer_ print(\u0026quot;final step size: {}\u0026quot;.format( kernel_results_.inner_results.extra.step_size_assign[-100:].mean()))  最后结果如下（两个分布的参数是取的mcmc给出的后验分布的均值）： \r\r\r最终，为了判断新的样本点应该属于哪一个类，可以使用贝叶斯公式将 $P(L_x = 1| x = 175 ) \\gt P(L_x = 0| x = 175 )$ 转为 $P( x=175 | L_x = 1 )P( L_x = 1 ) \\gt P( x=175 | L_x = 0 )P( L_x = 0 )$ 最后一个小问题：如何判断收敛？ 可以考虑相关性。 相关系数是一个数，定义为两个变量之间的协方差和标准差的商（这里是皮尔逊相关系数）： $${\\displaystyle \\rho _{X,Y}={\\mathrm {cov} (X,Y) \\over \\sigma _{X}\\sigma {Y}}={E[(X-\\mu {X})(Y-\\mu {Y})] \\over \\sigma {X}\\sigma {Y}}}$$ 互相关cross-correlation是一个算子，描述两个序列相似程度，f,g之间的互相关以$f\\star g$表示。$f\\star g = \\overline{f(-t)}\\ast g(t)$ 上划线表共轭。故其与卷积很类似，当只考虑实数时，卷积就是$f(t)\\star g(-t)$。 自相关Autocorrelation即$f\\star f$。 在统计量中，一个随机过程的自相关是该过程在不同时间的值之间的皮尔逊相关，作为两个时间间隔或时滞的函数。${\\displaystyle \\operatorname {R} {XX}(t{1},t{2})=\\operatorname {E} [X{t{1}}{\\overline {X{t{2}}}}]} {\\displaystyle }$。具体的： http://nanshu.wang/post/2015-03-15/ \r\r\rpytho中计算1-n阶自相关的方法是：\ndef autocorr(x): # from http://tinyurl.com/afz57c4 result = np.correlate(x, x, mode='full') result = result / np.max(result) return result[result.size // 2:]  例如对两个随机过程$$x_t \\sim \\text{Normal}(0,1), ;; x_0 = 0$$$$y_t \\sim \\text{Normal}(y_{t-1}, 1 ), ;; y_0 = 0$$ 其自相关如下： \r\r\r可以看到时间间隔越远相关性越差。MCMC算法的特点是探索附近的值，其返回的点与前一次的位置相关。因此当MCMC链开始呈现很高的自相关时，表明不再很好的探索周边区域。（MCMC的收敛是指收敛到一个区域，而非通常意义上收敛到某一点）。低自相关不是收敛的必要条件，但它是充分的。TFP有一个内置的自相关工具。\nChapter 4 大数定律： $$\\frac{1}{N} \\sum_{i=1}^N Z_i \\rightarrow E[ Z ], ;;; N \\rightarrow \\infty.$$ 随着N的增长，收敛会放缓，其斜率约为$\\frac{ \\sqrt{ ; Var(Z) ; } }{\\sqrt{N} }$： \r\r\r在贝叶斯推理中，要特别主要N不大时大数定律的失效。 **例子：Kaggle的美国人口普查回复率挑战** 任务目标是利用人口普查变量(收入中位数、社区女性人数、拖车停车场数量、儿童平均人数等)预测一个社区的人口普查信件回复率，其测量值在0到100之间。数据是区块给出的（每社区人口数不同，约几千），原始数据如下： \r\r\r可以观察到呈现典型的三角形，随着区块样本量增大，回复率方差收紧，这是大数定律的典型体现。 **例子：如何给reddit帖子排序** 给出了按照赞踩比进行帖子排序的方法。真实的赞踩比是个隐变量，可以对其进行推理。直接使用观测的赞踩比作为估计量并不是好主意，其未考虑样本量的问题而违反大数定律。使用如下的模型：$p\\sim U(0,1),observations\\sim Binomial(p,N)$。爬取了一些帖子，使用MCMC采样出赞踩比的后验。下图虚线是95%置信下限。 也有解析的解法，快得多： $$ \\frac{a}{a + b} - 1.65\\sqrt{ \\frac{ab}{ (a+b)^2(a + b +1 ) } }$$ 其中$$ a = 1 + u \\\nb = 1 + d \\\nu=赞数 \\\nd=踩数 \\\n$$ \r\r\r这样充分考虑了样本量的问题。不考虑样本量和试图对不稳定的对象进行排序会导致病态的排序。\nchapter 5 损失函数 一些损失函数设计的例子： 简单的均方损失函数：$L( \\theta, \\hat{\\theta} ) = f( \\theta, \\hat{\\theta} )$，对过大的错误惩罚很大。 不平衡均方损失函数，可进行偏好性惩罚： $$ L( \\theta, \\hat{\\theta} ) = \\begin{cases} ( \\theta - \\hat{\\theta} )^2 \\hat{\\theta} \\lt \\theta \\\\ c( \\theta - \\hat{\\theta} )^2 \\hat{\\theta} \\ge \\theta, ;; 0\\lt c \\lt 1 \\end{cases}$$ 绝对值损失函数，对大偏差的惩罚不那么极端：$L( \\theta, \\hat{\\theta} ) = | \\theta - \\hat{\\theta} |$ 0-1损失函数，常用于分类任务：$L( \\theta, \\hat{\\theta} ) = \\mathbb{1}{ \\hat{\\theta} \\neq \\theta }$ log损失函数，常用于分类：$L( \\theta, \\hat{\\theta} ) = -\\theta\\log( \\hat{\\theta} ) - (1- \\theta)\\log( 1 - \\hat{\\theta} ), ; ; \\theta \\in {0,1}, ; \\hat{\\theta} \\in [0,1]$ 改进的绝对误差：$L( \\theta, \\hat{\\theta} ) = \\frac{ | \\theta - \\hat{\\theta} | }{ \\theta(1-\\theta) }, ; ; \\hat{\\theta}, \\theta \\in [0,1]$ 这个很有意思，强调对接近0和1的真值的预测。 指数损失函数：$L( \\theta, \\hat{\\theta} ) = 1 - \\exp \\left( -(\\theta - \\hat{\\theta} )^2 \\right)$ ，其值在（0,1）区间，大误差时饱和，故对很大的预测偏离不很在意。 \r\r\r在贝叶斯推断中，损失函数定义在分布上，而非分布的某些采用值： $$ l(\\hat{\\theta} ) = E{\\theta}\\left[ ; L(\\theta, \\hat{\\theta}) ; \\right] $$ 根据大数定律： $$\\frac{1}{N} \\sum_{i=1}^N ;L(\\theta_i, \\hat{\\theta} ) \\approx E_{\\theta}\\left[ ; L(\\theta, \\hat{\\theta}) ; \\right] = l(\\hat{\\theta} ) $$ 根据任务目标合理设计loss很重要。 **例子：金融预测** 人工数据集如下： $X\\sim N(0,1)*0.025,Y\\sim 0.5X+0.01N(0,1)$,采样100点 \r\r\r模型定义：$R = \\alpha + \\beta x + \\epsilon$，参数先验均为N(0,1) 通过MCMC采样得到三个参数的后验分布。下面定义一个stock_loss并以其为依据给出最合适的预测。 stock_loss形如：\ndef stock_loss(true_return, yhat, alpha=100.): \u0026quot;\u0026quot;\u0026quot; Stock Loss function Args: true_return: float32 Tensor representing the true stock return yhat: float32 alpha:float32 Returns: float: absolute value of the difference between `true_return` and `yhat` \u0026quot;\u0026quot;\u0026quot; if true_return * yhat \u0026lt; 0: # opposite signs, not good return alpha * yhat ** 2 - tf.sign(true_return) * yhat \\ + tf.abs(true_return) else: return tf.abs(true_return - yhat)  接下来是简单的优化问题： $$R_i(x) = \\alpha_i + \\beta_ix + \\epsilon \\ \\arg \\min_{r} ;;E_{R(x)}\\left[ ; L(R(x), r) ; \\right] $$ 此处R(x)是一个分布（因，$\\alpha$等都是分布，故就是个随机变量函数的分布。编程上就是一堆采样值），r是预测（一个标量的数），L是上面的stock_loss。目标是找使得期望loss最小的预测r。具体argmin是用scipy的fmin做的，其实现了奈德-米德单纯形算法。 最终的结果如下： \r\r\r可以看出，在stock_loss下做出的预测并不是绝对精度最优，而对stock_loss最优。在x=0附近，因为采样结果很不明确，倾向于给出接近0的预测。作者称之为稀疏的预测，在不很确定时干脆就不行动，这在真实的决策过程中是非常合理并符合人类直觉的。\n一个很有有价值的例子：kaggle暗物质挑战赛 这是一个由给出的星图来判断暗物质位置的任务。任务数据量较小，百量级，冠军方案使用贝叶斯推理。 给出的数据形如下。如图是一片天空。其中的蓝色椭圆形小点代表星系，收到暗物质团的巨大质量的影响，时空弯曲，导致原本随机均匀分布的星系的位置和偏心率受到影响。 \r\r\r数据以csv文件给出，一张星图一个文件，每个文件包含了数百个星系的位置和偏心率描述，每张星图有1\u0026ndash;3个暗物质（1大2小）。偏心率e1 e2描述如下图，e1描述横向拉伸，e2描述45°方向拉伸。 \r\r\r竞赛冠军Tim Salimans使用贝叶斯推理方法进行了如下的建模思路。 以贝叶斯公式为基础，为了获得p(x|e)，即知道星系分布e的情况下暗物质x的分布。应用贝叶斯公式倒转依赖。$p(x|e)\\propto p(e|x)p(x)$。因为是x影响e的，故求取p(x|e)要在逻辑上容易得多。p(x)也不难通过假设确定。 具体的，考虑暗物质x的 坐标(x,y) 和 质量m，赋予如下先验 $$ x_i \\sim \\text{Uniform}( 0, 4200)\\\ny_i \\sim \\text{Uniform}( 0, 4200), ;; i=1,2,3\\\nm_{\\text{large} } = \\text{Uniform}( 40, 180 ) \\\nm_{\\text{small} } = 20 \\\n$$ 下面设定p(e|x)，设 $$ e_i | ( \\mathbf{x}, \\mathbf{y} ) \\sim \\text{Normal}( \\sum_{j = \\text{halo positions} }d_{i,j} m_j f( r_{i,j} ), \\sigma^2 ) $$ 来描述星系e在受暗物质x影响下的分布。 其中f项体现暗物质距离的影响。 大暗物质:\n$$ f( r_{i,j} ) = \\frac{1}{\\min( r_{i,j}, 240 ) } $$ 小暗物质\n$$ f( r_{i,j} ) = \\frac{1}{\\min( r_{i,j}, 70 ) } $$ $r_{i,j}$ 是欧氏距离\nd项体现方位的影响。 其计算很奇葩，返回$\\sin(2\\arctan(\\frac{\\Delta y}{\\Delta x}))$和$\\cos(2\\arctan(\\frac{\\Delta y}{\\Delta x}))$\nm项就是暗物质的质量 方差项直接设为星图的观测偏心率，约0.05\n使用星图数据data以MCMC方法处理p(e|x)，得到x 位置和质量的后验分布（先只考虑最大的暗物质）。注：此处的位置后验是以MCMC采样的后验分布的均值和方差绘制的。 如下是后验的绘制及其与真实值对比 \r\r\r推广到3个。（这里直接绘制MCMC给出的后验） \r\r\r方案最后提交的结果是后验均值，并获得了不错的效果。\n方案总结： 1.此方案并未直接使用给定的暗物质真实位置进行监督学习，真实位置只用于评价性能，起到验证集作用。 2.先验分布很精简，可避免过拟合 3.用了很多小trick，比如min(dis,240)钳位操作能有效避免极端值的影响。又比如观察到数据普遍由一个大两个小组成，因此考虑进行了区别对待。 4.此方案体现了精心设计的贝叶斯推断方法在小数据中的强大性能。\nchapter 6 先验 略\n","date":1557374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557374400,"objectID":"477022eaa14461b93117c000580867e6","permalink":"https://leidawt.github.io/en/post/bayesian-methods-for-hackers%E7%AC%94%E8%AE%B0/","publishdate":"2019-05-09T12:00:00+08:00","relpermalink":"/en/post/bayesian-methods-for-hackers%E7%AC%94%E8%AE%B0/","section":"post","summary":"这本开源书从实践角度初步入门概率编程。值得学习的有： 1.大佬优秀的可视化技巧 2.TFP包基础 3.概率编程和贝叶斯思想 书包含使用不同框架的版本","tags":[],"title":"Bayesian Methods for Hackers笔记","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"@[TOC] 本文的相关数据集，代码见文末百度云\n贝叶斯网络简介 贝叶斯网络是一种置信网络，一个生成模型。（判别模型，生成模型的区分可以这样：回答p(label|x)即样本x属于某一类别的可能的，就是判别模型，而回答p(x,label) 和p(x|label)的，即回答在给定的类别中找样本x及样本分布情况的，即为生成模型。生成模型给出的联合分布比判别网络能给出更多的信息，对其求边缘分布即可得p(label|x) p(x|label)）同时贝叶斯网络还是一个简单的白盒网络，提供了高可解释性的可能。相比于大热的几乎无所不能的深度神经网络，贝叶斯网络仍有他的优势和应用场景。比如在故障分析，疾病诊断里，我们不仅需要回答是不是，更重要的是回答为什么，并给出依据。这样的场景下，以贝叶斯网络为代表的一些可解释好的白盒网络更加有优势。\n贝叶斯推断思路 与频率派直接从数据统计分析构建模型不同，贝叶斯派引入一个先验概率，表达对事件的已有了解，然后利用观测数据对先验知识进行修正，如通常把抛硬币向上的概率认为是0.5，这是个很朴素的先验知识，若是实验结果抛出了500-500的结果，那么证明先验知识是可靠的，合适的，若是出现100-900结果，那么先验知识会被逐渐修改（越来越相信这是个作弊硬币），当实验数据足够多的时候，先验知识就几乎不再体现，这时候得到与频率派几乎相同的结果。如图 \r\r\r具体例子推导可见\rhere\r贝叶斯网络 贝叶斯网络结构如下所示，其是有特征节点和链接构成的有向无环图。节点上是概率P(A),P(B)\u0026hellip; 连接上是条件概率P(A|B) P(A|C) \u0026hellip; 即若有A指向B的连接，则连接代表的就应为P(B|A)，更多信息可参考以下内容，这里不再赘述，贝叶斯网络结构本身不困难，其难点主要在于推理算法等数值计算问题，如为应用则无需深究。 贝叶斯网络发展及其应用综述\r《贝叶斯网络引论》@张连文 静态贝叶斯网络\r\r\r\r贝叶斯网络的实现 相关工具一直很丰富，matlab，R上都有成熟的工具。这里使用了python下的pgmpy，轻量好用，不像pymc那样容易安装困难。 安装： conda install -c ankurankan pgmpy 或 pip install pgmpy\n应用步骤 1.先确定以那些变量（特征）为节点，这里还包括由特征工程特征选择之类的工作。当然若有专业知识的参与会得到更合理的特征选择。 2.确定网络结构（拓扑）用以反应变量节点之间的依赖关系。也就是明确图的结构。这里既可以在有专家参与的情况下手工设计，也可以自动找到高效合适的网络，称为结构学习。贝叶斯网络的结构对最终网络性能很关键，若是构建所谓全连接贝叶斯网（即各个变量间两两相连），虽没有遗漏关联，但会导致严重的过拟合，因为数据量很难支撑起全连接直接海量的条件概率。 3.明确每条边上的条件概率。和结构一样，参数也可由专家手工确定（先验），亦可通过数据自动学习（即参数学习），或两者同时进行。 下面以一个经典数据集为例展示如何利用pgmpy包进行贝叶斯网络建模\n泰坦尼克数据集背景介绍 ref:https://www.jianshu.com/p/9b6ee1fb7a60 https://www.kaggle.com/c/titanic 这是kaggle经典数据集，主要是让参赛选手根据训练集中的乘客数据和存活情况进行建模，进而使用模型预测测试集中的乘客是否会存活。乘客特征总共有11个，以下列出。这个数据集特征明确，数据量不大，很适合应用贝叶斯网络之类的模型来做，目前最好的结果是正确率应该有80+%（具体多少因为答案泄露不好讲了）\nPassengerId =\u0026gt; 乘客ID Pclass =\u0026gt; 客舱等级(1/2/3等舱位) Name =\u0026gt; 乘客姓名 Sex =\u0026gt; 性别 Age =\u0026gt; 年龄 SibSp =\u0026gt; 兄弟姐妹数/配偶数 Parch =\u0026gt; 父母数/子女数 Ticket =\u0026gt; 船票编号 Fare =\u0026gt; 船票价格 Cabin =\u0026gt; 客舱号 Embarked =\u0026gt; 登船港口 在开始建模之前，先进行下特征工程，处理原始数据集的缺项等。这里前面处理主要采用https://www.jianshu.com/p/9b6ee1fb7a60的方法（他应用pandas清理数据的技巧很值得一学），我在他的处理后，进一步进行了一些离散化处理，以使得数据符合贝叶斯网络的要求（贝叶斯网络也有支持连续变量的版本，但因为推理，学习的困难，目前还用的很少），最后保留5个特征。\n''' PassengerId =\u0026gt; 乘客ID Pclass =\u0026gt; 客舱等级(1/2/3等舱位) Name =\u0026gt; 乘客姓名 Sex =\u0026gt; 性别 清洗成male=1 female=0 Age =\u0026gt; 年龄 插补后分0,1,2 代表 幼年（0-15） 成年（15-55） 老年（55-） SibSp =\u0026gt; 兄弟姐妹数/配偶数 Parch =\u0026gt; 父母数/子女数 Ticket =\u0026gt; 船票编号 Fare =\u0026gt; 船票价格 经聚类变0 1 2 代表少 多 很多 Cabin =\u0026gt; 客舱号 清洗成有无此项，并发现有的生存率高 Embarked =\u0026gt; 登船港口 清洗na,填S ''' # combine train and test set. train=pd.read_csv('./train.csv') test=pd.read_csv('./test.csv') full=pd.concat([train,test],ignore_index=True) full['Embarked'].fillna('S',inplace=True) full.Fare.fillna(full[full.Pclass==3]['Fare'].median(),inplace=True) full.loc[full.Cabin.notnull(),'Cabin']=1 full.loc[full.Cabin.isnull(),'Cabin']=0 full.loc[full['Sex']=='male','Sex']=1 full.loc[full['Sex']=='female','Sex']=0 full['Title']=full['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip()) nn={'Capt':'Rareman', 'Col':'Rareman','Don':'Rareman','Dona':'Rarewoman', 'Dr':'Rareman','Jonkheer':'Rareman','Lady':'Rarewoman','Major':'Rareman', 'Master':'Master','Miss':'Miss','Mlle':'Rarewoman','Mme':'Rarewoman', 'Mr':'Mr','Mrs':'Mrs','Ms':'Rarewoman','Rev':'Mr','Sir':'Rareman', 'the Countess':'Rarewoman'} full.Title=full.Title.map(nn) # assign the female 'Dr' to 'Rarewoman' full.loc[full.PassengerId==797,'Title']='Rarewoman' full.Age.fillna(999,inplace=True) def girl(aa): if (aa.Age!=999)\u0026amp;(aa.Title=='Miss')\u0026amp;(aa.Age\u0026lt;=14): return 'Girl' elif (aa.Age==999)\u0026amp;(aa.Title=='Miss')\u0026amp;(aa.Parch!=0): return 'Girl' else: return aa.Title full['Title']=full.apply(girl,axis=1) Tit=['Mr','Miss','Mrs','Master','Girl','Rareman','Rarewoman'] for i in Tit: full.loc[(full.Age==999)\u0026amp;(full.Title==i),'Age']=full.loc[full.Title==i,'Age'].median() full.loc[full['Age']\u0026lt;=15,'Age']=0 full.loc[(full['Age']\u0026gt;15)\u0026amp;(full['Age']\u0026lt;55),'Age']=1 full.loc[full['Age']\u0026gt;=55,'Age']=2 full['Pclass']=full['Pclass']-1 from sklearn.cluster import KMeans Fare=full['Fare'].values Fare=Fare.reshape(-1,1) km = KMeans(n_clusters=3).fit(Fare) #将数据集分为2类 Fare = km.fit_predict(Fare) full['Fare']=Fare full['Fare']=full['Fare'].astype(int) full['Age']=full['Age'].astype(int) full['Cabin']=full['Cabin'].astype(int) full['Pclass']=full['Pclass'].astype(int) full['Sex']=full['Sex'].astype(int) #full['Survived']=full['Survived'].astype(int) dataset=full.drop(columns=['Embarked','Name','Parch','PassengerId','SibSp','Ticket','Title']) dataset.dropna(inplace=True) dataset['Survived']=dataset['Survived'].astype(int) #dataset=pd.concat([dataset, pd.DataFrame(columns=['Pri'])]) train=dataset[:800] test=dataset[800:] ''' 最后保留如下项目,并切出800的训练集： Pclass =\u0026gt; 客舱等级(0/1/2等舱位) Sex =\u0026gt; 性别 male=1 female=0 Age =\u0026gt; 年龄 插补后分0,1,2 代表 幼年（0-15） 成年（15-55） 老年（55-） Fare =\u0026gt; 船票价格 经聚类变0 1 2 代表少 多 很多 Cabin =\u0026gt; 客舱号 清洗成有无此项，并发现有的生存率高 '''  模型结构搭建 这里先手动设计网络结构。 凭借对数据的理解，先设计如下的结构\r\r\r\rfrom pgmpy.models import BayesianModel from pgmpy.estimators import BayesianEstimator #model = BayesianModel([('Age', 'Pri'), ('Sex', 'Pri'),('Pri','Survived'),('Fare','Pclass'),('Pclass','Survived'),('Cabin','Survived')]) model = BayesianModel([('Age', 'Survived'), ('Sex', 'Survived'),('Fare','Pclass'),('Pclass','Survived'),('Cabin','Survived')])  其中(\u0026lsquo;Age\u0026rsquo;, \u0026lsquo;Survived\u0026rsquo;)表示Age指向Survived\npgmpy没有提供可视化，这里简单用graphviz实现了一下。\ndef showBN(model，save=False): '''传入BayesianModel对象，调用graphviz绘制结构图，jupyter中可直接显示''' from graphviz import Digraph node_attr = dict( style='filled', shape='box', align='left', fontsize='12', ranksep='0.1', height='0.2' ) dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\u0026quot;12,12\u0026quot;)) seen = set() edges=model.edges() for a,b in edges: dot.edge(a,b) if save: dot.view(cleanup=True) return dot showBN(model)  模型参数构建 接下来就是确定网络的参数，也就是各个边上的条件概率。 若手工填入，可这样写 \r\r\rfrom pgmpy.factors.discrete import TabularCPD #构建cpd表 my_cpd= TabularCPD(variable='Pclass', variable_card=3, values=[[0.65, 0.3], [0.30, 0.6],[0.05,0.1]], evidence=['Fare'], evidence_card=[2]) # 填cpd表 model.add_cpds(my_cpd) # 执行检查（可选，用于检查cpd是否填错） cancer_model.check_model()  但在此例子里，使用参数学习的办法从数据里自动学习。 参数学习有两种典型方法，极大似然估计和贝叶斯估计。因为前者的过拟合严重，一般都使用后者进行参数学习。pgmpy提供的贝叶斯估计器提供三种先验分布的支持，‘dirichlet’, ‘BDeu’, ‘K2’，实际上都是dirichlet分布，这里解释下贝叶斯估计器的工作原理。\n贝叶斯估计器 在贝叶斯分析的框架下，待求参数θ被看做是随机变量，对他的估计就是在其先验上，用数据求后验，因此先要有对θ的先验假设。而我们通常取的先验分布就是dirichlet（狄利克雷）分布。对于一个含有i个离散状态的节点，我们设其参数为\r\r\r\r并令其先验为狄利克雷分布D[α1，α2\u0026hellip;αi]（i=2时也称beta分布）\n\r\r\r\r这个先验有i个参数，数学上证明了，这些参数就相当于将先验表达成了α个虚拟样本，其中满足X=xi的样本数为αi，这个α就成为等价样本量（equivalent_sample_size）。这个巧合其实正式先验函数取这个函数的缘由，另外，其计算后的后验分布也是狄利克雷分布（称这种行为叫共轭先验）。注：各个分布可参考 https://zhuanlan.zhihu.com/p/37976562 至此可以理解pgmpy提供的贝叶斯估计器的参数的含义了。 其定义为estimate_cpd(node, prior_type='BDeu\u0026rsquo;, pseudo_counts=[], equivalent_sample_size=5) node是节点名 当prior_type='BDeu\u0026rsquo; 就表示选择了一个equivalent_sample_size=5的无差别客观先验，认定各个概率相等，不提供信息，但并不是没用，这个先验起到了类似神经网络里头控制过拟合的正则项的作用。 当prior_type='dirichlet'表示选择一般的狄利克雷分布，这时候要主动填入[α1，α2\u0026hellip;αi] 当prior_type= ‘K2’ 意为 ‘dirichlet’ + setting every pseudo_count to 1 具体使用如下：\ndata = pd.DataFrame(data={'A': [0, 0, 1], 'B': [0, 1, 0], 'C': [1, 1, 0]}) model = BayesianModel([('A', 'C'), ('B', 'C')]) estimator = BayesianEstimator(model, data) cpd_C = estimator.estimate_cpd('C', prior_type=\u0026quot;dirichlet\u0026quot;, pseudo_counts=[1, 2]) model.add_cpds(cpd_C)  上面是一个一个填进去的，在本例中有更简单的方法，就是利用提供的fit函数，一并估计各个cpd（条件概率），即\nmodel.fit(train, estimator=BayesianEstimator, prior_type=\u0026quot;BDeu\u0026quot;) # default equivalent_sample_size=5  直接把前面处理得到dataframe 传入即可。这里记录一个bug：pgmpy目前将离散变量命名限制为从0开始，所以本例子里的Pclass 项从（1/2/3等级）都减一处理成了（0/1/2等级）以解决此问题。 dirichlet也可在fit函数里使用，只要传入pseudo_counts字典即可，如下面这样\npseudo_counts = {'D': [300, 700], 'I': [500, 500], 'G': [800, 200], 'L': [500, 500], 'S': [400, 600]} model.fit(data, estimator=BayesianEstimator, prior_type='dirichlet', pseudo_counts=pseudo_counts)  到此为止，模型已经完全构建完毕，下面可以开始使用其进行推理了。\n推理 首先可以通过一些方法查看模型\n#输出节点信息 print(model.nodes()) #输出依赖关系 print(model.edges()) #查看某节点概率分布 print(model.get_cpds('Pclass').values)  当然我们更关心的是给定某些节点后，感兴趣节点的概率等，这就是推理。 贝叶斯网络推理分成： 1.后验概率问题： 表达为求P（Q|E=e） 其中Q为查询变量 E为证据变量 即如本例子里已知一个人，女，\u0026lt;15岁，高票价，问生还几率是多少？ 2.最大后验假设问题（MAP）： \r\r\r已知证据E时，对某些变量的转态组合感兴趣（称假设变量H），找最可能组合就是最大后验假设问题。如本例子里，问一个活下来的女乘客最可能有是什么舱段，年龄？ 3.最大可能解释问题（MPE）：是2的特例，即假设包含网络里所有非证据变量（同时也可包含证据变量）\n贝叶斯网络推理主要有两类方法，精确推理（变量化简Variable Elination和置信传播）和近似推理(如mcmc采样)，一般精确推理足以解决 pgmpy解决1可以用query函数 解决2，3可以用map_query函数 通过这些查询可以获得我们感兴趣的关于因果关系信息，这是贝叶斯网络模型的一大优势。此处的因果关系并不可以解释为一般意义上的逻辑因果，而是表示一种概率上的相关，比如我们不能将P(天亮了|公鸡打鸣)很高解释为是因为公鸡打鸣天才亮的。\nfrom pgmpy.inference import VariableElimination model_infer = VariableElimination(model) q = model_infer.query(variables=['Survived'], evidence={'Fare': 0}) print(q['Survived']) ''' +------------+-----------------+ | Survived | phi(Survived) | +============+=================+ | Survived_0 | 0.6341 | +------------+-----------------+ | Survived_1 | 0.3659 | +------------+-----------------+ ''' q = model_infer.map_query(variables=['Fare','Age','Sex','Pclass','Cabin'], evidence={'Survived': 1}) print(q)#{'Sex': 0, 'Fare': 0, 'Age': 1, 'Pclass': 2, 'Cabin': 0}  上面的代码使用了VariableElimination方法，亦可用BeliefPropagation，其有相同的接口。\n与fit函数类似，也提供了输入dataframe的简便推理方法predict，如下 只要剔除想预测的列输入predict函数，就可以得到预测结果的dataframe\npredict_data=test.drop(columns=['Survived'],axis=1) y_pred = model.predict(predict_data) print((y_pred['Survived']==test['Survived']).sum()/len(test)) #测试集精度0.8131868131868132  只是用了泰坦尼克数据集的一部分特征随手设计网络就可以达到不错的效果了，精度81.3%，上传kaggle的正确率0.77990。\n自动设计网络结构-\u0026gt;使用结构学习方法 ref： https://github.com/pgmpy/pgmpy_notebook 《贝叶斯网络引论》\n自动设计网络结构的核心问题有两个，一个是评价网络好坏的指标，另一个是查找的方法。穷举是不可取的，因为组合数太大，只能是利用各种启发式方法或是限定搜索条件以减少搜索空间，因此产生两大类方法，Score-based Structure Learning与constraint-based structure learning 以及他们的结合hybrid structure learning。 1.Score-based Structure Learning 这个方法依赖于评分函数，常用的有bdeu k2 bic，更合理的网络评分更高，如下面的例子 此例子随机产生x y 并令z=x+y，显然X -\u0026gt; Z \u0026lt;- Y的结构合理\nimport pandas as pd import numpy as np from pgmpy.estimators import BdeuScore, K2Score, BicScore from pgmpy.models import BayesianModel # create random data sample with 3 variables, where Z is dependent on X, Y: data = pd.DataFrame(np.random.randint(0, 4, size=(5000, 2)), columns=list('XY')) data['Z'] = data['X'] + data['Y'] bdeu = BdeuScore(data, equivalent_sample_size=5) k2 = K2Score(data) bic = BicScore(data) model1 = BayesianModel([('X', 'Z'), ('Y', 'Z')]) # X -\u0026gt; Z \u0026lt;- Y model2 = BayesianModel([('X', 'Z'), ('X', 'Y')]) # Y \u0026lt;- X -\u0026gt; Z print(bdeu.score(model1)) print(k2.score(model1)) print(bic.score(model1)) print(bdeu.score(model2)) print(k2.score(model2)) print(bic.score(model2)) ''' -13936.101051153362 -14326.88012027081 -14292.1400887 -20902.744280734016 -20929.567083476162 -20946.7926535 '''  X -\u0026gt; Z \u0026lt;- Y 的评分更高 而依据评分函数进行搜索的搜索方法常用的有穷举（5个节点以下可用）和 爬山算法（一个贪婪算法）pympy的实现如下：\nfrom pgmpy.estimators import HillClimbSearch # create some data with dependencies data = pd.DataFrame(np.random.randint(0, 3, size=(2500, 8)), columns=list('ABCDEFGH')) data['A'] += data['B'] + data['C'] data['H'] = data['G'] - data['A'] hc = HillClimbSearch(data, scoring_method=BicScore(data)) best_model = hc.estimate() print(best_model.edges()) #[('A', 'C'), ('A', 'B'), ('C', 'B'), ('G', 'A'), ('G', 'H'), ('H', 'A')]  2.Constraint-based Structure Learning 比如根据独立性得到最优结构的方法，相对来讲前一种更有效\nfrom pgmpy.independencies import Independencies ind = Independencies(['B', 'C'], ['A', ['B', 'C'], 'D']) ind = ind.closure() # required (!) for faithfulness model = ConstraintBasedEstimator.estimate_from_independencies(\u0026quot;ABCD\u0026quot;, ind) print(model.edges()) #[('A', 'D'), ('B', 'D'), ('C', 'D')]  回到泰坦尼克的例子，使用HillClimbSearch试了以下，在自己的测试集得到了和之前手工设计网络相同的精度（kaggle测试成绩略低一些），但是模型结构更复杂，不过看看给出的模型可以发现一些有趣的东西，比如Cabin Pclass Fare 相关，Age还影响了Sex等等\nfrom pgmpy.estimators import HillClimbSearch from pgmpy.estimators import BdeuScore, K2Score, BicScore hc = HillClimbSearch(train, scoring_method=BicScore(train)) best_model = hc.estimate() #print(best_model.edges()) showBN(best_model)  \r\r\rbest_model.fit(train, estimator=BayesianEstimator, prior_type=\u0026quot;BDeu\u0026quot;) # default equivalent_sample_size=5 predict_data=test.drop(columns=['Survived'],axis=1) y_pred = best_model.predict(predict_data) (y_pred['Survived']==test['Survived']).sum()/len(test)#测试集精度 #0.8131868131868132  模型保存 略\n先验 这里摘录一些下文对先验的介绍 ref:Bayesian Methods for Hackers chp6\n贝叶斯先验可以分为两类:客观先验和主观先验。客观先验的目的是让数据对后验影响最大，主观先验的目的是让从业者对前验表达自己的观点。 事实上，从选择先验分布开始就已经开始搭建模型了，属于建模的一部分。如果后验不符合要求，自然可修改更换先验，这是无关紧要的。没有正确的模型，只有有用的模型。 经验贝叶斯方法 一种叫经验贝叶斯方法融合了频率派的做法，采用 数据\u0026gt;先验\u0026gt;数据\u0026gt;后验 的方法，从观测数据的统计特征构建先验分布。这种方法实则违背了贝叶斯推理 先验\u0026gt;数据\u0026gt;后验 的思路。 从专家获得先验分布 可以考虑实验转盘赌法捕获专家认为的先验分布形状。做法是让专家吧固定总数的筹码放在各个区间上以此来表达各个区段的概率，如下所示。之后可用合适的分布对此进行建模拟合，得到专家先验。 \r\r\r判断先验是否合适 只要先验在某处概率不为零，后验就有机会在此处表达任意的概率。当后验分布的概率堆积在先验分布的上下界时，那么很肯先验是不大对的。（比如用Uniform（0,0.5）作为真实值p=0.7的先验，那么后验推断的结果会堆积在0.5一侧，表明真实值极可能大于0.5）\n练手数据集 Binary Classification  \rIndian Liver Patient Records\r \rSynthetic Financial Data for Fraud Detection\r \rBusiness and Industry Reports\r \rCan You Predict Product Backorders?\r \rExoplanet Hunting in Deep Space\r \rAdult Census Income\r  Multiclass Classification  \rIris Species\r \rFall Detection Data from China\r \rBiomechanical Features of Orthopedic Patients\r  下载 链接: https://pan.baidu.com/s/1lR3-tiSt-NJgogg4al8u-w 提取码: 2yc1\n","date":1553400000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553400000,"objectID":"09b7490dee0ac8ed206484df0ce7749b","permalink":"https://leidawt.github.io/en/post/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9Cpython%E5%AE%9E%E6%88%98%E4%BB%A5%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%BA%E4%BE%8Bpgmpy%E5%BA%93/","publishdate":"2019-03-24T12:00:00+08:00","relpermalink":"/en/post/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9Cpython%E5%AE%9E%E6%88%98%E4%BB%A5%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%BA%E4%BE%8Bpgmpy%E5%BA%93/","section":"post","summary":"@[TOC] 本文的相关数据集，代码见文末百度云 贝叶斯网络简介 贝叶斯网络是一种置信网络，一个生成模型。（判别模型，生成模型的区分可以这样：回答p(lab","tags":[],"title":"贝叶斯网络python实战（以泰坦尼克号数据集为例，pgmpy库）","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"借助TensorBoardX,可使用优秀的TensorBoard工具 doc: https://tensorboardx.readthedocs.io/en/latest/tensorboard.html 首先pip安装TensorBoardX和TensorFlow（cpu版本即可）\n使用 首先引入并构建writer\nfrom tensorboardX import SummaryWriter writer = SummaryWriter('./runs/exp1')  注意程序最后执行writer.close()关闭writer 之后即可开始对TensorBoardX写入需要的信息了，几种常用功能如下：\n1.画loss曲线 writer.add_scalar('batch_loss', batch_loss, epoch_index)  \r\r\r2.画激活情况 用于检查深层网络里的层激活与权值分布情况，避免梯度消失等\nfor name, param in model.named_parameters(): writer.add_histogram( name, param.clone().data.numpy(), epoch_index)  \r\r\r3.画网络结构图 输入模型和输入尺寸（用于内部函数正确遍历网络）\nwriter.add_graph(model, t.Tensor(1, 784))  \r\r\r4.显示图片 writer.add_image('input', x, 1) writer.add_image('output', y, 1)  \r\r\r启动 在py文件所在目录运行：tensorboard \u0026ndash;logdir runs 即开启界面 \r\r\r","date":1547524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547524800,"objectID":"447b52afa44beec37b3c89639bea48a5","permalink":"https://leidawt.github.io/en/post/pytorch%E4%BD%BF%E7%94%A8tensorboard%E5%8F%AF%E8%A7%86%E5%8C%96/","publishdate":"2019-01-15T12:00:00+08:00","relpermalink":"/en/post/pytorch%E4%BD%BF%E7%94%A8tensorboard%E5%8F%AF%E8%A7%86%E5%8C%96/","section":"post","summary":"借助TensorBoardX,可使用优秀的TensorBoard工具 doc: https://tensorboardx.readthedocs.io/en/latest/tensorboard.html 首先pip安装TensorBoardX和TensorFlow（cpu","tags":[],"title":"pytorch使用TensorBoard可视化","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"AutoEncoder属于无监督学习的技术，其思想影响深远。 @[TOC]\n1.经典 AutoEncoder Ref:Reducing the Dimensionality of Data with Neural Networks \r\r\r简单采用背靠背的全连接层，形成一个瓶颈neck就为经典AutoEncoder的架构核心，层数一般不多，以1或2层隐含层为主。其是与PCA做了相似的事情。 Hinton提出也可Deep起来\n这里有趣之处是以RBM逐层初始化，然后穿起来Fine-tune。 此外这里的Encoder Decoder权重是共享的（仅取了转置），不过并不很重要。 结构如下所示 \r\r\r我实验发现直接使用relu激活加adam优化，在50000个MNIST数据集上跑，经10epochs就可达到较好的重建精度。 原图 \r\r\r重建 \r\r\rn.b. RBM及其训练 上述文章里训练每一层作为初始化时用到的RBM是一种经典的能量模型，（能量模型通俗解释https://www.zhihu.com/question/59264464），RBM很像但隐含层神经网络，但训练上区别很大，采用一种CD-K方法。 ref：https://wenku.baidu.com/view/7b9634e56bec0975f565e240.html 过程有点类似train一个单隐层AutoEncoder，参数有两个偏置项和一个链接权重项W，其目标函数F是依据能量背景提出的，优化目标是降低F，方法是梯度下降。比较有趣的地方是其是把概率p算出来后又取了采样（一般伯努利即可），使其变为0-1二值的。 RBM在relu，batchnorm等改善深度NN训练难题的技术普遍应用后，已经不再是必要的，而在06年左右，必需通过RBM逐层per-train才能train起来深度网路结构，故论文中还是应用了RBM。\n通过可视化隐含层，可以证明AutoEncoder确实学到了表征\n2.Denoising AutoEncoder ref: Extracting and Composing Robust Features with Denoising Autoencoders \r\r\r其思想很朴素，仅改动了一点：将输入数据加噪声（具体方法是随机把某些像素从0-\u0026gt;1）后送进模型，然后期望恢复没有噪声的原始图像。通过这样的方法，作者认为能提取到更好更鲁棒性的表征。解释如下。 1.从流型学习角度解释 下图直观可认为，我们期望算法能把红圈内加噪声的x（由此偏离了原位置）推回去，这样的能力就有可能在处理与x的相似样本时有更好的泛化能力 \r\r\r2.信息论观点解释（interesting） 这个AutoEncoder的工作可看做它把有噪声的的数据‘滤波’成了无噪声数据，使得信息量增加，这个增加正是来自‘滤波器’的注入，因此在训练时候隐含层参数就会保留（学习）到一些信息。因此最终的优化目标就可理解为在保留信息和尽可能最优化重建结果之间的权衡。\n3.Sparse AutoEncoder ref: An Analysis of Single-Layer Networks in Unsupervised Feature Learning http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B 这个工作使用了新的Loss函数，在经典AE的Loss上加了稀疏惩罚项（sparsity penalty）,即下图第二项 \r\r\r第二项展开为 \r\r\r其中s2为隐含层s2的神经元个数，ρ为一个固定的超参数，表达稀疏程度，ρ_head为 \r\r\r即在整个数据集上的平均激活度。 为了了解算法学到的特征，我们可以进行可视化，即寻找能最大化激活隐含层的输入图像（需要建立在某些约束条件下，比如约束图像像素和）。这里通过约束\r\r\r\r给出了能最佳激活第i个神经元的输入的解（其中xj是输入图像展开后的第j个像素） \r\r\r具体的代码如下： SAE稀疏自编码在MNIST数据集表现，进行10*10随机切割，不然可视化效果不明显。\nimport numpy as np import matplotlib.pyplot as plt import torch as t import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms from BasicModule import BasicModule from tensorboardX import SummaryWriter from sklearn.decomposition import PCA writer = SummaryWriter('./runs/exp1') t.manual_seed(1) root = './AI/DataSets/MNIST' batch_size = 32 data_spilt = 50000 # 截取的数据集大小，以减小计算量 inshape = (10, 10) trainData = datasets.MNIST(root, transform=transforms.Compose([ transforms.RandomCrop(10), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)), ])) trainData = t.utils.data.random_split( trainData, [data_spilt, len(trainData)-data_spilt])[0] train_loader = t.utils.data.DataLoader( trainData, batch_size=batch_size, shuffle=True) class SparseAutoEncoder(BasicModule): def __init__(self, BETA=1, ROU=0.01, hiddenshape=300, USE_P=False): super(SparseAutoEncoder, self).__init__() self.model_name = 'SparseAutoEncoder' self.BETA = BETA # 稀疏项考虑程度 self.ROU = ROU # 稀疏度 self.USE_P = USE_P self.inshape = inshape[0]*inshape[1] self.hiddenshape = hiddenshape self.encoder = nn.Linear(self.inshape, self.hiddenshape) self.decoder = nn.Linear(self.hiddenshape, self.inshape) def forward(self, x): encode = t.sigmoid(self.encoder(x)) decode = t.sigmoid(self.decoder(encode)) return encode, decode def display_hidden(self, index): with t.no_grad(): paras = [each for name, each in self.encoder.named_parameters()] # w,b w = paras[0] num = w[index, :] den = ((w[index, :]**2).sum())**0.5 plt.imshow((num/den).view(inshape).numpy()) plt.show() def cal_hidden(self): with t.no_grad(): paras = [each for name, each in self.encoder.named_parameters()] # w,b w = paras[0] out = t.Tensor(w.shape[0], 1, inshape[0], inshape[1]) for i in range(w.shape[0]): num = w[i, :] den = ((w[i, :]**2).sum())**0.5 out[i, 0] = (num/den).view(inshape) return out def predict(self, x, load=None): '''输入单张图片预测dataset元组(Tensor[1,28,28],class) ''' self.eval() with t.no_grad(): if load is not None: self.load(load) x, c = x x = x.view(1, -1) result = self(x)[1] result = result.detach() return x.view(inshape).numpy(), result.view(inshape).numpy() def trainNN(self, lr=1, weight_decay=1e-5, epochs=5): self.train() optimizer = optim.Adam( self.parameters(), lr=lr, weight_decay=weight_decay ) batch_loss = 0 for epoch in range(epochs): for i, (bx, by) in enumerate(train_loader): bx = bx.view(bx.shape[0], -1) optimizer.zero_grad() criterion = nn.MSELoss() # mse损失 encode, decode = self(bx) p_head = encode.sum(dim=0, keepdim=True)/encode.shape[0] p = t.ones(p_head.shape)*self.ROU penalty = (p*t.log(p/p_head)+(1-p) * t.log((1-p)/(1-p_head))).sum()/p.shape[1] if self.USE_P: loss = criterion(decode, bx) + \\ self.BETA*penalty else: loss = criterion(decode, bx) loss.backward() batch_loss += loss.item() optimizer.step() if epoch % 1 == 0: print('batch loss={}@epoch={}'.format(batch_loss, epoch)) batch_loss = 0  为方便，训练和可视化部分代码在jupyter进行，代码如下\n%load_ext autoreload %autoreload 2 import SparseAutoEncoder2 from SparseAutoEncoder2 import SparseAutoEncoder as SAE t.manual_seed(1) model = SAE(BETA=5.0, ROU=0.01, hiddenshape=100,USE_P=True) model.trainNN(lr=0.001, weight_decay=0, epochs=15) from torchvision.utils import make_grid,save_image hidden=model.cal_hidden() res=make_grid(hidden,normalize=True, scale_each=False) model.show(res) save_image(hidden,normalize=True,filename='./hidden.png')  不加稀疏约束的隐含层激活状态，可看到很混乱，不含有明显的分工 \r\r\r加入稀疏约束项后明显改善，可看到隐含层较好的提取到了各个笔画的特征 \r\r\r接下来我试着在输入图片加一点噪声看看是否稀疏约束对降噪性能会有帮助。因为注意到不加约束的时候，算法对抹去大面积空白处的盐粒噪声很积极，反而对有数字区域不敏感，感觉加一点稀疏约束会有帮助，因为稀疏约束让隐含层更有效的捕获有价值信息，而非仅有白噪声区域。实验证明确实会略好一些，能恢复更多些的细节，进一步体现了稀疏约束的有效。 \r\r\r\r\r\r4.VAE 原文： Auto-Encoding Variational Bayes 解读： https://zhuanlan.zhihu.com/p/22464764 https://zhuanlan.zhihu.com/p/34998569 近期进展： https://zhuanlan.zhihu.com/p/52676826?utm_source=qq\u0026amp;utm_medium=social\nVAE相对较新，是一种生成模型，常用于生成图片。核心优势是其生成图片可控性好，可通过code控制，缺点是生成的图像较为模糊，内容不清晰，相比之下GAN能生成更清晰的图像。 其核心架构仍是Encode-Decode，但在中间加了code模糊化的环节。 \r\r\r直觉上可如下理解：code有一定噪声区域时，就有更大的机会产生基于训练样本的新图片。如下： \r\r\r故，虽然VAE推导较为复杂，但其最终实现是及其简洁的，两个NN，以重构精度和约束（结构图黄色框内，显然若无约束，NN会为了重构精度将所有噪声都设为0）为优化目标，然后进行标准的SGD即可训练。至于约束目标的来历，是较为精彩和复杂的。下面描述之\n首先，有两大类学习模型：生成模型（Generative Model）和判别模型（Discriminative Model） 直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型 而生成模型基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后通过P(Y|X)= P(X,Y)/ P(X)得到P(Y|X) 故VAE的核心是： \r\r\rz是一些隐含变量，这在生成模型里经常出现，表达着一系列抽象特征。 这个式子在有些情况下是很好求解的，如在朴素贝叶斯分类器中，实现的是下式： \r\r\r其中p(ci)是i类在数据集的占比，（数一下就可，易），p(x|ci)是从数据集所有的标记为i的数据中，抽出x的概率（假想为多元高斯的概率密度函数（PDF））,p(ci|x)即x属于ci的概率（求解目标） 但这里，由于p(X)是连续的，即有无穷多class，故加法变为下面的积分，并不好求 \r\r\r为此，提出了一种方法直接近似出p(z|X)，这里的解决方法很经典，称之为Variational Inference。 可参考 http://blog.huajh7.com/2013/03/06/variational-bayes/ 这里提出q(z)逼近之，以KL散度为判断标准 不难整理出下式（代入贝叶斯公式，KL散度公式）： \r\r\r接下来的大部分工作就是处理右边来转换这个优化目标。 观察之，为使得KL（q|p）最小，考虑到log(p(X))是个未知的常数，故，期望右一小，右二大。 至此都是相对常规化的内容，下面进入VAE论文的核心Reparameterization Trick，解释见 https://zhuanlan.zhihu.com/p/22464768 通过此技巧换z=g(X+e),e为随机变量，同时有q(z)=p(e)，带入有 \r\r\r这一步前进了一大步，可以看到q(z|X)出现了。进一步，对右边第二项KL散度里两个分布均假设为标准高斯分布，即令q(\u0026hellip;)=N(miu,sigma),p(z)=N(0,I)便可化简为 \r\r\r用NN吐出这个复杂的q的均值和方差，这就是ENCODER（重要思想，用NN代替难以计算和表达的复杂函数），其优化loss就是上面式子。实际实现有个细节就是吐出 logσ^2 而不是方差，因为方差只有正。同时注意到正是假设了p(z)为标准高斯，形成先验分布，才保证了优化中不会产生方差都变0的退化。 至此就给出了q的具体计算和其loss目标。 右边第一项即表达的是p(X|z)的对数似然，即DECODER的loss目标。VAE没有对DECODER进行假设，实现上直接由NN进行z-\u0026gt;X_head的重建，以X X_head之间的BCE loss进行优化，与其他AutoEncoder一样。 实现： pytorch官方有实现 https://github.com/pytorch/examples/tree/master/vae 其核心model为：\nclass VAE(nn.Module): def __init__(self): super(VAE, self).__init__() self.fc1 = nn.Linear(784, 400) self.fc21 = nn.Linear(400, 20) self.fc22 = nn.Linear(400, 20) self.fc3 = nn.Linear(20, 400) self.fc4 = nn.Linear(400, 784) def encode(self, x): h1 = F.relu(self.fc1(x)) return self.fc21(h1), self.fc22(h1) def reparameterize(self, mu, logvar): std = torch.exp(0.5*logvar) eps = torch.randn_like(std) return eps.mul(std).add_(mu) def decode(self, z): h3 = F.relu(self.fc3(z)) return torch.sigmoid(self.fc4(h3)) def forward(self, x): mu, logvar = self.encode(x.view(-1, 784)) z = self.reparameterize(mu, logvar) return self.decode(z), mu, logvar def loss_function(recon_x, x, mu, logvar): BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum') KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) return BCE + KLD  大致运行情况： 重建情况 \r\r\rdecoder对随机输入的输出，可看到VAE的特点，大概意思对，但并不清晰 \r\r\r5. 卷积AutoEncoder ref: Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction https://github.com/L1aoXingyu/pytorch-beginner/blob/master/08-AutoEncoder/conv_autoencoder.py 思路是将卷积加到AE架构中，改善了解决图像问题的性能，利用了CNN不会权重爆炸等一系列优点。核心是 反卷积 和 反池化怎么实现。（原论文通过使用发现maxpooling池化效果较好，其解释是max起到一种稀疏约束）\n反池化很简单，非最大的部分全置零即可 pytorch实现有maxunpool层，但其需要maxpool层额外返回记录最大值索引的矩阵，比较麻烦，故这里就不采用，直接用反卷积卷回去也不错 \r\r\r反卷积实际就是再做一次卷积即可（通过补零，信号与系统典型套路） pytorch实现有ConvTranspose2d层 \r\r\r实现 用MNIST_FASHION来做实验，MNIST_FASHION细节更丰富，同时计算量较小\nimport numpy as np import matplotlib.pyplot as plt import torch as t import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms from BasicModule import BasicModule from torchvision.utils import make_grid, save_image # 加噪声的SAE t.manual_seed(1) # root = ./AI/DataSets/MNIST' root = './AI/DataSets/MNIST_FASHION' batch_size = 32 data_spilt = 50000 # 截取的数据集大小，以减小计算量 inshape = (28, 28) trainData = datasets.FashionMNIST(root, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ])) trainData = t.utils.data.random_split( trainData, [data_spilt, len(trainData)-data_spilt])[0] train_loader = t.utils.data.DataLoader( trainData, batch_size=batch_size, shuffle=True) class ConvAutoEncoder(BasicModule): def __init__(self): super(ConvAutoEncoder, self).__init__() self.model_name = 'ConvAutoEncoder' self.encoder = nn.Sequential( nn.Conv2d(1, 16, 3, stride=3, padding=1), # b, 16, 10, 10 nn.ReLU(True), nn.MaxPool2d(2, stride=2), # b, 16, 5, 5 nn.Conv2d(16, 8, 3, stride=2, padding=1), # b, 8, 3, 3 nn.ReLU(True), nn.MaxPool2d(2, stride=1) # b, 8, 2, 2 ) self.decoder = nn.Sequential( nn.ConvTranspose2d(8, 16, 3, stride=2), # b, 16, 5, 5 nn.ReLU(True), nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1), # b, 8, 15, 15 nn.ReLU(True), nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1), # b, 1, 28, 28 nn.Tanh() ) def forward(self, x): encode = self.encoder(x) decode = self.decoder(encode) return encode, decode def predict(self, x, load=None): '''输入单张图片预测dataset元组(Tensor[1,28,28],class) ''' self.eval() with t.no_grad(): if load is not None: self.load(load) x, c = x x = t.unsqueeze(x, 0) # (1,28,28) -\u0026gt;(1,1,28,28) result = self(x)[1] result = result.detach() return x.view(inshape).numpy(), result.view(inshape).numpy() def show(self): def showim(img): npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest') plt.show() for i, (bx, by) in enumerate(train_loader): img_in = make_grid(bx, normalize=True) showim(img_in) self.eval() with t.no_grad(): img_out = (self(bx)[1]).detach() img_out = make_grid(img_out, normalize=True) showim(img_out) break def trainNN(self, lr=1, weight_decay=1e-5, epochs=10): self.train() optimizer = optim.Adam( self.parameters(), lr=lr, weight_decay=weight_decay ) batch_loss = 0 for epoch in range(epochs): for i, (bx, by) in enumerate(train_loader): optimizer.zero_grad() criterion = nn.MSELoss() # mse损失 encode, decode = self(bx) loss = criterion(decode, bx) loss.backward() batch_loss += loss.item() optimizer.step() if epoch % 1 == 0: print('batch loss={}@epoch={}'.format(batch_loss, epoch)) batch_loss = 0  训练\nimport numpy as np import matplotlib.pyplot as plt import ConvAutoEncoder from ConvAutoEncoder import ConvAutoEncoder as CAE model=CAE() model.trainNN(lr=0.003, weight_decay=1e-5, epochs=18) x,y=model.predict(ConvAutoEncoder.trainData[0]) plt.imshow(x) plt.show() plt.imshow(y) plt.show() model.show()  \r\r\r","date":1547524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547524800,"objectID":"bd5a004cdb15f4e515b7eb55114b12bb","permalink":"https://leidawt.github.io/en/post/%E5%87%A0%E7%A7%8Dautoencoder%E5%8E%9F%E7%90%86/","publishdate":"2019-01-15T12:00:00+08:00","relpermalink":"/en/post/%E5%87%A0%E7%A7%8Dautoencoder%E5%8E%9F%E7%90%86/","section":"post","summary":"AutoEncoder属于无监督学习的技术，其思想影响深远。 @[TOC] 1.经典 AutoEncoder Ref:Reducing the Dimensionality of Data with Neural Networks 简单采用背靠背的全连接层，形成一个瓶颈neck就为经","tags":[],"title":"几种AutoEncoder原理","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"迁移学习overview \r\r\r\r相关论文与实现： https://github.com/jindongwang/transferlearning/tree/master/code\n1.源于目标均有标签 微调：最简单的迁移方法。在图像分类里效果显著。常通过微调在ImageNet上训练好的大模型，微调后几层，或更极端的，直接重训最后的全连接层。 例，在猫狗大战数据集里，通过重新训练最后的fc层就可轻松达到99.4%左右的测试集精度。\n多任务学习：通过相似任务共享几层隐含层进行。在NLP里效果很好 \r\r\r2.源数据集有标签，目标数据集无标签 有很多方法。 1.域对抗训练方法 基本思想借鉴了GAN的对抗。理论基础是一个好的特征提取应该是对数据域不敏感的，因此设计了由三个子网络构成的NN。其中特征提取器用于提取抽象特征，分类器根据抽象特征给出分类，域分辨器用于分辨抽象特征来自哪个数据域。优化目标是在有高的分类精度的同时，让域分辨器不能分辨抽象特征的来源（意味着特征不但有效且对数据域变更有强鲁棒性）。 https://blog.csdn.net/a1154761720/article/details/51020105 https://github.com/fungtion/DANN \r\r\r2.zero/one shot learning 是一种极端形式的迁移学习，可迁移到从未处理过的样本。思路和NLP的词向量很相似，通过映射到一定维数的Embedding 空间，然后通过判断在Embedding 空间的距离远近来分类。\n3.源无标签，目标数据有标签 这其实是半监督学习。图中提到的self-taught learning具体做法是把稀疏自编码器与Softmax回归分类器串联起来。先用大量无标签数据集进行无监督的自编码器学习，得到特征表达，再用小的有标签数据集借助前面无监督学习到的特征在后边训练一个简单的有监督分类器，比如softmax。这样大大减轻了数据标注工作量。\n4.略 ","date":1547524800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547524800,"objectID":"bc46c0fe53101231972ef5a6335d99ab","permalink":"https://leidawt.github.io/en/post/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/","publishdate":"2019-01-15T12:00:00+08:00","relpermalink":"/en/post/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/","section":"post","summary":"迁移学习overview 相关论文与实现： https://github.com/jindongwang/transferlearning/tree/master/code 1.源于目标均有标签 微调：最简单的迁移方法。在图像分类里效果显著。常通过微调在ImageNet上训","tags":[],"title":"迁移学习","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"概述 tSNE是一个很流行的降维可视化方法，能在二维平面上把原高维空间数据的自然聚集表现的很好。这里学习下原始论文，然后给出pytoch实现。整理成博客方便以后看\nSNE tSNE是对SNE的一个改进，SNE来自Hinton大佬的早期工作。tSNE也有Hinton的参与。先解释下SNE。 SNE 全称叫Stochastic Neighbor Embedding。思想是这样的，分别在降维前和降维后的数据集上定义一个函数，计算每个点对（i与j）之间的‘距离’。在SNE中，这个距离是非线性的高斯分布，所以SNE是一种非线性降维方法，这也是其相比PCA等线性方法能更加强大的一个核心原因。在SNE里，函数具体如下定义。后面称之为相似度函数好了，这样比较直观 \r\r\r\r\r\r注：pii qii均令为零，因为这里讨论点对之间关系，与自身的没有意义\n然后算法的工作就是怼出来最合适的降维，来让降维前后点对之间相似度最接近（即原来远的还远，原来近的还近）。这个前后相似的判别标准SNE里使用了KL散度，比较直观。 \r\r\r然后这玩意就用SGD之类的train就好了。不过原文提到不大好train，用到了模拟退火之类的方法。\ntSNE 全称为t-Distributed Stochastic Neighbor Embedding 做为对SNE的改进，有以下两点\n1.解决不对称问题 SNE的相似度函数是条件概率，p(i|j)!=p(j|i)，这会导致一个问题：其对原来远的点表达近了这种错误很灵敏，对原来近的表达远了的错误惩罚不佳（从其优化目标KL散度的公式可看出。注意乘的p(j|i)），所以这会令降维后，同类点比较散，不集中。解决方案是换成联合概率。 \r\r\r\r\r\r但实践上采用 \r\r\rWhy? 离群点的影响。可如下图所示（涂黑表示很小），假设2号点是离群点，离所有的点都很远，那么左边是未引入对称化的相似度矩阵（矩阵值表示的是相对值，因为有归一化），右侧是采用联合概率的相似矩阵，这个肯定是对称阵，所以可以看到2对任何点的距离都很小了。（均以黑色表示）若采用 \r\r\r（计算上就是（P+P转置）/2） 则不会有此问题，第离群点距离其他点的相对远近还可以正确表达。 \r\r\r这就是第一点改进，并不是最重要的一点。相比原SNE其效果并未显著提高，原话是 some times even better\n2.解决拥挤问题 修改了对降维后数据的相似函数定义。这是全文最精彩的部分。改为t分布 \r\r\r这里形式上于单位柯西分布一致。 修改原理可这样理解\r\r\r\r上图画出两个分布的对比，t分布有更长的尾巴。横轴是距离，纵轴表示相似度，越大越相似。直观解释是，在以相似度最接近为优化目标时，算法对于原来就相似的点对（比如属于同类的点对就是这种），不希望降维后分的太开，相对小的距离就很满意了。而对原来不那么相似的（比如不属于同一类的点对），就得距离很大才足够满意。这样的结果是同类拉近，异类排斥，正式所期望的结果。 这个对相似函数精巧设计很有启发性，十分巧妙。\n实现 完整代码，jupyter，数据见文末 之前自己实现有些问题，后来参考了原作者给出的numpy实现做了些修改。 具体实现思路是这样的，先算出来降维前的相似度（显然只需计算一次），然后调降维结果来让降维后相似度尽可能逼近之。 第一步，计算降维前点对相似度P 观察p(j|i)公式，发现每个点都有自己的方差，这个如何确定？这个方差大小显著影响点对其附近多远的点考虑临近，显然不能全取1了之，具体方法如下 1.用户设置一个超参数：困惑度（perp），通常取5-50 2.对每个点i，找出σi使得 \r\r\r其中H为香农熵 \r\r\r为什么这样？香农熵表达的是混乱程度，这里实际意义是点i的邻近点多少，原始数据集里，点在某些地方密度大，某些地方密度小，故为达到相同的困惑度。在密度大的地方的点，σi会小一些，反之大一些。具体计算可通过二分查找确定，因为σi与困惑度是正相关的，通过二分查找很快就可迭代到满意精度（e-5量级） 这样模型通过不同的σi就考虑了数据集在原高纬空间分布不均匀的问题\ndef cal_distance(self, data): '''计算欧氏距离 https://stackoverflow.com/questions/37009647/ Arguments: data {Tensor} -- N*features Returns: Tensor -- N*N 距离矩阵，D[i,j]为distance(data[i],data[j]) ''' assert data.dim() == 2, '应为N*features' r = (data*data).sum(dim=1, keepdim=True) D = r-2*data@data.t()+r.t() return D def Hbeta(self, D, beta=1.0): '''计算给定某一行(n,)与sigma的pj|i与信息熵H Arguments: D {np array} -- 距离矩阵的i行，不包含与自己的，大小（n-1,) Keyword Arguments: beta {float} -- 即1/(2sigma^2) (default: {1.0}) Returns: (H,P) -- 信息熵 , 概率pj|i ''' # Compute P-row and corresponding perplexity P = np.exp(-D.copy() * beta) sumP = sum(P) H = np.log(sumP) + beta * np.sum(D * P) / sumP P = P / sumP return H, P def p_j_i(self, distance_matrix, tol=1e-5, perplexity=30): '''由距离矩阵计算p(j|i)矩阵，应用二分查找寻找合适sigma Arguments: distance_matrix {np array} -- 距离矩阵(n,n) Keyword Arguments: tol {float} -- 二分查找允许误差 (default: {1e-5}) perplexity {int} -- 困惑度 (default: {30}) Returns: np array -- p(j|i)矩阵 ''' print(\u0026quot;Computing pairwise distances...\u0026quot;) (n, d) = self.X.shape D = distance_matrix P = np.zeros((n, n)) beta = np.ones((n, 1)) logU = np.log(perplexity) # 遍历每一个数据点 for i in range(n): if i % 500 == 0: print(\u0026quot;Computing P-values for point %d of %d...\u0026quot; % (i, n)) # 准备Di， betamin = -np.inf betamax = np.inf Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] (H, thisP) = self.Hbeta(Di, beta[i]) Hdiff = H - logU tries = 0 # 开始二分搜索，直到满足误差要求或达到最大尝试次数 while np.abs(Hdiff) \u0026gt; tol and tries \u0026lt; 50: if Hdiff \u0026gt; 0: betamin = beta[i].copy() if betamax == np.inf or betamax == -np.inf: beta[i] = beta[i] * 2. else: beta[i] = (beta[i] + betamax) / 2. else: betamax = beta[i].copy() if betamin == np.inf or betamin == -np.inf: beta[i] = beta[i] / 2. else: beta[i] = (beta[i] + betamin) / 2. (H, thisP) = self.Hbeta(Di, beta[i]) Hdiff = H - logU tries += 1 # 最后将算好的值写至P，注意pii处为0 P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP print(\u0026quot;Mean value of sigma: %f\u0026quot; % np.mean(np.sqrt(1 / beta))) return P  这里欧拉距离矩阵的计算采用了我采用了向量化实现，详见注释，蛮interesting的。p_j_i的实现主要套用了原作者实现。这里说明下代码里H的计算公式是手动化简了的，很好化简，化简后结果大大提高了计算效率并保证了计算稳定性。 之后可计算出对称的P\ndef cal_P(self, data): '''计算对称相似度矩阵 Arguments: data {Tensor} - - N*N Keyword Arguments: sigma {Tensor} - - N个sigma(default: {None}) Returns: Tensor - - N*N ''' distance = self.cal_distance(data) # 计算距离矩阵 P = self.p_j_i(distance.numpy(), perplexity=self.perp) # 计算原分布概率矩阵 P = t.from_numpy(P).float() # p_j_i为numpy实现的，这里变回Tensor P = (P + P.t())/P.sum() # 对称化 P = P * 4. # 夸张 P = t.max(P, t.tensor(1e-12)) # 保证计算稳定性 return P  P = (P + P.t())/P.sum() 这里与论文式子不大一样，多归一化了一些，其实无所谓。夸张是论文提到的小trick，在迭代前几十次时使用，目的是加快优化速度。 第二步 定义降维后点对相似度矩阵Q，这个函数会在迭代中反复调用\ndef cal_Q(self, data): '''计算降维后相似度矩阵 Arguments: data {Tensor} - - Y, N*2 Returns: Tensor - - N*N ''' Q = (1.0+self.cal_distance(data))**-1 # 对角线强制为零 Q[t.eye(self.N, self.N, dtype=t.long) == 1] = 0 Q = Q/Q.sum() Q = t.max(Q, t.tensor(1e-12)) # 保证计算稳定性 return Q  第三步 硬train一发 既然选择pytorch，就没求导数了。直接捏着loss函数train就好了，嗯。\ndef train(self, epoch=1000, lr=10, weight_decay=0, momentum=0.9, show=False): '''训练 Keyword Arguments: epoch {int} -- 迭代次数 (default: {1000}) lr {int} -- 学习率，典型10-100 (default: {10}) weight_decay {int} -- L2正则系数 (default: {0}) momentum {float} -- 动量 (default: {0.9}) show {bool} -- 是否显示训练信息 (default: {False}) Returns: Tensor -- 降维结果(n,2) ''' # 先算出原分布的相似矩阵 P = self.cal_P(self.X) optimizer = optim.SGD( [self.Y], lr=lr, weight_decay=weight_decay, momentum=momentum ) loss_his = [] print('training started @lr={},epoch={},weight_decay={},momentum={}'.format( lr, epoch, weight_decay, momentum)) for i in range(epoch): if i % 100 == 0: print('running epoch={}'.format(i)) if epoch == 100: P = P/4.0 # 100轮后取消夸张 optimizer.zero_grad() Q = self.cal_Q(self.Y) loss = (P*t.log(P/Q)).sum() loss_his.append(loss.item()) loss.backward() optimizer.step() print('train complete!') if show: print('final loss={}'.format(loss_his[-1])) plt.plot(np.log10(loss_his)) loss_his = [] plt.show() return self.Y.detach()  结果 用论文作者给的MNIST子集测试下。喂数据前先吧数据用PCA降到30维 \r\r\r\r\r\r对比下sklearn \r\r\r半斤八两吧。有几个数字分的还不错。论文提到要通过随机尝试得到最佳结果。 tSNE的计算复杂度是比较高的，有N^2级别，工业上有NlogN的近似实现。\n总结  实现时要注意计算稳定性，尤其是0对log和除法的影响，max（data,1e-12）是个很好的实现，同时手动化简下表达式也很有帮助。 向量化计算欧拉距离的方法很有启发性 论文通过分析修改原方法通过直观感觉定义的函数的做法很有启发性 解决离群点的做法很有启发性  参考 Visualizing Data using t-SNE 李宏毅机器学习\n下载 链接: https://pan.baidu.com/s/1HQjsXiavCyDNigpl93ITpQ 提取码: wwiv\n","date":1544673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544673600,"objectID":"a9b38344548b873a504e89f7cbe462a4","permalink":"https://leidawt.github.io/en/post/t-sne%E5%8F%8Apytorch%E5%AE%9E%E7%8E%B0/","publishdate":"2018-12-13T12:00:00+08:00","relpermalink":"/en/post/t-sne%E5%8F%8Apytorch%E5%AE%9E%E7%8E%B0/","section":"post","summary":"概述 tSNE是一个很流行的降维可视化方法，能在二维平面上把原高维空间数据的自然聚集表现的很好。这里学习下原始论文，然后给出pytoch实现。","tags":[],"title":"t-SNE及pytorch实现","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"本文通过实现简单的回归来入门下numpy与pytorch dataSet文末给出\n线性回归 线性回归是个古老的问题了，对于线性回归，就是简单找到一组w使得目标函数能最好的拟合数据集X，这个好定义为总均方误差最小。线性回归的解析解数学课本已经给出，证明也不困难，简单的解一个矩阵方程即可。具体可见https://blog.csdn.net/Fleurdalis/article/details/54931721， 一个关键点是矩阵求导法则，除此之外就仅为一个简单的求最值问题\nnumpy版本 这里用纯numpy以梯度下降求一下\n#! python3 # -*- coding:utf-8 -*- # 单变量线性回归 梯度下降法 import numpy as np import pandas as pd import matplotlib.pyplot as plt plt.rcParams['font.sans-serif'] = ['SimHei'] # 用来正常显示中文标签 plt.rcParams['axes.unicode_minus'] = False # 用来正常显示负号 class Linear_regration_single: def __init__(self, data, alpha=0.01, theta=None, step=1500): \u0026quot;\u0026quot;\u0026quot;初始化线性回归类 Arguments: data {str} -- 数据集地址 Keyword Arguments: alpha {float} -- 学习步长 (default: {None}) theta {float} -- 起始theta[t0,t1] (default: {None}) step {int} -- 迭代步数 (default: {None}) \u0026quot;\u0026quot;\u0026quot; self.cost_history = [] self.df = pd.read_csv(data) self.m = len(self.df) # 数据总数 # 学习率 self.alpha = alpha # thetas if theta is None: self.theta = np.array([0, 0]).reshape(2, 1) else: self.theta = np.array(theta).reshape(2, 1) # 迭代数量 self.steps = step self.x = np.array(self.df['x']).reshape(self.m, 1) self.y = np.array(self.df['y']).reshape(self.m, 1) self.X = np.hstack((np.ones((self.m, 1)), self.x)) # 补充为其次形式 def h(self): \u0026quot;\u0026quot;\u0026quot;预测函数 Returns: np.array -- θ0+θ1*x向量 \u0026quot;\u0026quot;\u0026quot; return np.dot(self.X, self.theta) def predict(self, x): \u0026quot;\u0026quot;\u0026quot;预测函数对外接口 Arguments: x {float} -- x值 Returns: y -- 预测 \u0026quot;\u0026quot;\u0026quot; return self.theta[0, 0]+self.theta[1, 0]*x def J(self, theta=None): \u0026quot;\u0026quot;\u0026quot;cost function Returns: float -- cost \u0026quot;\u0026quot;\u0026quot; if theta is None: # 默认内部theta return np.sum((self.h()-self.y)**2)/(2*self.m) else: # 计算给定的theta cost return np.sum((np.dot(self.X, theta)-self.y)**2)/(2*self.m) def descend_gradient(self): \u0026quot;\u0026quot;\u0026quot;以梯度下降更新theta \u0026quot;\u0026quot;\u0026quot; for i in range(self.steps): #print(self.theta, self.J()) self.cost_history.append(self.J()) # 记录cost # 更新theta derta = np.dot(self.X.T, (self.h()-self.y))*self.alpha/self.m self.theta = self.theta-derta def closed_solution(self): \u0026quot;\u0026quot;\u0026quot;封闭解 Returns: np.array -- theta封闭解 \u0026quot;\u0026quot;\u0026quot; from numpy.linalg import inv return np.dot(np.dot(inv(np.dot(self.X.T, self.X)), self.X.T), self.y) def plot(self): \u0026quot;\u0026quot;\u0026quot;绘制结果 \u0026quot;\u0026quot;\u0026quot; fig = plt.figure() ax1 = fig.add_subplot(211) # 高1 宽2 图号1 ax2 = fig.add_subplot(212) # 高1 宽2 图号2 ax1.scatter(self.x, self.y, label='skitscat', color='k', s=25, marker=\u0026quot;x\u0026quot;) line_x = [min(self.x), max(self.x)] line_y = [self.predict(x) for x in line_x] ax1.plot(line_x, line_y) ax1.set_xlabel('x') ax1.set_ylabel('y') ax2.plot(self.cost_history) ax2.set_xlabel('迭代次数') ax2.set_ylabel('cost') ax1.set_title('单变量线性回归梯度下降') plt.show() def info(self): print('decent method solution:') print('final cost: '+str(self.J())) print('theat: '+str(self.theta)) print('closed solution:') print('final cost: '+str(self.J(self.closed_solution()))) print('theat: '+str(self.closed_solution())) if __name__ == \u0026quot;__main__\u0026quot;: data_dir = './ex1data1.txt' t = Linear_regration_single(data_dir) t.descend_gradient() t.info() t.plot()  可观测到与封闭解的结果还是很接近的 decent method solution: final cost: 4.964362046184745 theat: [[-1.58199122] [ 0.96058838]] closed solution: final cost: 4.476971375975179 theat: [[-3.89578088] [ 1.19303364]] \r\r\rpytorch版本 pytorch version=0.4.1 下面的版本是以pytorch实现的，借助自动求导，只要定义好前向计算方法和loss即可。\nimport numpy as np import torch.utils.data import torch as t import matplotlib.pyplot as plt t.manual_seed(1) # reproducible fig = plt.figure() # 首先造出来一些带噪声的数据 N = 1100 x = t.linspace(0, 10, N) y = 10*x+5+t.rand(N)*5 ax1 = fig.add_subplot(211) # 高2 宽2 图号1 ax1.plot(x.numpy(), y.numpy()) # 从Tensor构建DataSet对象 data = t.utils.data.TensorDataset(x, y) # 随机分割数据集，测试集 train, test = t.utils.data.random_split(data, [1000, 100]) # 建立dataloader shuffle来使得每个epoch前打乱数据 trainloader = t.utils.data.DataLoader(train, batch_size=100, shuffle=True) testloader = t.utils.data.DataLoader(test, batch_size=100, shuffle=True) # 定义模型参数 for y_head=w@x+b w = t.rand(1, 1, requires_grad=True) b = t.zeros(1, 1, requires_grad=True) optimizer = t.optim.SGD([w, b], lr=0.03, momentum=0.6) #optimizer = t.optim.Adam([w,b],lr=10) loss_his = [] batch_loss = 0 for epoch in range(10): for i, (bx, by) in enumerate(trainloader): bx = bx.view(1, -1) # torch.Size([100])-\u0026gt;torch.Size([1,100]) y_head = w@bx+b loss = 0.5*(y_head-by)**2 loss = loss.mean() batch_loss += loss.item() optimizer.zero_grad() # 先清除梯度 loss.backward() optimizer.step() #print('training:epoch {} batch {}'.format(epoch,i)) loss_his.append(batch_loss) batch_loss = 0 def main(): print('w={},b={}'.format(w.item(), b.item())) y_head = w*x+b ax1.plot(x.numpy(), y_head.detach().numpy().flatten()) ax1.set_title('fit') print('final_loss={}'.format(loss_his[-1])) ax2 = fig.add_subplot(212) # 高2 宽2 图号2 ax2.set_title('loss per epoch (log)') ax2.plot(np.log(loss_his)) plt.show() if __name__ == '__main__': main()  \r\r\rpytorch神经网络回归 借助神经网络的强大非线性能力，可以对任意曲线拟合，下面以拟合sin函数为例，定义了简单的全连接神经网络解决问题。 这里训练采用全局训练，未取batch 此外观察到采用BatchNorm之后训练稳定性明显提高，收敛略有提速\nimport numpy as np import math import matplotlib.pyplot as plt import torch as t from torch import nn, optim import torch.utils.data import torch.nn.functional as F N = 1000 x = t.linspace(0, 2*np.pi, N) y = t.sin(x) #y = t.sin(x)+t.randn(len(x))/30 print('x.shape, y.shape:', x.shape, y.shape) t.manual_seed(1) class Dense(nn.Module): # 自定义全连接层 def __init__(self, inshape, outshape): super(Dense, self).__init__() # 这里对w采用Xavier初始化（实为变种，仅考虑输入尺寸，这为普遍做法） alpha = 1./math.sqrt(inshape) self.w = nn.Parameter(t.randn(inshape, outshape)*alpha) # 偏置的初始化0即可 self.b = nn.Parameter(t.zeros(outshape)) def forward(self, x): return x@self.w+self.b class NN(nn.Module): def __init__(self, useDense=True): super(NN, self).__init__() if useDense: # 使用自定义的全连接层 self.l1 = Dense(1, 20) self.l2 = Dense(20, 40) self.l3 = Dense(40, 1) else: self.l1 = nn.Linear(1, 20) self.l2 = nn.Linear(20, 40) self.l3 = nn.Linear(40, 1) self.BN1 = nn.BatchNorm1d(20) self.BN2 = nn.BatchNorm1d(40) def forward(self, x): out = self.l1(x.view(-1, 1)) out = self.BN1(out) out = F.relu(out) out = self.l2(out) out = self.BN2(out) out = F.relu(out) out = self.l3(out) return out def predict(self, x): with t.no_grad(): return self.forward(x) net = NN() iter_num = 500 criterion = nn.MSELoss() #optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9) optimizer = optim.Adam(net.parameters(), lr=0.07) loss_his = [] net.train() for epoch in range(iter_num): y_head = net.forward(x) loss = criterion(y_head.view(-1), y) loss_his.append(loss.item()) if loss.item() \u0026lt; 1e-5: print('epoch=', epoch) break net.zero_grad() loss.backward() optimizer.step() net.eval() print('final loss:{}'.format(loss_his[-1])) fig = plt.figure() f1 = fig.add_subplot(211) f2 = fig.add_subplot(212) f1.plot(np.log10(np.array(loss_his))) y_head = net.predict(x) f2.plot(y.numpy()) f2.plot(y_head.numpy()) plt.show()  \r\r\r\repoch= 192 final loss:9.999754183809273e-06\n总结  numpy的shape=(3,)与shape=(3,1)的数据是不同的，这在numpy中常会错，可考虑assert或注意归并操作的keepdim。 numpy的sum等归并操作默认丢弃维数为1的维度，可指定keepdim=True或使用reshape方法。reshape不移动数据而是生成新的观察数据的方式，开销很小。 plt.rcParams[\u0026lsquo;font.sans-serif\u0026rsquo;] = [\u0026lsquo;SimHei\u0026rsquo;] # 用来正常显示中文标签 plt.rcParams[\u0026lsquo;axes.unicode_minus\u0026rsquo;] = False # 用来正常显示符号 对cost曲线取对数可看的更清楚些 pytorch view 与 numpy的reshape等价，不会幅复制数据 pytorch取计算图的Tensor并转numpy要注意先从计算图上取下，即调用detach方法 数据的归一化效果显著 对于pytorch里不需要记录梯度的计算，采用no_grad  def predict(self, x): with t.no_grad(): return self.forward(x)  numpy的计算值可用到单一cpu核，pytorch的blas后端已经做好了并行，通常可满载 data来自cs229的coursera板 链接: https://pan.baidu.com/s/1ibqIBTB7qTcq69EQ4qWcSw 提取码: 4jem  ","date":1544587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544587200,"objectID":"9b221e4894027564e54ca7be084f5476","permalink":"https://leidawt.github.io/en/post/numpy%E4%B8%8Epytorch%E5%AE%9E%E7%8E%B0%E5%9B%9E%E5%BD%92/","publishdate":"2018-12-12T12:00:00+08:00","relpermalink":"/en/post/numpy%E4%B8%8Epytorch%E5%AE%9E%E7%8E%B0%E5%9B%9E%E5%BD%92/","section":"post","summary":"本文通过实现简单的回归来入门下numpy与pytorch dataSet文末给出 线性回归 线性回归是个古老的问题了，对于线性回归，就是简单找到一","tags":[],"title":"numpy与pytorch实现回归","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"简述 逻辑回归虽简单，但其思想颇有价值 Logistic Regression 更原本的翻译应为对数回归，因同义词的缘故，习惯性的称之为逻辑回归了。虽称其为回归，但实际上这个算法是处理分类问题的。\nwhy Logistic Regression? 我们也可用线性模型进行分类预测，比如设y=wx+b 并判定y\u0026gt;阈值为正类。 \r\r\r但是问题在于，若数据集中有很正类的样本，会极大的影响w权重向自身倾斜，这是不希望看到的，如下 \r\r\r所以，思路便是引入非线性，对很正类的样本加以限制，这里用的是sigmoid函数，利用了其饱和特性。sigmoid函数最初来自18实际对人口增长的研究。 \r\r\r引入后变为这样 \r\r\r实际上，这便是一个采用sigmoid激活函数的单层神经网络 \r\r\r由于非线性的引入，整个函数不再是凸函数了，优化就只有进行迭代求解了。 我们定义代价函数以便进行参数训练。通常有均方误差和交叉熵（极大似然）两种定义，下面分别叙述\n 均方误差 \r\r\r很朴素的想法，对数据集内每个样本点预测误差求和 交叉熵（极大似然） 交叉熵 与 极大似然 是两种截然不同的思路，但最终的形式是一致的，有些殊途同归的感觉。 交叉熵优化的其实是其KL散度部分，其理解详见 https://www.jianshu.com/p/43318a3dc715 KL散度用于表达两个分布之间相似程度。其其思想简言之就是分布里概率越重的点越要像。 交叉熵误差在分类问题效果远好于均方误差，我们易从概率论的极大似然方法推出其形式 \r\r\r即预测函数取组参数w,b是，其在数据集上能正确表述所有样本点的可能性大小。优化目标自然是找最合适的w,b使得这个概率值最大化。通常还要取一下负对数，一是为了乘法变加法，二是从求max变成求min。 Why better？ 原因可由其求导结果体现出来。 https://blog.csdn.net/liweibin1994/article/details/79510237 简言之，均方误差的问题在于其对误差很大的点反而矫正意愿不强烈，如图（这个图来源于著名的Understanding the difﬁculty of training deep feedforward neural networks一文） \r\r\r  Toy set上实战 使用纯numpy实现\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt import scipy df = pd.read_csv('./ex2data1.txt') m = len(df) # 数据总数 pos = df[(df['y'] == 1)] neg = df[(df['y'] == 0)] x = np.mat(df.iloc[:, 0:-1]) y = np.mat(df.iloc[:, -1]).T # m*1 mat X = np.hstack((np.ones((m, 1)), x)) # m*n mat #theta = np.zeros((X.shape[1], 1)) # n*1 mat theta=np.array([0,0,0]).reshape(3,1) cost_history = [] def sigmoid(x): return 1.0 / (1 + np.exp(-x)) def J(theta,X,y,mylambda=0): theta=theta.reshape(3,1) #print(theta.shape,X.shape,y.shape) return -1*float((y.T*np.log(h(X,theta))+(1-y.T)*np.log(1-h(X,theta)))/m)+float(theta.T.dot(theta))*mylambda/(2*m) def h(X,theta): return sigmoid(X.dot(theta)) from scipy import optimize def opt(): global theta result = optimize.fmin(J, x0=theta, args=(X, y,0.), maxiter=400, full_output=True) theta=result[0] print(theta) def plot(): fig = plt.figure() plt.plot(pos.iloc[:, 0], pos.iloc[:, 1], 'k+', label='pos') plt.plot(neg.iloc[:, 0], neg.iloc[:, 1], 'yo', label='neg') # 画边界 x0 = [float(df.x0.min()), float(df.x0.max())] x1 = [float(-1*theta[0]/theta[2] - theta[1]/theta[2]*each) for each in x0] plt.plot(x0, x1) plt.grid(True) plt.show() opt() plot()  注意这里的优化调用了scipy 的现成算法，会快一些。数据见文末。若想获取非线性决策边界，一个简单的方法是对输入进行扩展。即x-\u0026gt;[x,x某平方,交叉项\u0026hellip;]然后一并喂给逻辑回归即可。 \r\r\r二类分类-\u0026gt;n类分类 多分类实现有两种常见思路，一是实现n个二分类器（集成学习的思想，打群架），二是把sigmoid函数换成其多分类版本softmax，其吐出的是对各个类的可能性数值，同时配合交叉熵为代价函数 \r\r\rsoftmax实际的实现有一些trick，主要是保证其计算稳定而不会溢出。核心方法是都剪掉max(x)再计算exp使得不会溢出。 此外，对于为什么使用softmax而不是其他函数的讨论可见：https://www.zhihu.com/question/40403377 主要是 softmax+交叉熵的求导结果及其简洁\nMNIST测试 这里采用对每个数字分别训练一个二分类器的方法，当时选了均方误差Loss，故训练困难，之后采用了L-BFGS-B优化器代替手写的简单梯度下降进行训练。 关于L-BFGS-B算法 http://www.hankcs.com/ml/l-bfgs.html https://www.zhihu.com/question/46441403 通常的多层神经网络或一些复杂的模型均采用梯度下降及其各种改进方法。不会使用BFGS之类的拟牛顿法，主要有下面考虑 神经网络优化问题有三个特点：大数据（样本多），高维问题（参数极多），非凸，BFGS这类利用二阶导信息的方法中计算成本高，易陷入鞍点的问题就比较显著了。 牛顿和拟牛顿法在凸优化情形下，如果迭代点离全局最优很近时，收敛速率快于gd。 code\nimport numpy as np import matplotlib.pyplot as plt import pandas as pd import scipy.io #Used to load the OCTAVE *.mat files import scipy.misc #Used to show matrix as an image import random #To pick random images to display import scipy from scipy import optimize from scipy.optimize import minimize from scipy.special import expit #Vectorized sigmoid function datafile = './ex3data1.mat' mat = scipy.io.loadmat( datafile ) X, y = mat['X'], mat['y'] y[y==10]=0 #原数据0 标记为10 这里改回0 #Insert a column of 1's to X as usual X = np.insert(X,0,1,axis=1)# 插入1 m = X.shape[0] # 数据总数 n = X.shape[1] #特征数，包括常数项 theta=np.zeros((n,1)) #theta=np.random.random(n) print(X.shape,y.shape,theta.shape,m,n) print(np.unique(y)) def J(mytheta,myX,myy,mylambda=0): #误差函数，返回float mytheta=mytheta.reshape(n,1)#这个处理防止相乘报错。因为输入可能为(n,)而不是（n,1） #print('J input theta x y',theta.shape,X.shape,y.shape) a=myy.T.dot(np.log(h(mytheta,myX))) b=(1-myy).T.dot(np.log(1-h(mytheta,myX))) c=theta.T.dot(mytheta)*mylambda/(2*m) #print(a.shape,b.shape,c.shape) return float(-1/m*(a+b)+c) def h(mytheta,myX): #Logistic hypothesis function #假设函数 返回（m,1） res=expit(myX.dot(mytheta)).reshape(m,1) #print('h theta X return',mytheta.shape,myX.shape,res.shape) return res def grad(mytheta,myX,myy,mylambda = 0.): #计算梯度向量，未考虑正则项 返回（n,） #print('grad theta X y',mytheta.shape,myX.shape,myy.shape) derta=h(mytheta,myX)-myy #print(derta.shape) res=(myX.T.dot(derta)).reshape(n)/m return res def opt(mytheta,myX,myy,c,mylambda=0.): #递归优化 c为类名（0-9） #minimize的输入mytheta为(n,)其传给调用函数的形式也为(n,) #这里传入(myy == c)*1是一个[0 0 0 1 1 ... ]向量 #maxiter最大迭代数，disp显示优化信息 #由于函数的复杂性，其不一定收敛的彻底，50次迭代已经很好，随仍可继续下降，但可能过拟合 #L-BFGS-B为优化的拟牛顿法，因为利用了二阶倒数信息，下降比较快。不传入自定义的grad会导致 #其使用数值方法计算grad，及其缓慢 res = minimize(J, mytheta, args=( myX, (myy == c)*1), method='L-BFGS-B', jac=grad, options={'maxiter':50,'disp':False}) #print(res) #res.x 是优化好的theta值 return res.x #final_theta=opt(theta,X,y,1) #print(final_theta.shape) #print(X[495:505,:].dot(final_theta)) #print(y[495:505,:]) #对0-9分别训练，以预测值最大的类别作为最终预测 thetas=np.zeros((10,n))#(10,401) for each in range(10): thetas[each]=opt(theta,X,y,each) predict=thetas.dot(X.T)# (10,5000) #argmax函数对每一类求最大值index predict=predict.argmax(axis=0) #(5000,) #最后统计正确率 print(np.mean((predict==y.reshape(m)))*100,'%') #产生的RuntimeWarning可能来自某些变量在优化中变0，导致log计算出错，会变为NaN，不影响最终结果  在我的低压cpu上6.33 s 就可在训练集获得96.84 %的正确率，可见表达力还是不错的，一是采用10个分类器的原因，二是MNIST本身确实简单\n参考资料 cs229 李宏毅机器学习\nDATA 链接: https://pan.baidu.com/s/1rxQM1HCICnAaUW3ih2GeYQ 提取码: ryj4\n","date":1544587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544587200,"objectID":"65ed7b038ffec961202dd9d15c72e905","permalink":"https://leidawt.github.io/en/post/numpy%E5%AE%9E%E7%8E%B0%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92mnist%E6%B5%8B%E8%AF%95/","publishdate":"2018-12-12T12:00:00+08:00","relpermalink":"/en/post/numpy%E5%AE%9E%E7%8E%B0%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92mnist%E6%B5%8B%E8%AF%95/","section":"post","summary":"简述 逻辑回归虽简单，但其思想颇有价值 Logistic Regression 更原本的翻译应为对数回归，因同义词的缘故，习惯性的称之为逻辑回归了。虽称其为回归，但实际上这个算法是","tags":[],"title":"numpy实现逻辑回归\u0026MNIST测试","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"概述 贝叶斯属于生成模型的一种，其实现很简单，就是应用贝叶斯公式。这是一种指定先验分布，求后验的方法。 概率论课本里著名的贝叶斯公式如下 \r\r\rp(ci)是i类在数据集的占比，（数一下就可，易） p(x|ci)是从数据集所有的标记为i的数据中，抽出x的概率（核心） p(ci|x)即x属于ci的概率 通过argmax（p(ci|x)）就可分类了。 所以只要解决p(x|ci)的计算，就ok了。我们假定p(x|ci)是服从多元高斯分布的。如下。其中D为feature数，Σ是(nfeature*nfeature)的协方差矩阵，μ是(nfeature)均值。当然也可以取别的分布。 \r\r\r\r\r\r高斯分布的密度图如上所示，μ决定中心位置，Σ的对角线元素就是维度上的方差，决定了上面椭圆状密度图的长轴短轴比，Σ其余元素是协方差，决定椭圆状密度图的倾斜。 为得到每个类的Σ，μ，可以用极大似然估计来做。已经证明了估计结果就是data的mean 和 cov。故至此，我们就可算出数据集每一类的p(x|ci)函数，p(ci|x)自然也就得到了。 预测新点很简单，只需要计算对每个类的p(ci|x)，最大的就是其归类。\n实现 关于测试数据 使用sklearn的鸢尾花数据集 https://www.cnblogs.com/meelo/p/4272036.html 鸢尾花数据集背景：鸢尾花数据集是原则20世纪30年代的经典数据集。它是用统计进行分类的鼻祖。样本数目150，特征数目4，分别是花萼长度(cm)、花萼宽度(cm)、花瓣长度(cm)、花瓣宽度(cm)。类数为3，是花的名字，每类正好50样本。对前三维可视化如下，可见还是很好分的 \r\r\rcode 代码如下，采用120训练集，30测试集\nimport numpy as np from sklearn import datasets iris = datasets.load_iris() from sklearn.naive_bayes import GaussianNB data=iris.data.copy() target=iris.target.copy().reshape(-1,1) np.random.seed(1)#！！！seed值的有效次数仅为一次 np.random.shuffle(data) np.random.seed(1) np.random.shuffle(target) train_x=data[0:120]#120*4 test_x=data[120:150]#30*4 train_tar=target[0:120].reshape(-1,1)#(120, 1) test_tar=target[120:150].reshape(-1,1)#(30 ,1) print(train_x[0]) class Native_Bayes: def __init__(self): #这几个变量由fit函数写，predict函数取用 self.P_x_c_fun=None self.c_num=None self.P_c=None def fit(self,train_x,train_tar): #计算P(c) n=len(train_x) c_num=len(np.unique(train_tar))#分类数 self.c_num=c_num P_c=[sum(train_tar==i)/n for i in range(c_num)] P_c=np.array(P_c).reshape(1,-1)#1*c_num self.P_c=P_c #计算P(x|c) P_x_c_fun=[] #计算分布参数，给出概率计算函数 for i in range(c_num): data=train_x[(train_tar==i).flatten()] mu=data.mean(axis=0)#(nfeature,) sigma=np.cov(data.T)#(nfeature,nfeature) sigma_det=(np.linalg.det(sigma)) sigma_inv=(np.linalg.inv(sigma)) temp=(1/(((2*np.pi)**(c_num/2))*(sigma_det**0.5))) #小心闭包错误 def g(mu,sigma,sigma_det,sigma_inv,temp): def fun(x): nonlocal temp,mu,sigma_inv x=x.reshape(1,-1) res=float(temp*(np.exp(-0.5*(x-mu).dot(sigma_inv).dot((x-mu).T)))) return res return fun P_x_c_fun.append(g(mu,sigma,sigma_det,sigma_inv,temp)) self.P_x_c_fun=P_x_c_fun P_x_c=np.empty((n,c_num)) for i in range(n): for j in range(c_num): P_x_c[i,j]=P_x_c_fun[j](train_x[i]) #计算p(c|x) PP=P_x_c*P_c PP=PP/PP.sum(axis=1,keepdims=True)#注意keepdims，不然无法自动扩展 c_head=np.argmax(PP,axis=1) return c_head def predict(self,test_x): n=len(test_x) P_x_c_fun=self.P_x_c_fun P_x_c=np.empty((n,self.c_num)) for i in range(n): for j in range(self.c_num): P_x_c[i,j]=P_x_c_fun[j](test_x[i]) PP=P_x_c*self.P_c PP=PP/PP.sum(axis=1,keepdims=True) c_head=np.argmax(PP,axis=1) return c_head model=Native_Bayes() res=model.fit(train_x,train_tar) print('训练集正确率：{}/120'.format(np.sum(res==train_tar.flatten()))) res=model.predict(test_x) print('测试集正确率：{}/30'.format(np.sum(res==test_tar.flatten()))) #对比工业实现 gnb = GaussianNB() res=gnb.fit(train_x, train_tar.flatten()).predict(test_x) np.sum(res==test_tar.flatten())  结果： 训练集正确率：118/120 测试集正确率：28/30\n总结 踩坑： np.random.seed(1)#！！！seed值的有效次数仅为一次 np.cov输入（nfeaturen）-\u0026gt;（nfeaturenfeature）的cov矩阵 其他： 通常这种简单生成模型的效果不会好于逻辑回归之类的方法，但当数据集数量很少时，优势就较为明显，故在数据集比较小时可以考虑。同时可以看到生成模型的空间占用和计算速度都很不错。\n","date":1544587200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544587200,"objectID":"1da6b2d42e3eb8f8cf1f3561db6325cf","permalink":"https://leidawt.github.io/en/post/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BBnumpy%E5%AE%9E%E7%8E%B0/","publishdate":"2018-12-12T12:00:00+08:00","relpermalink":"/en/post/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BBnumpy%E5%AE%9E%E7%8E%B0/","section":"post","summary":"概述 贝叶斯属于生成模型的一种，其实现很简单，就是应用贝叶斯公式。这是一种指定先验分布，求后验的方法。 概率论课本里著名的贝叶斯公式如下 p(ci","tags":[],"title":"贝叶斯分类numpy实现","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"@[toc]\n参考与指针 均常用于函数需要传入被修改值时使用，指针与c用法相同而参考（reference)用\u0026amp;表达，如下面的函数用于交换两个值参考指向的是被代指对象，而不再拷贝。\nvoid swap(int \u0026amp;val1, int \u0026amp;val2) { int temp = val1; val1 = val2; val2 = temp; }  相比c风格用指针实现，都达到了传地址的目的，仅写法不同（不必同指针一样用* 和 -\u0026gt;了。\n堆栈 new delete 这是cpp的动态内存部分，开辟和回收内存。注意配对使用。\nint *pi; pi = new int;//申请 delete pi;//释放 pi = new int[10];//申请 delete[] pi;//释放  函数的高级参数写法和应用 /* ex1:给函数添加一个可选的默认关闭的输出流参数 */ void fun1(ofstream *ofil = 0) { if (ofil) { (*ofil) \u0026lt;\u0026lt; \u0026quot;hello\\n\u0026quot;; } } ofstream outfile(\u0026quot;data.txt\u0026quot;); fun1(\u0026amp;outfile);//传入 fun1();//默认  重载函数 同函数名，不同函数参数类型 如\nvoid disp(char ch); void disp(const string\u0026amp;);  模板函数 提供类型泛型抽象，时常与重载联用\n//声明一个模板函数 //const强调这里的引用不会改变传入值 template \u0026lt;typename eleType\u0026gt; void disp(const string \u0026amp;msg, const vector\u0026lt;eleType\u0026gt; \u0026amp;vec) { cout \u0026lt;\u0026lt; msg \u0026lt;\u0026lt;endl; for (unsigned int i = 0; i \u0026lt; vec.size(); i++) { eleType temp = vec[i]; cout \u0026lt;\u0026lt; temp \u0026lt;\u0026lt;endl; } } //测试 vector \u0026lt;int\u0026gt; a; a.push_back(1); a.push_back(2); a.push_back(3); vector \u0026lt;string\u0026gt; b; b.push_back(\u0026quot;hello\u0026quot;); b.push_back(\u0026quot;world\u0026quot;); disp(\u0026quot;a:\u0026quot;,a); disp(\u0026quot;b:\u0026quot;,b);  函数指针 典型形式如\nconst vector\u0026lt;int\u0026gt;* (*fun)(int); //这个指针用于指向具有const vector\u0026lt;int\u0026gt;*返回值，有一个int参数的函数 fun=the_fun;//将指针指向the_fun()函数 //应用： int fun1(int i) { return i+1; } int fun2(int i) { return i+2; } //构建以指向fun1类型的函数指针为成员的数组，并初始化 int(*fun_array[])(int) = { fun1,fun2 }; //这里enum的使用避免记忆fun1 fun2的数组下标 enum fun_names { n_fun1,n_fun2 }; //调用 cout\u0026lt;\u0026lt;fun_array[n_fun1](1);  迭代器初步 iterator 通常可理解为泛型指针，在构建泛型算法中用处很大。\nstring strs[] = { \u0026quot;hello\u0026quot;,\u0026quot;world\u0026quot; }; vector\u0026lt;string\u0026gt; vec(strs,strs+2);//利用上面的数组构建向量 vector\u0026lt;string\u0026gt;::iterator iter = vec.begin();//构建一个iterator while (iter != vec.end()) { cout \u0026lt;\u0026lt; *iter \u0026lt;\u0026lt; endl; iter++; }  函数对象（function object） 是一个程序设计的对象允许被当作普通函数来调用的特性 c++预置了一些\nequal_to\u0026lt;type\u0026gt;() 结果为(param1 == param2) not_equal_to\u0026lt;type\u0026gt;() 结果为(param1 != param2) less\u0026lt;type\u0026gt;() 结果为 (param1 \u0026lt; param2) greater\u0026lt;type\u0026gt;() 结果为(param1 \u0026gt; param2) less_equal\u0026lt;type\u0026gt;() 结果为 (param1 \u0026lt;= param2) greater_equal\u0026lt;type\u0026gt;() 结果为 (param1 \u0026gt;= param2) logical_not\u0026lt;type\u0026gt;() 结果为 (!param1) logical_and\u0026lt;type\u0026gt;() 结果为 (param1 \u0026amp;\u0026amp; param2) logical_or\u0026lt;type\u0026gt;() 结果为 (param1 || param2)  可如下使用\nint a[] = { 7,8,1,2,4,5,10,23,23 }; vector\u0026lt;int\u0026gt; vec(a, a + 9); //用函数对象做比较函数传入排序算法 sort(vec.begin(), vec.end(), greater\u0026lt;int\u0026gt;()); //遍历打印（use c++11新特性） for (int \u0026amp;i : vec) cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026quot; \u0026quot;;  Function Adapter 用于组合（combine）、变换（transform）、操作（manipulate）函数对象、特定参数值、或者特定函数。 STL提供了一些，如\nbind2nd(less\u0026lt;int\u0026gt;,val) //将val 绑定在less的第二个元素上，使得less与val比较  泛型算法实现样例 以实现泛型的过滤器为例，引入的迭代器起到泛型指针作用，避免对vector int[]等重载函数造成的重复实现，导入的函数指针扩展了过滤逻辑的灵活性\n//泛型过滤器，接收一对迭代器指向输入 //接收一个比较函数和一个比较值 //接收一个输出迭代器 template\u0026lt;typename Iter,typename ele\u0026gt; void filter(Iter first, Iter last,Iter out, ele val, bool(*perd)(ele, ele)) { while (first != last) { if (perd(*first, val)) { cout \u0026lt;\u0026lt; *first \u0026lt;\u0026lt; \u0026quot; \u0026quot;; *out++ = *first; } first++; } cout \u0026lt;\u0026lt; endl; } //比较函数 bool myless(int a, int b) { return a \u0026lt; b ? true : false; } //准备测试数据 const int ele_num = 9; int a[] = { 7,8,1,2,4,5,10,23,23 };//数据 int a1[ele_num] = { 0 };//接收结果数组 vector\u0026lt;int\u0026gt; vec(a, a + ele_num);//初始化向量 vector\u0026lt;int\u0026gt; vec1(ele_num);//接收向量 filter(vec.begin(),vec.end(),vec1.begin(), 5, myless);//调用 for (int \u0026amp;i : vec1)//遍历打印 cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026quot; \u0026quot;; cout \u0026lt;\u0026lt; endl; filter(a, a + ele_num, a1, 10, myless);//调用 for (int \u0026amp;i : a1)//遍历打印 cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026quot; \u0026quot;;  MAP简介 map\u0026lt;string, int\u0026gt; wordlist; string word; //这里cin在收到类型不满足string时返回0使得while退出 //通过输入ctrl+z 回车后退出 while (cin \u0026gt;\u0026gt; word) wordlist[word]++;//对输入的内容归类统计 for (auto \u0026amp;i : wordlist)//遍历打印，这里i auto为一个pair cout \u0026lt;\u0026lt; \u0026quot;str= \u0026quot;\u0026lt;\u0026lt;i.first\u0026lt;\u0026lt;\u0026quot; num= \u0026quot;\u0026lt;\u0026lt;i.second \u0026lt;\u0026lt; endl; //wordlist.find(\u0026quot;xxx\u0026quot;);检索 //wordlist.count(\u0026quot;xxx\u0026quot;);统计个数  类 基本 下面的例子展示类基础语法： 构造 解构 成员 方法 静态成员 静态方法\n//通常下面的类声明放到h文件中 class Test { public: //公开接口和属性 int a = 0; //静态成员，可在所有类实例里共享 static int static_member; //类函数声明 void print() const; //末尾const声明为一const 成员函数， //const 关键字只能放在函数声明的尾部，大概是因为其它地方都已经被占用了 //a. const对象只能访问const成员函数 //b. const对象的成员是不可修改的,然而const对象通过指针维护的对象却是可以修改的. //c. const成员函数不可以修改对象的数据 //d. 然而加上mutable修饰符的数据成员,对于任何情况下通过任何手段都可修改 void input_data(int*); void print_buf(); static void print_hello_world();//声明一个静态方法 //一组构造函数，因为短，习惯上实现和声明都写类里 //无参数构造函数，以 Test xxx;实例化时使用 Test() {}; //一般构造函数 Test(int num) { a = num; buf = new int[a];//申请一块内存,长度a } //一般构造函数 Test(int num1, int num2) { a = num1; b = num2; } //特殊构造函数，处理Test a=b;的初始化方式 Test(const Test \u0026amp;rhs); //解构函数，在类生命周期结束时调用 ~Test() { delete[] buf;//释放内存 } private: //私有 int b = 0; int * buf; }; //下面这些实现放cpp文件中 //这里的是类方法实现，Test::用于指明其属的类 Test::Test(const Test \u0026amp; rhs) { //默认的逐一copy方式不会分配新的heap空间，要手工处理下 a = rhs.a; b = rhs.b; buf = new int[a]; for (int i = 0; i \u0026lt; rhs.a; ++i) { buf[i] = rhs.buf[i]; } } void Test::print() const{ cout \u0026lt;\u0026lt; \u0026quot;a=:\u0026quot; \u0026lt;\u0026lt; a \u0026lt;\u0026lt;\u0026quot; b=\u0026quot; \u0026lt;\u0026lt; b \u0026lt;\u0026lt;endl; } void Test::input_data(int *data){ for (int i = 0; i \u0026lt; a; ++i) buf[i] = data[i]; } void Test::print_buf(){ for (int i = 0; i \u0026lt; a; ++i) cout \u0026lt;\u0026lt; buf[i] \u0026lt;\u0026lt; \u0026quot; \u0026quot;; cout \u0026lt;\u0026lt; endl; } //静态函数实现时不需要重复添加static关键字 void Test::print_hello_world(){ cout \u0026lt;\u0026lt; \u0026quot;hello world\u0026quot; \u0026lt;\u0026lt; endl; } //可在此处初始化，不可在函数中进行 int Test::static_member = 1; //main int buf[] = { 1,2,3 }; Test t0(3); t0.print(); t0.input_data(buf); Test t1 = t0; t1.print(); t0.print_buf(); t1.print_buf(); Test::print_hello_world();//使用静态函数，类似java static method cout \u0026lt;\u0026lt; Test::static_member \u0026lt;\u0026lt; endl;//访问  运算符重映射 注：全部的符号重载方法 http://www.runoob.com/cplusplus/cpp-overloading.html\nclass TestOpOrid { public: int num; TestOpOrid() { num = 0; } TestOpOrid(int n) { num = n; } //重载加，定义为返回num成员相加的新对象 TestOpOrid operator+ (const TestOpOrid\u0026amp; rhs) { TestOpOrid t; t.num = this-\u0026gt;num + rhs.num; return t; } //重载==为判断内部num相等 bool operator== (const TestOpOrid\u0026amp; rhs) { return this-\u0026gt;num == rhs.num; } }; //mian TestOpOrid a(1), b(2); cout \u0026lt;\u0026lt; \u0026quot;a==b \u0026quot; \u0026lt;\u0026lt; (a == b) \u0026lt;\u0026lt; endl; TestOpOrid c = a + 1; cout \u0026lt;\u0026lt; \u0026quot;c==b \u0026quot; \u0026lt;\u0026lt; (c == b) \u0026lt;\u0026lt; endl; return 0;  面向对象 c++ 以如下的语法支持面向对象\n//基础类 class LibMat { public: //定义virtual的方法就可重载了 virtual void print() const{ cout \u0026lt;\u0026lt; \u0026quot;LibMat print\u0026quot; \u0026lt;\u0026lt; endl; } }; //继承LibMat class Book : public LibMat { public: //注意这里的传参是写作const 引用，否则常数输入会报错 //另可简单写作Book(const int a,const int b),对传值类型，这样更好 Book(const int \u0026amp;a,const int \u0026amp;b) { this-\u0026gt;a = a; this-\u0026gt;b = b; } virtual void print() const{ //可这样调用上层方法 LibMat::print(); cout \u0026lt;\u0026lt; \u0026quot;Book print\u0026quot; \u0026lt;\u0026lt; endl; } void print_member() { cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt;','\u0026lt;\u0026lt; b \u0026lt;\u0026lt; endl; } protected: //此关键字表达此2个变量仅可在此派生类使用 int a, b; }; //继承机制使得操作一套类相当便利 void display(LibMat \u0026amp; o) { o.print(); } //main LibMat l; Book book(1,1); book.print(); book.print_member(); display(l); display(book);  模板类 tamplete类的声明使用 因编译的特化原因，通常的做法是将模板类实现直接写在头文件中。 h文件\ntemplate \u0026lt;typename T\u0026gt; class BinaryTree { public: BinaryTree(); ~BinaryTree(); BTnode\u0026lt;T\u0026gt; *_root; }; template \u0026lt;typename T\u0026gt; BinaryTree\u0026lt;T\u0026gt;::BinaryTree() { } template \u0026lt;typename T\u0026gt; BinaryTree\u0026lt;T\u0026gt;::~BinaryTree() { }  tamplete 默认参数特性 \r\r\r调用： \r\r\r","date":1541131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541131200,"objectID":"a5f32c0c7cbde4a1ea327edea44f5618","permalink":"https://leidawt.github.io/en/post/essential_cpp%E7%AC%94%E8%AE%B0/","publishdate":"2018-11-02T12:00:00+08:00","relpermalink":"/en/post/essential_cpp%E7%AC%94%E8%AE%B0/","section":"post","summary":"@[toc] 参考与指针 均常用于函数需要传入被修改值时使用，指针与c用法相同而参考（reference)用\u0026amp;表达，如下面的函数用于交换两个值参考","tags":[],"title":"essential_cpp笔记","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"@[toc]\n文档 https://docs.scipy.org/doc/numpy/reference/\n先导 引入 import numpy as np numpy 数据结构-ndarray numpy 使用的数组类是 ndarray \r\r\r一些重要属性如下： ndarray.ndim 维数 ndarray.shape 返回(n, m)，n行 m列 ndarray.dtype 类型 numpy 数据结构-mat mat是ndarray的派生，进行矩阵运算比ndarray方便 a=np.mat(\u0026lsquo;4 3; 2 1\u0026rsquo;) a=np.mat(np.array([[1,2],[3,4]])) mat 的*重载为矩阵乘法 求逆a.I\nnumpy常量 numpy.inf numpy.nan numpy.e numpy.pi\n创建数组（矩阵） a = np.array([2,3,4]) a = np.array([[1, 1], [1, 1]]) #指定类型 np.array( [ [1,2], [3,4] ], dtype=complex ) np.zeros( (3,4) ) np.ones( (2,3,4), dtype=np.int16 ) np.empty( (2,3) ) #值不初始化，为内存乱值 #创建数字序列 np.arange( 10, 30, 5 ) #array([10, 15, 20, 25]) np.arange(15).reshape(3, 5) #array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]])  附： 全部类型 \r\r\r操作数组（矩阵） 常见操作符均已重载，其中注意：*分配成了逐一乘（matlab中.*），矩阵乘法采用np.dot(A, B) 拷贝：d = a.copy() 在不同数组类型之间的操作，结果数组的类型趋于更普通或者更精确的一种 array的索引，切片和迭代与python[]同 切片 \r\r\r上下拼接 np.vstack((a,b)) 左右拼接 np.hstack((a,b))\n通用数学函数 https://docs.scipy.org/doc/numpy/reference/ufuncs.html\n广播规则* 当向量和矩阵结构不匹配响应运算时，会启用广播规则处理,可认为是一种自动补全机制 \r\r\r索引 \r\r\raxis axis=0，对每列操作 axis=1，对每行操作 如对 \r\r\r\r\r\r排序 若x为(n,) sorted_indices = np.argsort(x)#根据x产生升序排序索引 sorted_indices = np.argsort(-x)#根据x产生降序排序索引 这样使用sorted_indices 来排序其他array 若y为(n,m) y=y[sorted_indices,:]\n","date":1539662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539662400,"objectID":"df89356e9ef4de26c908431f3c497f8e","permalink":"https://leidawt.github.io/en/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A2%84%E5%A4%87-numpy/","publishdate":"2018-10-16T12:00:00+08:00","relpermalink":"/en/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A2%84%E5%A4%87-numpy/","section":"post","summary":"@[toc] 文档 https://docs.scipy.org/doc/numpy/reference/ 先导 引入 import numpy as np numpy 数据结构-ndarray numpy 使用的数组类是 ndarray 一些重要属性如下： ndarray.ndim 维数 ndarray.shape 返回(n, m)，n行 m列 ndarray.dtype 类型 numpy 数据结构-ma","tags":[],"title":"机器学习预备-numpy","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"用于以pwm信号控制模拟舵机 使用\n//@stm32f103 72mhz PWM_Init(1000-1,1440-1);//开12路pwm 50hz,arr=1000 RATIO_0(150);//设置位置，对模拟舵机CCR:100右极限，200左极限  h文件\n#ifndef __pwm_H #define __pwm_H #include \u0026quot;sys.h\u0026quot; /* 使用 PWM_Init(1000-1,1440-1);//开12路pwm 50hz,arr=1000,对模拟舵机 CCR:100右极限，200左极限 RATIO_0(150);//... */ #define RATIO_0(RATIO) TIM_SetCompare1(TIM3, RATIO) // PA6 #define RATIO_1(RATIO) TIM_SetCompare2(TIM3, RATIO) // PA7 #define RATIO_2(RATIO) TIM_SetCompare3(TIM3, RATIO) // PB0 #define RATIO_3(RATIO) TIM_SetCompare4(TIM3, RATIO) // PB1 #define RATIO_4(RATIO) TIM_SetCompare1(TIM4, RATIO) // PB6 #define RATIO_5(RATIO) TIM_SetCompare2(TIM4, RATIO) // PB7 #define RATIO_6(RATIO) TIM_SetCompare3(TIM4, RATIO) // PB8 #define RATIO_7(RATIO) TIM_SetCompare4(TIM4, RATIO) // PB9 #define RATIO_8(RATIO) TIM_SetCompare1(TIM5, RATIO) // PA0 #define RATIO_9(RATIO) TIM_SetCompare2(TIM5, RATIO) // PA1 #define RATIO_10(RATIO) TIM_SetCompare3(TIM5, RATIO) // PA2 #define RATIO_11(RATIO) TIM_SetCompare4(TIM5, RATIO) // PA3 void PWM_Init(u16 arr, u16 psc); //频率为7200000/psc/arr #endif // TIM_SetCompare2(TIM3,led0pwmval);调占空比  c文件\n#include \u0026quot;pwm.h\u0026quot; //TIM_SetCompare2(TIM3,led0pwmval);调占空比 //PWM频率 = 72M / ((arr+1)*(psc+1))(单位：Hz) //PWM占空比 = TIM3-\u0026gt;CCR1 / arr(单位：%) //TIM3 TIM4 TIM5 12路 PWM初始化 //PWM输出初始化 //arr：自动重装值 //psc：时钟预分频数 void PWM_Init(u16 arr,u16 psc) { GPIO_InitTypeDef GPIO_InitStructure; TIM_TimeBaseInitTypeDef TIM_TimeBaseStructure; TIM_OCInitTypeDef TIM_OCInitStructure; RCC_APB1PeriphClockCmd(RCC_APB1Periph_TIM3|RCC_APB1Periph_TIM4|RCC_APB1Periph_TIM5, ENABLE);\t//使能定时器345时钟 RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOA|RCC_APB2Periph_GPIOB| RCC_APB2Periph_AFIO, ENABLE); //使能GPIO外设和AFIO复用功能模块时钟 //设置该引脚为复用输出功能,输出TIM3 CH2的PWM脉冲波形\tGPIOA.7 GPIO_InitStructure.GPIO_Pin = GPIO_Pin_0|GPIO_Pin_1|GPIO_Pin_2|GPIO_Pin_3|GPIO_Pin_6|GPIO_Pin_7; //A组上管脚 GPIO_InitStructure.GPIO_Mode = GPIO_Mode_AF_PP; //复用推挽输出 GPIO_InitStructure.GPIO_Speed = GPIO_Speed_10MHz; GPIO_Init(GPIOA, \u0026amp;GPIO_InitStructure);//初始化GPIOA组 GPIO_InitStructure.GPIO_Pin = GPIO_Pin_0|GPIO_Pin_1|GPIO_Pin_6|GPIO_Pin_7|GPIO_Pin_8|GPIO_Pin_9;//B组上管脚 GPIO_Init(GPIOB, \u0026amp;GPIO_InitStructure);//初始化GPIOB组 //初始化TIM3,4,5 TIM_TimeBaseStructure.TIM_Period = arr; //设置在下一个更新事件装入活动的自动重装载寄存器周期的值 TIM_TimeBaseStructure.TIM_Prescaler =psc; //设置用来作为TIMx时钟频率除数的预分频值 TIM_TimeBaseStructure.TIM_ClockDivision = 0; //设置时钟分割:TDTS = Tck_tim TIM_TimeBaseStructure.TIM_CounterMode = TIM_CounterMode_Up; //TIM向上计数模式 TIM_TimeBaseInit(TIM3, \u0026amp;TIM_TimeBaseStructure); //根据TIM_TimeBaseInitStruct中指定的参数初始化TIMx的时间基数单位 TIM_TimeBaseInit(TIM4, \u0026amp;TIM_TimeBaseStructure); //根据TIM_TimeBaseInitStruct中指定的参数初始化TIMx的时间基数单位 TIM_TimeBaseInit(TIM5, \u0026amp;TIM_TimeBaseStructure); //根据TIM_TimeBaseInitStruct中指定的参数初始化TIMx的时间基数单位 //初始化TIM3,4,5 Channel,2,3,4 PWM模式\tTIM_OCInitStructure.TIM_OCMode = TIM_OCMode_PWM1; //选择定时器模式:TIM脉冲宽度调制模式2 TIM_OCInitStructure.TIM_OutputState = TIM_OutputState_Enable; //比较输出使能 TIM_OCInitStructure.TIM_OCPolarity = TIM_OCPolarity_High; //输出极性:TIM输出比较极性高 //根据T指定的参数初始化外设通道，共12个 TIM_OC1Init(TIM3, \u0026amp;TIM_OCInitStructure); TIM_OC2Init(TIM3, \u0026amp;TIM_OCInitStructure); TIM_OC3Init(TIM3, \u0026amp;TIM_OCInitStructure); TIM_OC4Init(TIM3, \u0026amp;TIM_OCInitStructure); TIM_OC1Init(TIM4, \u0026amp;TIM_OCInitStructure); TIM_OC2Init(TIM4, \u0026amp;TIM_OCInitStructure); TIM_OC3Init(TIM4, \u0026amp;TIM_OCInitStructure); TIM_OC4Init(TIM4, \u0026amp;TIM_OCInitStructure); TIM_OC1Init(TIM5, \u0026amp;TIM_OCInitStructure); TIM_OC2Init(TIM5, \u0026amp;TIM_OCInitStructure); TIM_OC3Init(TIM5, \u0026amp;TIM_OCInitStructure); TIM_OC4Init(TIM5, \u0026amp;TIM_OCInitStructure); //使能TIM 在CCR1,2,3,4上的预装载寄存器 TIM_OC1PreloadConfig(TIM3, TIM_OCPreload_Enable); TIM_OC2PreloadConfig(TIM3, TIM_OCPreload_Enable); TIM_OC3PreloadConfig(TIM3, TIM_OCPreload_Enable); TIM_OC4PreloadConfig(TIM3, TIM_OCPreload_Enable); TIM_OC1PreloadConfig(TIM4, TIM_OCPreload_Enable); TIM_OC2PreloadConfig(TIM4, TIM_OCPreload_Enable); TIM_OC3PreloadConfig(TIM4, TIM_OCPreload_Enable); TIM_OC4PreloadConfig(TIM4, TIM_OCPreload_Enable); TIM_OC1PreloadConfig(TIM5, TIM_OCPreload_Enable); TIM_OC2PreloadConfig(TIM5, TIM_OCPreload_Enable); TIM_OC3PreloadConfig(TIM5, TIM_OCPreload_Enable); TIM_OC4PreloadConfig(TIM5, TIM_OCPreload_Enable); //使能TIM3,4,5 TIM_Cmd(TIM3, ENABLE); TIM_Cmd(TIM4, ENABLE); TIM_Cmd(TIM5, ENABLE); }  ","date":1539576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539576000,"objectID":"ab32fb13bc94e97c0a64eb65cacfc4c1","permalink":"https://leidawt.github.io/en/post/stm32-12%E8%B7%AFpwm%E8%88%B5%E6%9C%BA%E6%8E%A7%E5%88%B6%E5%99%A8/","publishdate":"2018-10-15T12:00:00+08:00","relpermalink":"/en/post/stm32-12%E8%B7%AFpwm%E8%88%B5%E6%9C%BA%E6%8E%A7%E5%88%B6%E5%99%A8/","section":"post","summary":"用于以pwm信号控制模拟舵机 使用 //@stm32f103 72mhz PWM_Init(1000-1,1440-1);//开12路pwm 50hz,arr=1000 RATIO_0(150);//设置位","tags":[],"title":"stm32 12路pwm舵机控制器","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"这个模块提供了格式化发送字符串到HC05的功能，占用stm32的串口3 1.头文件\n#ifndef __HC_05 #define __HC_05 /* 本模块为HC05蓝牙透传模块，只写了通信，AT指令部分 单独用串口测试 因此本模块基本就是个串口 占用串口USART3 PB10 TX 5v兼容 PB11 RX 5v兼容 */ #include \u0026quot;sys.h\u0026quot; #define HC05_REC_LEN 100 //定义最大接收字节数 100 #define HC05_TEC_LEN 100 //发送缓存区 100 void initHC05(void);\t//初始化 void sendToHC05(char* fmt ,...);//发送到HC05 static void receiveHandler(void); static void SendCharToHC05(u8 ch);//私有 extern char HC05_RX_BUF[HC05_REC_LEN]; #endif  2.c文件\n#include \u0026quot;hc05.h\u0026quot; #include \u0026quot;stdarg.h\u0026quot; #include \u0026quot;stdio.h\u0026quot; #include \u0026quot;delay.h\u0026quot; #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026quot;main.h\u0026quot; //PB10 模块RX //BB11 模块TX //注意,读取USARTx-\u0026gt;SR能避免莫名其妙的错误 char HC05_RX_BUF[HC05_REC_LEN]; //接收缓冲 //接收状态 //bit15，\t接收完成标志 //bit14，\t接收到0x0d //bit13~0，\t接收到的有效字节数目 u16 HC05_RX_STA=0; //接收状态标记\tvoid initHC05(void){ GPIO_InitTypeDef GPIO_InitStructure; USART_InitTypeDef USART_InittStructure; NVIC_InitTypeDef NVIC_InitStructure; RCC_APB2PeriphClockCmd(RCC_APB2Periph_GPIOB, ENABLE); RCC_APB1PeriphClockCmd(RCC_APB1Periph_USART3, ENABLE); USART_DeInit(USART3); GPIO_InitStructure.GPIO_Pin = GPIO_Pin_10; GPIO_InitStructure.GPIO_Mode = GPIO_Mode_AF_PP; //TX复用推挽输出 GPIO_InitStructure.GPIO_Speed = GPIO_Speed_10MHz; GPIO_Init(GPIOB, \u0026amp;GPIO_InitStructure); GPIO_InitStructure.GPIO_Pin = GPIO_Pin_11; GPIO_InitStructure.GPIO_Mode = GPIO_Mode_IN_FLOATING; //RX浮空输入 GPIO_Init(GPIOB, \u0026amp;GPIO_InitStructure); USART_InittStructure.USART_BaudRate = 9600; //波特率 USART_InittStructure.USART_HardwareFlowControl = USART_HardwareFlowControl_None; //硬件流设置 USART_InittStructure.USART_Mode = USART_Mode_Rx|USART_Mode_Tx; //接收发送模式 USART_InittStructure.USART_Parity = USART_Parity_No; //奇偶校验位 USART_InittStructure.USART_StopBits = USART_StopBits_1; //停止位 USART_InittStructure.USART_WordLength = USART_WordLength_8b; //字长 USART_ITConfig(USART3, USART_IT_RXNE, ENABLE); //设置中断类型，接收中断 NVIC_InitStructure.NVIC_IRQChannel = USART3_IRQn; //串口3中断，在stm32F10x.h中有定义 NVIC_InitStructure.NVIC_IRQChannelPreemptionPriority = 0; //抢占优先级为3 NVIC_InitStructure.NVIC_IRQChannelSubPriority = 0; //响应优先级为3 NVIC_InitStructure.NVIC_IRQChannelCmd = ENABLE; //IRQ通道使能 NVIC_Init(\u0026amp;NVIC_InitStructure); USART_Init(USART3, \u0026amp;USART_InittStructure); USART_Cmd(USART3, ENABLE); } void USART3_IRQHandler(void) { u8 Res; if(USART_GetITStatus(USART3, USART_IT_RXNE) != RESET){ Res =USART_ReceiveData(USART3);\t//读取接收到的数据 if((HC05_RX_STA\u0026amp;0x8000)==0){\t//接收未完成 if(HC05_RX_STA\u0026amp;0x4000){\t//接收到了0x0d if(Res!=0x0a){ HC05_RX_STA=0;//接收错误,重新开始 } else { //接收完成了 HC05_RX_STA|=0x8000; //完成逻辑: printf(\u0026quot;recrived: %s\u0026quot;,HC05_RX_BUF); receiveHandler(); HC05_RX_STA=0; HC05_RX_STA\u0026amp;=~0x8000;//清除完成标志 }\t} else { //还没收到0X0D\tif(Res==0x0d){ HC05_RX_STA|=0x4000; } else { HC05_RX_BUF[HC05_RX_STA\u0026amp;0X3FFF]=Res ; HC05_RX_STA++; if(HC05_RX_STA\u0026gt;(HC05_REC_LEN-1))HC05_RX_STA=0;//接收数据错误,重新开始接收\t}\t} } } } //向蓝牙发送一个字节 void SendCharToHC05(u8 ch){ while((USART3-\u0026gt;SR\u0026amp;0X40)==0) ;//等待发送完毕 USART3-\u0026gt;DR = ch; } //格式化发送到蓝牙 void sendToHC05(char* fmt ,...){ unsigned char i,num; char lcd_buf[HC05_TEC_LEN]; va_list ap; va_start(ap,fmt); num=vsprintf(lcd_buf,fmt,ap); //printf(\u0026quot;num=%d\\r\\n\u0026quot;,num); for(i=0;i\u0026lt;num;i++){ SendCharToHC05(lcd_buf[i]); //printf(\u0026quot;char=%c\\r\\n\u0026quot;,lcd_buf[i]); } va_end(ap); } //接收回调，处理和解析命令 //指令格式：XXXX;arg0;arg1;arg2 void receiveHandler() { //指令表 const char* list[] = {\u0026quot;setpid\u0026quot;, \u0026quot;restart!\u0026quot;, \u0026quot;hello\u0026quot;,\u0026quot;SetCapBeginFlag\u0026quot;,\u0026quot;restart\u0026quot;}; unsigned char count = 0; double args[3] = {0}; char* token; char* cmd; char delim[] = \u0026quot;;\u0026quot;; int i; int length = sizeof(list) / sizeof(char*); //先获得命令,';'分割 cmd = strtok(HC05_RX_BUF, delim); printf(\u0026quot;cmd= %s\\r\\n\u0026quot;, cmd); //然后解析参数，最多3个，解析成double token = strtok(NULL, delim); while (token != NULL \u0026amp;\u0026amp; count \u0026lt; 3) { args[count] = atof(token); count++; token = strtok(NULL, delim); } for (i = 0; i \u0026lt; length; i++) { if (strcmp(cmd, list[i]) == 0) break; } //printf(\u0026quot;cmd N=%d\\r\\n\u0026quot;,i); //printf(\u0026quot;length=%d\\r\\n\u0026quot;,length); //按照解析的命令查找执行 if (i \u0026lt; length) { switch (i) { case 0: pid.Kp = args[0]; pid.Ki = args[1]; pid.Kd = args[2]; printf(\u0026quot;setpid:p=%f i=%f d=%f\\r\\n\u0026quot;,args[0],args[1],args[2]); savePIDdata(); __set_FAULTMASK(1);// 关闭所有中断 NVIC_SystemReset();// 复位 break; case 1: __set_FAULTMASK(1);// 关闭所有中断 NVIC_SystemReset();// 复位 printf(\u0026quot;cmd%d\\r\\n\u0026quot;, i); break; case 2: printf(\u0026quot;cmd%d\\r\\n\u0026quot;, i); printf(\u0026quot;hello world\\r\\n\u0026quot;); break; case 3: printf(\u0026quot;cmd%d\\r\\n\u0026quot;, i); pauseFlag=0; break; case 4: printf(\u0026quot;cmd%d\\r\\n\u0026quot;, i); pauseFlag=1; break; default: break; } } }  IO配置： PB10 TX 5v兼容 PB11 RX 5v兼容\n初始化initHC05() 其中发送可用格式化发送函数sendToHC05，与printf的格式相同 接收需要修改receiveHandler() 函数。接受到数据后会在中断里调用此函数\n","date":1539576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539576000,"objectID":"a2fe85150e0b48607153469aae7e2be7","permalink":"https://leidawt.github.io/en/post/stm32-hc05%E9%A9%B1%E5%8A%A8/","publishdate":"2018-10-15T12:00:00+08:00","relpermalink":"/en/post/stm32-hc05%E9%A9%B1%E5%8A%A8/","section":"post","summary":"这个模块提供了格式化发送字符串到HC05的功能，占用stm32的串口3 1.头文件 #ifndef __HC_05 #define __HC_05 /* 本模块为HC05蓝牙透传模块，只写了通信，AT指令","tags":[],"title":"stm32 HC05驱动","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"这个程序利用stm32普通定时器构成了执行时间测量功能和间隔执行函数功能，使用如下，两种功能复用，通过#define选择\n//执行时间测试模式下 #define StopWatch //使用： #include \u0026quot;Runtime.h\u0026quot; Runtime_init(); while(1){ Runtime_start(); delay_ms(1); Runtime_stop(); delay_ms(1000); } //setInterval模式 #define setInterval setInterval(fun,1000);//传入回调，周期1.024*1000ms  c文件\n#include \u0026quot;Runtime.h\u0026quot; unsigned int nTime = 0; void Runtime_init(void) { TIM_TimeBaseInitTypeDef TIM_TimeBaseStructure; NVIC_InitTypeDef NVIC_InitStu; RCC_APB1PeriphClockCmd(RCC_APB1Periph_TIM6, ENABLE); //使能TIM6时钟 #if defined(StopWatch) /*基础设置，最大可测时间定为8.192ms*/ TIM_TimeBaseStructure.TIM_Period = 65536 - 1; // arr放最大，以实现最大测量范围 #endif #if defined(setIntervalMODE) /*1.024ms溢出*/ TIM_TimeBaseStructure.TIM_Period = 8192 - 1; // arr放最大，以实现最大测量范围 #endif TIM_TimeBaseStructure.TIM_Prescaler = 9 - 1; //预分频 TIM_TimeBaseStructure.TIM_CounterMode = TIM_CounterMode_Up; //向上计数 TIM_TimeBaseInit(TIM6, \u0026amp;TIM_TimeBaseStructure); TIM_ITConfig(TIM6, TIM_IT_Update, ENABLE); //使能TIM6中断 TIM_Cmd(TIM6, ENABLE); //使能定时器6 NVIC_InitStu.NVIC_IRQChannel = TIM6_IRQn; //外部中断线，定时器6 NVIC_InitStu.NVIC_IRQChannelCmd = ENABLE; NVIC_InitStu.NVIC_IRQChannelPreemptionPriority = 1; //抢占优先级 NVIC_InitStu.NVIC_IRQChannelSubPriority = 1; //子优先级 NVIC_Init(\u0026amp;NVIC_InitStu); } #if defined(StopWatch) void TIM6_IRQHandler(void) { //判断是否为定时器6的更新中断 if (TIM_GetITStatus(TIM6, TIM_IT_Update) != RESET) { nTime++; //注意要清除中断标志 TIM_ClearITPendingBit(TIM6, TIM_IT_Update); } } void Runtime_start(void) { nTime = 0; //清次数 TIM_SetCounter(TIM6, 0); //清空定时器的CNT } void Runtime_stop(void) { unsigned int count = TIM6-\u0026gt;CNT; // TIM_GetCounter(TIM6); TIM_ITConfig(TIM6, TIM_IT_Update, DISABLE); //关TIM6中断 printf(\u0026quot;run time:%f us %f ms\\n\u0026quot;, (float)count / 8 + 8192 * nTime, (float)count / 8000 + 8.192 * nTime); TIM_ITConfig(TIM6, TIM_IT_Update, ENABLE); //使能TIM6中断 } #endif #if defined(setIntervalMODE) callbackType callback = NULL; unsigned int intervalTime=0; //初始化和设置setInterval void setInterval(callbackType cb,unsigned int time){ callback=cb; intervalTime=time; Runtime_init(); } void TIM6_IRQHandler(void) { //判断是否为定时器6的更新中断 if (TIM_GetITStatus(TIM6, TIM_IT_Update) != RESET) { nTime++; if(nTime\u0026gt;=intervalTime){ TIM_ITConfig(TIM6, TIM_IT_Update, DISABLE); //关TIM6中断 nTime=0; //需要定时执行的逻辑 if(callback){ callback(); } TIM_SetCounter(TIM6, 0); //清空定时器的CNT TIM_ITConfig(TIM6, TIM_IT_Update, ENABLE); //使能TIM6中断 } //注意要清除中断标志 TIM_ClearITPendingBit(TIM6, TIM_IT_Update); } } #endif  h文件\n#ifndef __RUNTIME__ #define __RUNTIME__ ////////MODE SET///////// //#define StopWatch #define setIntervalMODE ///////////////////////// #include \u0026quot;sys.h\u0026quot; #include \u0026quot;usart.h\u0026quot; typedef void (*callbackType)(void); void Runtime_init(void); //初始化 #if defined(StopWatch) void Runtime_start(void); //开始执行时间测试 void Runtime_stop(void); //结束执行时间测试，打印结果到串口 #endif #if defined(setIntervalMODE) void setInterval(callbackType cb,unsigned int time);//初始化和设置setInterval #endif #endif  ","date":1539576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539576000,"objectID":"68931c4306c35ad6218a38ae6852fc61","permalink":"https://leidawt.github.io/en/post/stm32-%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4%E6%B5%8B%E9%87%8F%E4%B8%8E%E9%97%B4%E9%9A%94%E6%89%A7%E8%A1%8C/","publishdate":"2018-10-15T12:00:00+08:00","relpermalink":"/en/post/stm32-%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4%E6%B5%8B%E9%87%8F%E4%B8%8E%E9%97%B4%E9%9A%94%E6%89%A7%E8%A1%8C/","section":"post","summary":"这个程序利用stm32普通定时器构成了执行时间测量功能和间隔执行函数功能，使用如下，两种功能复用，通过#define选择 //执行时间测试模式","tags":[],"title":"stm32 运行时间测量与间隔执行","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"机器学习预备系列博客记述服务机器学习的使用前导知识 记录下python下绘图的方法 首先引用下cheet sheet 工作流 \r\r\r典型操作\n#! python3 # -*- coding:utf-8 -*- import matplotlib.pyplot as plt # 基本绘图的引用 from matplotlib import style # 为使用更漂亮的风格 # 折线图 plt.plot([1, 2, 3], [4, 5, 6], label='first line') plt.plot([1, 2, 3], [7, 8, 9], label='second line') plt.xlabel('x') plt.ylabel('y') plt.title('demo') plt.legend() # 画图例 plt.show() # 散点图 x = [1, 2, 3, 4, 5, 6, 7, 8] y = [5, 2, 4, 2, 1, 4, 5, 2] plt.scatter(x, y, label='skitscat', color='k', s=25, marker=\u0026quot;o\u0026quot;) plt.xlabel('x') plt.ylabel('y') plt.title('Interesting Graph\\nCheck it out') plt.legend() plt.show() # 从文件绘图 import csv # 读入文件 x = [] y = [] with open('Matplotlib_basic_data.csv', 'r') as csvfile: plots = csv.reader(csvfile, delimiter=',') for row in plots: x.append(int(row[0])) y.append(int(row[1])) # 或利用numpy #x, y = np.loadtxt('example.txt', delimiter=',', unpack=True) plt.plot(x, y, label='Loaded from file!') plt.xlabel('x') plt.ylabel('y') plt.title('Interesting Graph\\nCheck it out') plt.legend() plt.show() # 多图 style.use('fivethirtyeight') # 使用更漂亮的风格 fig = plt.figure() ax1 = fig.add_subplot(221) # 高2 宽2 图号1 ax2 = fig.add_subplot(222) # 高2 宽2 图号2 x1 = [1, 2, 3] y1 = [2, 5, 4] x2 = [1, 2, 3] y2 = [5, 8, 7] ax1.plot(x1, y1) ax2.plot(x2, y2) plt.show()  最后附完整的cheet sheet \r\r\r","date":1539576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539576000,"objectID":"c82acd0863b360b7abe5b68f6b7dea32","permalink":"https://leidawt.github.io/en/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A2%84%E5%A4%87-matplotlib%E7%BB%98%E5%9B%BE/","publishdate":"2018-10-15T12:00:00+08:00","relpermalink":"/en/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A2%84%E5%A4%87-matplotlib%E7%BB%98%E5%9B%BE/","section":"post","summary":"机器学习预备系列博客记述服务机器学习的使用前导知识 记录下python下绘图的方法 首先引用下cheet sheet 工作流 典型操作 #! python3 # -*- coding:utf-8 -*- import matplotlib.pyplot as plt # 基","tags":[],"title":"机器学习预备-Matplotlib绘图","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"先导 1.pandas 依赖numpy包 2.pandas 以dataframe 和 series两种对象承载数据，前者是二维的，后者一维的 3.pandas 对dataframe 的操作逻辑是返回新的操作过的dataframe ， 所以这样才可更新数据操作： df=df.drop(columns=[\u0026lsquo;B\u0026rsquo;, \u0026lsquo;C\u0026rsquo;]) 或 df.drop(columns=[\u0026lsquo;B\u0026rsquo;, \u0026lsquo;C\u0026rsquo;],inplace=True) 两者等价 4.dataframe 和 series均重载了内部magic function，可想列表一样索引 5.print(df.head())来获取表结构\n总结下，pandas的工作流程大致如下 \r\r\r1.获取导入数据 从字典构建pd数据结构\ndf = { 'a': [1, 2, 3], 'b': [2, 3, 4] } df = pd.DataFrame(df) print(df.head())  从文件构建pd数据结构\ndf = pd.read_csv('./data.csv') df = pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states') #从二进制pickle文件,重要 datas = pd.read_pickle('all_state_data.pickle') #...  all导入函数如下 \r\r\r2.节选 #节选行 df=df[1:3] #matlab方式选行列 df.ix[:,0:2] df.ix[1:3,3:4] #节选某些行（下为meters\u0026lt;1000的） df = df[(df['meters'] \u0026lt; 1000)] #删除某列 df = df.drop('AL', axis=1) df = df.drop(columns=['a','b'])  3.剔除坏值 #直接删除含有NaN行 df.dropna(inplace=True) #NaN用前填补 df.fillna(method='ffill',inplace=True) #NaN用后填补 df.fillna(method='bfill',inplace=True) #NaN填值定值 df.fillna(value=-99999,inplace=True) #替换值 df.replace([np.inf, -np.inf], np.nan, inplace=True)  4.预处理 结构\n#设置索引行为a df.set_index('a', inplace=True) #重置索引名a-\u0026gt;b df = df.rename(columns={'a': 'b'}) #添加新行c df['c']=df['a']+df['b']  滑动窗类统计 形如df.rolling(xxx).xxx() rolling填滑动窗统计 \r\r\r自定义函数\ndef handle(n): return n**2 df['meters'] = df['meters'].apply(handle) print(df)  5.导出 绘图 这时需另行引入matplotlib\nimport matplotlib.pyplot as plt from matplotlib import style style.use('fivethirtyeight') df.plot.line() plt.show()  所有绘图类型\r\r\r\r导出到文件 形如 datas.to_pickle(\u0026lsquo;datas.pickle\u0026rsquo;) 函数表见导入部分 pickle形式存为二进制流，最高效\n","date":1539576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539576000,"objectID":"bb555686948ab80ca4cec1c632f57ac4","permalink":"https://leidawt.github.io/en/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A2%84%E5%A4%87-pandas%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/","publishdate":"2018-10-15T12:00:00+08:00","relpermalink":"/en/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A2%84%E5%A4%87-pandas%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/","section":"post","summary":"先导 1.pandas 依赖numpy包 2.pandas 以dataframe 和 series两种对象承载数据，前者是二维的，后者一维的 3.pandas 对dataframe 的操作逻辑是返回新","tags":[],"title":"机器学习预备-Pandas数据预处理","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"主要简介动力学系统的建模理论与simulink工程实现，并给出仿真slx文件下载\nsimulink实用技巧 为更好的建立模型，归纳了以下技巧 1.双击信号线可对其命名，信号命名后便于监视绘图 \r\r\r2.用信号监视器比scope要更加方便 \r\r\r如上添加监视后可在此查看 \r\r\r3.关于求解器，通常使用自动选择的求解器。常见ode45，这是基于泰勒展开的方法。会自动根据4 5阶的结果差选择仿真步长。在变化快处计算密集，缓慢处计算稀疏。一般限制其最大允许仿真步长保证时间分辨率 \r\r\r4.为了模拟真实世界传感器从连续世界中读取的离散值，一般用零阶保持器离散结果。 \r\r\r5.simulink模型层次 一个好的模型要有清晰的层次结构，使用子系统模块先搭建好顶层模型，再对各个模块细节进行实现 \r\r\r官方样例清晰的表达了一个扫地机器人建模的最佳实践 https://www.mathworks.com/help/simulink/gs/architecture-and-interfaces.html \r\r\r6.simscape 为更加便利化物理系统，可使用simscape 下面的例子对其构成进行了基本简介 \r\r\r基本理论 机械系统常常由下面三个元件构成 1.惯性元件 2.弹性元件 F=k*(derta x) 3.阻尼表示如下 \r\r\r要特别注意偶数次的符号，一般如下处理 \r\r\r下面是重要的基本物理原理 \r\r\r\r\r\r系统分析的常用数学知识有 虚位移原理 http://netedu.xauat.edu.cn/jpkc/netclass/jpkc/lx/jxzy/wljc/15.pdf 泛函与变分原理 http://www.cad.zju.edu.cn/home/zhx/FAVM/1.pdf 自动控制原理 现代控制原理\n弹球例子 仿真一个从10m处以15m/s向上抛出的弹性球，每次反弹速度衰减为0.8倍 首先由力学建立方程 \r\r\r之后搭建仿真如下，注意积分初值 \r\r\r仿真后从查看器查看速度和位移图如下 \r\r\r状态空间例子 \r\r\r\r非线性例子仿真波形 \r\r\r线性仿真波形 \r\r\r一个磁悬浮的例子 现仿真一个电磁铁吸引小球的系统。 \r\r\r如图，对求做受力分析建立模型，其中忽略空气阻力，将电磁力简化成仅与输入电流和电磁铁与球距离线性相关。 这里的建模要注意避免代数环问题，原子系统在仿真中会被认为是直通而在闭环中造成代数环问题，所有这里的子系统不可使用原子子系统，而应使用子系统 \r\r\r用自动整定进行pid参数选择 首先先手动粗略调节使得系统为稳定系统 之后打开整定器 \r\r\r系统特殊，自动线性化失败。通过新插入plant选择稳定区让系统去识别即可 \r\r\r选择稍靠后的稳定区后即可成功线性化 \r\r\r即得最终仿真波形 \r\r\r倒立摆 1.考虑转动 利用是刚体的假设，有 M（力矩）=J（转动惯量）*a（角加速度） 2.考虑质心受力 \r\r\r\r\r\r不难列写如下的微分方程，其中XY指力,m杆质量 M车质量： 对杆 \r\r\r\r\r\r\r\r\r对小车 \r\r\r简化：令sin(x)=x cos(x)=1 消元导出 \r\r\r令m=0.1 M=1 l=1 j=0.003 g=10 \r\r\r取x theta 为输出 \r\r\r模型得到之后，进行控制，即极点配置，关键是得到反馈阵K\nmatlab计算如下 A=[0 1 0 0;0 0 -0.8834 0;0 0 0 1;0 0 19.4346 0]; B=[0;0.9893;0;-1.7667]; P=[-2+2*sqrt(3)1i,-2-2sqrt(3)*1i,-10,-10]; K=acker(A,B,P)\n之后再simulink中搭建如下 \r\r\r其中gain模块如下配置成矩阵形式 \r\r\r最终的控制效果极佳，角度变化很小 \r\r\r这个模型是实际可用的，不需再加观测器，两只编码器就可测得所有参数\n下载 pan 链接: https://pan.baidu.com/s/1y0DYSMCjhJWX7kCLcn_mKA 提取码: vwgy\n","date":1539489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539489600,"objectID":"536e4ab2a6191d5960ac6bd5aeae5b98","permalink":"https://leidawt.github.io/en/post/%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%B3%BB%E7%BB%9Fsimulink%E5%BB%BA%E6%A8%A1%E5%88%86%E6%9E%90/","publishdate":"2018-10-14T12:00:00+08:00","relpermalink":"/en/post/%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%B3%BB%E7%BB%9Fsimulink%E5%BB%BA%E6%A8%A1%E5%88%86%E6%9E%90/","section":"post","summary":"主要简介动力学系统的建模理论与simulink工程实现，并给出仿真slx文件下载 simulink实用技巧 为更好的建立模型，归纳了以下技巧 1.","tags":[],"title":"动力学系统simulink建模分析","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"@[toc]\n列表生成器 使用列表生成器可以简便的产生列表，省去for 注意，python3 对其中num变量视作局部变量，而python2则不是，这会影响其上下文的同名变量值！！！ 列表生成器和列表推导几乎替代了map reduce filter等高阶函数的绝大多数功能，一般不再使用高阶函数和匿名函数\nl = [num*2 for num in range(1, 5)] print(l) #[2, 4, 6, 8] la = [1, 2] lb = [4, 5, 6] l = [Point(a, b) for a in la for b in lb] #[Point(x=1, y=4), Point(x=1, y=5), Point(x=1, y=6), Point(x=2, y=4), Point(x=2, y=5), Point(x=2, y=6)]  生成器generator 语法与列表生成器类似，仅改为（）。区别在于不一次性计算出列表，二是返回一个可迭代对象generator object，这是可迭代的，可以直接送入max,sum等函数\ndef a(i): return i*2 def b(i): return i*3 l = [a, b] print(list(each(10) for each in l))#[20,30] print(max(each(10) for each in l))#30  列表推导 对于过滤等操作很方便\nsymbols = '$¢£¥€¤' beyond_ascii = [ord(s) for s in symbols if ord(s) \u0026gt; 127] #[162, 163, 165, 8364, 164] beyond_ascii = list(filter(lambda c: c \u0026gt; 127, map(ord, symbols))) #[162, 163, 165, 8364, 164]  运算符类与map_reduce 尽管列表推导，生成器代替了绝大多数map_reduce应用，仍有些时候会用到。如计算hash值时需要将一系列值异或起来，如下。 其中operator类提供了所有运算符，避免写lambda函数\nimport functools import operator def hash_all(h): hashes = (hash(x) for x in h) # 这里做一个可迭代的生成器 return functools.reduce(operator.xor, hashes, 0) #调用运算符类和reduce算法计算h中所有元素相异或的值  namedtuple命名元组 用于方便的构建仅有几个属性的类，生成的namedtuple可以像类一样引用其属性，如下namedtuple用于表示平面点坐标\nimport collections Point = collections.namedtuple('Point', ['x', 'y']) p1 = Point(1, 2) p2 = Point(3, 4) print(p1.x)  元组 t=(a,b) first,second=t#拆包 print(t[0]) a, b, *rest = range(5)#平行赋值，多余的存成列表赋给rest #嵌套的 a=(1,2,(1,2)) q,w,e=a #q=1 w=2 e=(1,2)  切片 切片的下标逻辑如下，在索引前切开 \r\r\r还可以用如下的形式对 s 在 a 和 b之间以 c 为间隔取值。c 的值还可以为负 \r\r\r借助切片可方便的赋值 \r\r\r排序与插入 fruits = ['grape', 'raspberry', 'apple', 'banana'] #一般排序，原列表并没有变化，返回排好的 print(sorted(fruits)) #传入排序依据函数 print(sorted(fruits, key=len)) fruits.sort()#就地排序 import bisect import random random.seed(1729)#输入随机种子 my_list = [random.randrange(1, 10) for each in range(10)]#生成0-9随机类列表 print(my_list) my_list.sort()#就地排序 bisect.insort(my_list, 3)#按顺序插入（二分） print(my_list)  数组 接近c数组，可降低list开销\nfrom array import array from random import random floats = array('d', (random() for i in range(100)))#‘d’用于指出存储类型 fp = open('floats.bin', 'wb') floats.tofile(fp)#可输出为二进制 fp.close() floats2 = array('d') fp = open('floats.bin', 'rb') floats2.fromfile(fp, 100)#可从二进制输入 print(floats[0])#可如列表一样引用  DICT a = ['a', 'b', 'c'] b = [1, 2, 3] d = zip(a, b)#返回zip对象，可迭代出(a,b) dic = dict(d) print(dic['a']) dicc=dic #只读DICT from types import MappingProxyType d = {1:'A'} d_proxy=MappingProxyType(d) print(d_proxy[1])#只读，不可修改 d[2]='B' #之后d_proxy会同步更新  SET set是无重复集。与数学上的集合相似，可进行集合运算\na = [1, 2, 3, 3] b = [2, 3, 4] sa = set(a) sb = set(b) print(sa | sb) print(sa-sb) print(sa \u0026amp; sb) print(sa ^ sb)#求差集 #另有子集，包含等判断  函数 这是一个例子，其中 第一个参数后面的任意个参数会被 *content 捕获，存入一个元组 tag 函数签名中没有明确指定名称的关键字参数会被 **attrs 捕 获，存入一个字典。 cls 参数只能作为关键字参数传入。\ndef tag(name, *content, cls=None, **attrs): \u0026quot;\u0026quot;\u0026quot;生成一个或多个HTML标签\u0026quot;\u0026quot;\u0026quot; if cls is not None: attrs['class'] = cls if attrs: attr_str = ''.join(' %s=\u0026quot;%s\u0026quot;' % (attr, value) for attr, value in sorted(attrs.items())) else: attr_str = '' if content: return '\\n'.join('\u0026lt;%s%s\u0026gt;%s\u0026lt;/%s\u0026gt;' % (name, attr_str, c, name) for c in content) else: return '\u0026lt;%s%s /\u0026gt;' % (name, attr_str)  函数内省 在计算机编程中，自省是指这种能力：检查某些事物以确定它是什么、它知道什么以及它能做什么。即当你拿到一个“函数对象”的时候，你可以继续知道，它的名字，参数定义状况等信息。python关于参数信息存放在函数的特殊属性中，时实际不方便使用，借助inspect包可很好处理 内省常用于检查传入的函数是否符合要求\nfrom inspect import signature def myfun(a, b=1, *c): pass sig = signature(myfun) print(sig) for name, param in sig.parameters.items(): print(param.kind, ':', name, '=', param.default)  其中kind有五种 POSITIONAL_OR_KEYWORD 可以通过定位参数和关键字参数传入的形参（多数 Python 函数的参 数属于此类）。 VAR_POSITIONAL 定位参数元组。 VAR_KEYWORD 关键字参数字典。 KEYWORD_ONLY 仅限关键字参数（Python 3 新增）。 POSITIONAL_ONLY 仅限定位参数；目前，Python 声明函数的句法不支持，但是有些使 用 C 语言实现且不接受关键字参数的函数（如 divmod）支持。\n函数注解 python3重要的新特性，用于为函数声明中的参数和返回值附加元数据\ndef clip(text:str, max_len:'int \u0026gt; 0'=80) -\u0026gt; str: ➊ \u0026quot;\u0026quot;\u0026quot;在max_len前面或后面的第一个空格处截断文本 \u0026quot;\u0026quot;\u0026quot; end = None if len(text) \u0026gt; max_len: space_before = text.rfind(' ', 0, max_len) if space_before \u0026gt;= 0: end = space_before else: space_after = text.rfind(' ', max_len) if space_after \u0026gt;= 0: end = space_after if end is None: # 没找到空格 end = len(text) return text[:end].rstrip()  函数声明中的各个参数可以在 : 之后增加注解表达式。如果参数有默认值，注解放在参数名和 = 号之间。如果想注解返回值，在 ) 和函数声明末尾的 : 之间添加 -\u0026gt; 和一个表达式。那个表达式可以是任何类型。注解中最常用的类型是类（如 str 或 int）和字符串（如 \u0026lsquo;int \u0026gt;0\u0026rsquo;）。在示例 5-19 中，max_len 参数的注解用的是字符串。python 对注解所做的唯一的事情是，把它们存储在函数的__annotations__ 属性里 print(signature(f).parameters.values()) 可调出属性\nABC抽象方法 python本身不提供抽象方法，可用ABC类来辅助检查\nfrom abc import ABC, abstractmethod class Base(ABC): @abstractmethod def load(self, input): \u0026quot;\u0026quot;\u0026quot;Retrieve data from the input source and return an object.\u0026quot;\u0026quot;\u0026quot; return class A(Base): def load(self, input): print(input) class B(Base): pass a = A() b = B()#err  变量作用域\u0026amp;闭包 Python 编译函数的定义体时，默认吧内部变量看做局部变量，若实则指全局，需要显示的声明 global xxx\n闭包指延伸了作用域的函数，其中包含函数定义体中引用、但是不在定义体中定义的非全局变量,如\ndef make_averager(): series = [] def averager(new_value): series.append(new_value) total = sum(series) return total/len(series) return averager  这个函数返回avr函数，其中使用了series存储历史值 当make_averager返回时，它会保留定义函数时存在的自由变量的绑定（此处的series），这样调用函数时，虽然定义作用域不可用了，但是仍能使用那些绑定！\n通过使用nonlocal关键字，可不用列表实现上述函数（这样可正确闭包，否则解释器会将count,total视作局部变量，不加以闭包绑定）\ndef make_averager(): count = 0 total = 0 def averager(new_value): nonlocal count, total count += 1 total += new_value return total / count return averager  装饰器 装饰器的一大特性是，能把被装饰的函数替换成其他函数。第二个特性是，装饰器在加载模块时立即执行。（函数装饰器在导入模块时立即执行，而被装饰的函数只在明确调用时运行） 装饰器的典型行为：把被装饰的函数替换成新函数，二者接受相同的参数，而且（通常）返回被装饰的函数本该返回的值，同时还会做些额外操作。 比如可借助装饰器帮助注册函数到列表 叠放：越靠近函数的先起装饰作用\nfun_list = [] def isfun(fun): fun_list.append(fun) return fun @isfun def a(i): return i*2 @isfun def b(i): return i*3 print(fun_list) #[\u0026lt;function a at 0x0000000000C2EEA0\u0026gt;, \u0026lt;function b at 0x0000000000C2EF28\u0026gt;]  为更好的处理传入函数的参数和特殊属性等，最佳实践是利用functools 中wraps来辅助构建，如下的例子实现了执行时间测试功能，注意传参的处理\nimport time from functools import wraps def clock(func): @wraps(func) def clocked(*args, **kwargs): t0 = time.time() result = func(*args, **kwargs) elapsed = time.time() - t0 name = func.__name__ arg_lst = [] if args: arg_lst.append(', '.join(repr(arg) for arg in args)) if kwargs: pairs = ['%s=%r' % (k, w) for k, w in sorted(kwargs.items())] arg_lst.append(', '.join(pairs)) arg_str = ', '.join(arg_lst) print('[%0.8fs] %s(%s) -\u0026gt; %r ' % (elapsed, name, arg_str, result)) return result return clocked @clock def fun(a, b, **c): print('a=', a, 'b=', b) print(c) fun(1, 2, x=1, y=2) print(fun.__name__) # 不改变名称  标准库内置了几个很有用的装饰器 1. import functools @functools.lru_cache() 作用是用缓存优化递归算法（通过保存调用结果），此外lru_cache 在从 Web 中获取信息的应用中也能发挥巨大作用。 可接受参数 functools.lru_cache(maxsize=128, typed=False) maxsize 参数指定存储多少个调用的结果 typed 参数如果设为 True，把不同参数类型得到的结果分开保存 被 lru_cache 装饰的函数会有 cache_clear 和 cache_info 两个方法，分别用于清除缓存和查看缓存信息。 2 @classmethod 用于将方法变成java中静态方法的概念，即不必生成实例即可调用\n@classmethod def pargs(self, *args): print(*args)  python思维-鸭子类型 “当看到一只鸟走起来像鸭子、游泳起来像鸭子、叫起来也像鸭子，那么这只鸟就可以被称为鸭子。” 在python的思维中，我们并不关心对象是什么类型，到底是不是鸭子，只关心行为，这一点是和java完全不同的。如下的例子，可见所谓鸭子类型对多态特性的轻巧实现。同样的python提供的大量特殊方法（magic method）也应用了这一思想，如任何实现了 __iter__ 和 __next__方法的对象都可称之为迭代器，而任何类只要实现__getitem__方法，那python的解释器就会把它当做一个collection\nclass Duck: def quack(self): print \u0026quot;Quaaaaaack!\u0026quot; class Bird: def quack(self): print \u0026quot;bird imitate duck.\u0026quot; class Doge: def quack(self): print \u0026quot;doge imitate duck.\u0026quot; def in_the_forest(duck): duck.quack() duck = Duck() bird = Bird() doge = Doge() for x in [duck, bird, doge]: in_the_forest(x)  对象的一些问题 首先一些概念声明 ==与is == 运算符比较两个对象的值（对象中保存的数据），而 is 比较对象的 标识。可以在自己的类中定义 __eq__ 方法，决定 == 如何比较实例。如果不覆盖 __eq__ 方法，那么从 object 继承的方法比较对象的 ID 浅复制 如下，只复制浅层，子列表仍指向相同的内存\nl1 = [1, 2, [1, 2]] l2 = list(l1) print(id(l1) == id(l2))#flase print(id(l1[-1]) == id(l2[-1]))#true  若要深复制，使用copy包的deepcopy函数 传参 python传参全部为传引用 这会导致一个不易察觉的问题，即在类中函数用可变对象作为函数的默认参数时，可能会导致多个实例引用到同一个内存上，导致错误。\nclass常用特殊方法 特殊方法的使用可使类更符合python的思维，便于接入python的str,sum,==迭代，散列等优秀特性。如下面的类\nclass Test: a = 0 b = None def __init__(self, a, b=None): self.a = a self.b = b # 用于被str()方法调用 def __str__(self): return '__str__ method' # 调试信息 def __repr__(self): return '__repr__ method' # 用于==运算符 def __eq__(self, other): #print(self.a, other.a) return self.a == other.a # 定义散列函数，使得可用set def __hash__(self): return hash(self.a) # 迭代。这里返回b的生成器 def __iter__(self): return (each for each in self.b) # 使得实例可用len() def __len__(self): return len(self.b) if self.b is not None else 0 # 这是声明了静态方法 @classmethod def pargs(self, *args): print(*args)  另外实现如下的两个特殊方法可实现序列属性，可以实现切片和迭代特性，并可实现[] 访问\nclass Test: _components = [] def __init__(self, l): self._components = l def __len__(self): return len(self._components) def __getitem__(self, index): return self._components[index] t = Test([1, 2, 3]) print(t[0]) for each in t: print(each)  全部特殊方法： 流畅的python.pdf-p57\n异常 通过捕获异常可以避免脚本直接退出 典型异常处理如下\ntry: fh = open(\u0026quot;testfile\u0026quot;, \u0026quot;w\u0026quot;) fh.write(\u0026quot;hello world!!\u0026quot;) except IOError: #捕获IOError错误 print(\u0026quot;Error: open fail\u0026quot;) else: #不发生异常时执行，和try是一对 print(\u0026quot;success!\u0026quot;) fh.close()  python定义好的标准异常有如下这些 http://www.runoob.com/python/python-exceptions.html 可如下发起异常\nraise Exception(\u0026quot;Invalid level!\u0026quot;)  或raise其他标准异常，通常不必自定义异常类\n封包与import from…import 与 import 区别在于import的要用 模块名.函数名 来调用，而前者直接用函数名即可。import机制会自动处理重复引用问题，无需多虑 包 包就是文件夹，只要该文件夹下存在 __init__.py 文件。 http://www.runoob.com/python/python-modules.html 一般简单的多文件项目不必使用包，直接import 文件名即可引入\n单元测试 使用内置unittest进行 需要import unittest并在测试文件里创建一个测试类。测试文件采用 被测文件名_test.py命名。测试类继承unittest.TestCase来获得测试方法。测试类的测试函数要以test_t开头。 有如下常用的测试函数\n#测试值相等 self.assertEqual(add(0, 0), 0) #测试是否正确抛出异常 with self.assertRaises(AttributeError): value = d.empty  测试例子如下:\n#temp.py def add(a, b): return a+b def sub(a, b): return a-b  #temp_test.py import unittest from temp import add, sub class TestTest(unittest.TestCase): def test_add(self): self.assertEqual(add(1, 2), 3) self.assertEqual(add(2, 3), 5) self.assertEqual(add(3, 3), 6) self.assertEqual(add(4, 3.1), 7.1) # self.assertTrue() def test_sub(self): self.assertEqual(sub(2, 1), 1)  在vscode中，会自动识别出测试类，下方信息条展示测试情况 \r\r\r\r\r\r\r\r\r","date":1537934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537934400,"objectID":"df2020b85d8f637bd1d38e9409b3369d","permalink":"https://leidawt.github.io/en/post/python%E7%AC%94%E8%AE%B0/","publishdate":"2018-09-26T12:00:00+08:00","relpermalink":"/en/post/python%E7%AC%94%E8%AE%B0/","section":"post","summary":"@[toc] 列表生成器 使用列表生成器可以简便的产生列表，省去for 注意，python3 对其中num变量视作局部变量，而python2则不是，这会影响其","tags":[],"title":"python笔记","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"1.考虑直流指标 \r\r\r1.1输入偏置电流与输入失调电流 定义运算放大器两输入端流进或流出直流电流的平均值为输入偏置电流Ib。由输入级的输入电阻不足造成（input bias current） 输入失调电流是运算放大器两输入端输入偏置电流之差的绝对值(input offest current) 由输入级不对称造成。 这两个参数对输出造成的总误差为 \r\r\r1.2输入失调电压 而为了使Vo＝0 而必须在V+和V－间加入的矫正电压Vos 即被称为运算放大器的输入失调电压（input offset voltage）。 这个参数对输出造成的误差为\nVo=Vos(1+R2/R1)\n2.考虑交流指标 2.1增益带宽积 闭环增益与带宽的乘积。留100倍余量时增益精度很好。\n2.2压摆率 \r\r\r2.3总谐波失真加噪声（THD+N） 相关指标： 信号与噪声加失真比SINAD: SINAD＝20 log(1/THD+N) 以dB 为单位 有效位数ENOB： ENOB＝（SINAD－1.76）/ 6.02 （位）\n","date":1537588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537588800,"objectID":"b3cedb51d26376810566057dd6135496","permalink":"https://leidawt.github.io/en/post/%E8%BF%90%E6%94%BE%E9%80%89%E5%9E%8B%E7%AD%96%E7%95%A5/","publishdate":"2018-09-22T12:00:00+08:00","relpermalink":"/en/post/%E8%BF%90%E6%94%BE%E9%80%89%E5%9E%8B%E7%AD%96%E7%95%A5/","section":"post","summary":"1.考虑直流指标 1.1输入偏置电流与输入失调电流 定义运算放大器两输入端流进或流出直流电流的平均值为输入偏置电流Ib。由输入级的输入电阻不足造","tags":[],"title":"运放选型策略","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"1.何为运放噪声 放大器的噪声模型如下 \r\r\r大致有三部分组成，运放电压噪声，运放电流噪声，和反馈电阻产生的噪声。三者的平方和开根号就是总噪声。在处理精密信号时，显然噪声的问题就很关键了。 2.运放噪声基础 \r\r\r运放的噪声信息以上图这样的形式给出，这是一个频率谱密度图。因为噪声和频率相关，此图表征了噪声包含的各个频率分量的大小。图分两个区间，前面线性下降部分称为闪烁噪声，后半部分为宽白噪声。对此频谱积分得到总噪声。再讨论噪声带宽问题，显然上图直接积分到正无穷是无限大，显然不合理。其实噪声也是有带宽的。这个带宽决定于运放的带宽。和运放带宽-3db定义不同，噪声定义在完全截止处。 \r\r\r因此存在经验修正系数，如下。 \r\r\r注意普通放大器看做一个一阶滤波器。 2.实战噪声计算 以opa842构成的10倍放大器为例子 首先从手册找出增益带宽积算出截止频率（这个频率会做为上面的频谱积分的上限） \r\r\r所以带宽为200/10=20MHz 乘上1.57修正为31.4MHz 接下来找出噪声参数 \r\r\r再找出噪声频谱图 \r\r\r下面的工作交给Bruce Trump的一款计算软件了（网上可下载），就是个excel表 先算闪烁噪声 \r\r\r左上角黄色格子依次填入闪烁噪声：100Hz处20nv/根号赫兹，以及白噪声2.6nv/根号赫兹。都可以从下图读出 \r\r\r右下角写闪烁噪声和白噪声的转折频率。前面的默认写0.1Hz，后面写刚才算出的频谱31.4MHz。 得到如下的结果 \r\r\r接下来切换到总噪声计算页面。由说明书填写数据。 \r\r\r\r\r\r得到结果了： \r\r\r这个数再乘根号下31.4MHz就是总电压噪声169微伏，对于精密电路相当的大了。还可以看到绝大部分噪声来自电压噪声。\n","date":1528257600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528257600,"objectID":"d9f06207b84879758d18937ce6cb2552","permalink":"https://leidawt.github.io/en/post/%E6%94%BE%E5%A4%A7%E5%99%A8%E5%99%AA%E5%A3%B0%E5%88%86%E6%9E%90%E8%AE%A1%E7%AE%97/","publishdate":"2018-06-06T12:00:00+08:00","relpermalink":"/en/post/%E6%94%BE%E5%A4%A7%E5%99%A8%E5%99%AA%E5%A3%B0%E5%88%86%E6%9E%90%E8%AE%A1%E7%AE%97/","section":"post","summary":"1.何为运放噪声 放大器的噪声模型如下 大致有三部分组成，运放电压噪声，运放电流噪声，和反馈电阻产生的噪声。三者的平方和开根号就是总噪声。在处理","tags":[],"title":"放大器噪声分析计算","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"连续函数的卷积过程动画展示，修改函数定义可以做任意函数卷积，注意分段函数的定义要正确。 为方便起见，只计算了[-3,3]区间的函数值 代码如下\n%任意函数卷积过程演示 clc clear %定义函数 f=@(x) (x.*0+1).*(x\u0026gt;=0 \u0026amp; x\u0026lt;1)+0;%0-1的阶跃 高1 g=@(x) (x.*0+2).*(x\u0026gt;=0 \u0026amp; x\u0026lt;1)+0;%0-1的阶跃 高2 %计算并画出f，仅计算[-3,3] x_of_f=-3:0.01:3; y_of_f=f(x_of_f); figure(1) hold on grid on plot(x_of_f,y_of_f,'r'); axis ([-3 3 0 3]) %调整坐标显示范围 xlabel('τ','FontSize',16); x_of_g=-3:0.01:3;%这个区域下计算g for t = -3:0.04:3 %定义conv(t)=f(t)*g(t)，下面描绘不同t下的情况，构成动画 y_of_g=g(t-x_of_g);%tao为自变量反折后平移t，这里是一个向量，一组tao y_of_g_plot=plot(x_of_g,y_of_g,'b');%画出g pause(0.001);%暂停 delete(y_of_g_plot);%删除原曲线 %求卷积，仅在-3，3区间积分即可 sum=0; for tao=-3:0.01:3 sum=sum+0.01*(f(tao)*g(t-tao)); end disp(sum); plot(t,sum,'.'); end hold off  \r\r\r","date":1523160000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523160000,"objectID":"44c5e58800467c058fd105dffb24d685","permalink":"https://leidawt.github.io/en/post/matlab%E5%8D%B7%E7%A7%AF%E5%8A%A8%E7%94%BB%E6%BC%94%E7%A4%BA/","publishdate":"2018-04-08T12:00:00+08:00","relpermalink":"/en/post/matlab%E5%8D%B7%E7%A7%AF%E5%8A%A8%E7%94%BB%E6%BC%94%E7%A4%BA/","section":"post","summary":"连续函数的卷积过程动画展示，修改函数定义可以做任意函数卷积，注意分段函数的定义要正确。 为方便起见，只计算了[-3,3]区间的函数值 代码如下 %","tags":[],"title":"MATLAB卷积动画演示","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"####Vue+VUX ui库+Cordova打包混合应用 ####模板工程详见文末pan链接 ####1.项目的Cordova基本命令 初始化文件夹 cordova create cordova-app com.lxlazy.www.app cordovaApp 进入 cd cordova-app 添加平台使用630API不然找不到 cordova platform add android@6.3.0 检查依赖 cordova requirements 真机调试 cordova run 添加插件 cordova plugin add XXXX 查看安装的插件 cordova plugins 卸载插件 cordova plugin remove XXXX 编译 cordova build android ####2.项目的npm命令 开启开发服务器 npm run dev 编译web文件，更新cordova设置，编译apk并下载到手机运行 npm run update ####3.工程结构 \r\r\rindex.html是主入口 src文件夹包含vue组件，vue路由在main.js中，assets存放静态文件。 vue-cordova文件夹存放的是一个vue插件，用于衔接vue和cordova提供的插件。项目地址：\rvue-cordova\r####4.添加cordova插件到项目 首先运行cordova plugin add XXXX安装好插件 然后注册到vue-cordova插件方便vue组件使用，如下操作： \r\r\r以添加蓝牙插件为例： 创建cordova-plugin-bluetooth-serial.js文件到上图位置，内容如下\nexports.install = function (Vue, options, cb) { document.addEventListener('deviceready', () =\u0026gt; { if (typeof bluetoothSerial === 'undefined') { return cb(false) } // pass through the bluetoothSerial object Vue.cordova.bluetoothSerial = bluetoothSerial return cb(true) }, false) }  然后在上述目录的index.js注册 \r\r\r最后，在vue中使用的方法\ndata: function () { return { cordova: Vue.cordova } }  插件加载好后会被vue-cordova挂在在这里。 这样拿到后就可以参考cordova的相应文档使用了。 ######注意项目中App.vue中mounted内函数对页面的触发控制，只有这样后续才可正常加载！ cordova的插件也可从window上获取，如window.navigator.vibrate(100) ####5.调试 先在dev下完善ui和逻辑，也可在真机上调试web: 打开 Chrome 浏览器，输入地址chrome://inspect，默认进入 chrome://inspect/#devices，将在页面显示当前可用设备，点击链接弹出控制台界面，然后跟普通页面一样调试 之后使用npm run update安装到手机进行混合应用调试 ####6.图标 png图标放在res/android目录下，可分大中小，详见cordova 之后修改congig.xml中响应内容即可 ####7.一些常用cordova插件 一 震动手机 安装vibrate插件 window.navigator.vibrate(Time in ms); 二 蓝牙串口 安装 bluetoothSerial 项目地址\r这是个和蓝牙串口模块通信的插件 使用流程： 获取设备列表 Vue.cordova.bluetoothSerial.list 链接设备 Vue.cordova.bluetoothSerial.connect 向设备发送 Vue.cordova.bluetoothSerial.write 监听设备回复 Vue.cordova.bluetoothSerial.subscribe 三 存储简单数据 简便方法是使用h5的storage特性，无存储期限限制，但大小不得过大，\r详细介绍\r例子如下\nSaveCurrentData: function () { //var value = storage.getItem(key); // 传递键的名字获取对应的值。 var value = JSON.stringify(this.pineappleNums);//将要存储的内容序列化 //console.log(\u0026quot;SaveCurrentData: \u0026quot;, value); localStorage.setItem(\u0026quot;pineappleNums\u0026quot;, value) // 传递键的名字和对应的值去添加或者更新这个键值对。 //storage.removeItem(key) // 传递键的名字去从LocalStorage里删除这个键值对。 }, restoreData: function () { var value = localStorage.getItem(\u0026quot;pineappleNums\u0026quot;); // 传递键的名字获取对应的值。 console.log(\u0026quot;restoreData:\u0026quot;, JSON.parse(value)); if (value != null) { this.pineappleNums = JSON.parse(value); } }  ####demo工程下载： 链接：https://pan.baidu.com/s/1wcaCn4PPFGnSxBcYjWFKEg 密码：7w8b 无需再npm install\n","date":1522555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522555200,"objectID":"b6831df171538356e1c0ebd48cd561c2","permalink":"https://leidawt.github.io/en/post/vuecordova%E6%B7%B7%E5%90%88%E5%BA%94%E7%94%A8/","publishdate":"2018-04-01T12:00:00+08:00","relpermalink":"/en/post/vuecordova%E6%B7%B7%E5%90%88%E5%BA%94%E7%94%A8/","section":"post","summary":"####Vue+VUX ui库+Cordova打包混合应用 ####模板工程详见文末pan链接 ####1.项目的Cordova基本命令 初始化文件夹 cordova create cordova-app com.lxlazy.www.app cordovaApp 进入 cd cordova-app","tags":[],"title":"vue\u0026cordova混合应用","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"概述 为在嵌入式开发中碰到的算法验证问题，借助matlab平台可以更方便的调试。如控制算法，可以先验证算法编写的正确性，防止盲目调参的无用功。借助的是SIMULINK 与 S-Function Builder\n方法 首先保证 matlab MEX部分能正常工作，可以参考 mex -setup相关信息。主要是让mex找到正确的编译器，如：已安装了gcc套件，则只需设置下环境变量：\tsetenv(\u0026lsquo;MW_MINGW64_LOC\u0026rsquo;,\u0026lsquo;F:\\MinGW64\u0026rsquo;); 即可 S-Function 是一个将c c++ 等编译为simulink模块的工具，吧待检测的代码构建为simulink 标准模块便可借助 simulink 强大功能仿真了。S-Function 有特定的格式，可以手写，这里用更简单的S-Function Builder做。 下面以一个iir滤波器算法为例 1.待验证的iir实现如下，保存为iir_souce.c\n/** * @brief 离散 IIR 滤波器算法，被s function builder * 使用并建立iir.c(与builder设置的函数名相同) * * @param u 输入 * @param xD 离散变量寄存器， sfuncton builder 提供 * @return double 输出 */ double iir(double u, double* xD) { double y; // IIR 的查分方程： y(n)=0.2x(n)+0.3x(n-1)+0.5x(n-2) //定义 xD[0] x(n-1) xD[1] x(n-2) y = 0.2 * u + 0.3 * xD[0] + 0.5 * xD[1]; xD[1] = xD[0]; xD[0] = y; return y; }  2.开启新的simulink，导入S-Function Builder 模块，双击打开编辑 2.1 命名 要求与刚才c文件不同命，不然会覆盖\n\r\r\r\r2.2 离散状态设置 这是s function 特殊性，就是离散化的算法中的x[n-1]这样的历史值需要构建为离散状态。如上图设置两个，给x[n-1] x[n-2]用 2.3 函数输入输出设置 iir只需要单输入单输出 宽度都为1 double(默认的) 格式 \r\r\r2.4 说明要编译的文件 左侧 iir_souce.c 指出要编译的文件 右侧extern double iir(double u, double* xD); 提示一会儿要在后面用到里面的这个函数 \r\r\r2.5 这一步指出实现 y0[0]=iir(u0[0],xD); u0[0] 是输信号，y0[0]为输出信号，xD是刚才定义的离散状态数组 \r\r\r2.6 编译 如图，默认设置就好，还可以产生TLC用于matlab builder 自动生成代码 \r\r\r2.7测试 构建如下系统： \r\r\r\r\r\r至此，成功将目标代码构建到sinmulink中仿真 #参考 官方文档\r参考书 基于模型的设计及其嵌入式实现\n","date":1520136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520136000,"objectID":"7a2652675c9a36949507ed9c547dab1d","permalink":"https://leidawt.github.io/en/post/%E5%80%9F%E5%8A%A9matlab%E4%B8%8Esimulink%E4%BB%BF%E7%9C%9F%E5%B5%8C%E5%85%A5%E5%BC%8Fc%E7%AE%97%E6%B3%95/","publishdate":"2018-03-04T12:00:00+08:00","relpermalink":"/en/post/%E5%80%9F%E5%8A%A9matlab%E4%B8%8Esimulink%E4%BB%BF%E7%9C%9F%E5%B5%8C%E5%85%A5%E5%BC%8Fc%E7%AE%97%E6%B3%95/","section":"post","summary":"概述 为在嵌入式开发中碰到的算法验证问题，借助matlab平台可以更方便的调试。如控制算法，可以先验证算法编写的正确性，防止盲目调参的无用功。","tags":[],"title":"借助MATLAB与SIMULINK仿真嵌入式C算法","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"通过设置MATLAB内部环境变量可以让MEX找到已经安装的编译器\nMATLAB环境变量 %示例 %设置并检索环境变量 TEMP 的新值： setenv('TEMP', 'C:\\TEMP'); getenv('TEMP') #将 Perl\\bin 文件夹附加到您的系统 PATH 变量： setenv('PATH', [getenv('PATH') ';D:\\Perl\\bin']);  ####MEX编译器的环境变量设置\n%可以这样初始化,选择编译器 mex -setup -v; %若环境变量不正确则更改即可，如： setenv('MW_MINGW64_LOC','F:\\MinGW64');  ","date":1520049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520049600,"objectID":"8aeb794465baf4513284b9cf5f37dcd9","permalink":"https://leidawt.github.io/en/post/matlab-mex-%E9%85%8D%E7%BD%AE%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/","publishdate":"2018-03-03T12:00:00+08:00","relpermalink":"/en/post/matlab-mex-%E9%85%8D%E7%BD%AE%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F/","section":"post","summary":"通过设置MATLAB内部环境变量可以让MEX找到已经安装的编译器 MATLAB环境变量 %示例 %设置并检索环境变量 TEMP 的新值： setenv('TEMP', 'C:\\TEMP'); getenv('TEMP') #将 Perl\\bin 文件夹附","tags":[],"title":"MATLAB MEX 配置与环境变量","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"#配置 使用插件Junit Generator 来辅助进行测试 ##插件配置 设置中搜索 Junit Generator找到配置项，可更改下JUnit 的模板，以解决乱码和依赖，方法为添加import static org.junit.Assert.*;并在下面注释中删除日期，以避免编码问题的乱码 ##使用 选中类名，右键Generate,JUnit4. 在相应位置编写测试：\n/** * Method: add2int(int a, int b) */ @Test public void testAdd2int() throws Exception { assertEquals(3, new MainClass().add2int(1,2)); }  ctrl+shift+F10运行测试 #JUnit ##使用断言\n void assertEquals(boolean expected, boolean actual) 检查两个变量或者等式是否平衡 void assertTrue(boolean expected, boolean actual) 检查条件为真 void assertFalse(boolean condition) 检查条件为假 void assertNotNull(Object object) 检查对象不为空 void assertNull(Object object) 检查对象为空 void assertSame(boolean condition) assertSame() 方法检查两个相关对象是否指向同一个对象 void assertNotSame(boolean condition) assertNotSame() 方法检查两个相关对象是否不指向同一个对象 void assertArrayEquals(expectedArray, resultArray) assertArrayEquals() 方法检查两个数组是否相等  ##套件测试 测试套件意味着捆绑几个单元测试用例并且一起执行他们。\nimport org.junit.runner.RunWith; import org.junit.runners.Suite; @RunWith(Suite.class) @Suite.SuiteClasses({ TestJunit1.class, TestJunit2.class }) public class JunitTestSuite { }  ##其他 @Test(timeout=1000) 来限定执行时间上限 @Test(expected = ArithmeticException.class) 来进行异常测试 #参考 http://wiki.jikexueyuan.com/project/junit/\n","date":1518408000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518408000,"objectID":"56752d7455b0fc9011fe33912c9a0dbc","permalink":"https://leidawt.github.io/en/post/idea-junit-java%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/","publishdate":"2018-02-12T12:00:00+08:00","relpermalink":"/en/post/idea-junit-java%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/","section":"post","summary":"#配置 使用插件Junit Generator 来辅助进行测试 ##插件配置 设置中搜索 Junit Generator找到配置项，可更改下JUnit 的模板，以解决乱码和依赖，方","tags":[],"title":"IDEA JUnit JAVA单元测试","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"1.特殊运算符 .*\t数组乘法 （逐位相乘 .的作用，下同） .^ 数组求幂运算符 \\\t矩阵左除 /\t矩阵右除 .\\\t阵列左除 ./\t阵列右除 %\t注释标志\n2.矩阵、向量的创建 r = [7 8 9 10 11] %行向量 r = [7 8 9 10 11]\u0026rsquo; %列向量 m = [1 2 3; 4 5 6; 7 8 9] %矩阵\n3. 常用命令 系统 clc\t清除命令窗口 clear\t从内存中删除变量 help\t搜索帮助主题 disp\t显示一个数组或字符串的内容 input\t显示提示并等待输入 MATLAB向量，矩阵和阵列命令\n用于数组、矩阵、向量 cat\t连接数组 find\t查找非零元素的索引 length\t计算元素数量 linspace\t创建间隔向量 logspace\t创建对数间隔向量 max\t返回最大元素 min\t返回最小元素 prod\t计算数组元素的连乘积 reshape\t重新调整矩阵的行数、列数、维数 size\t计算数组大小 sort\t排序每个列 sum\t每列相加 eye\t创建一个单位矩阵 ones\t生成全1矩阵 zeros\t生成零矩阵 cross\t计算矩阵交叉乘积 dot\t计算矩阵点积 det\t计算数组的行列式 inv\t计算矩阵的逆 pinv\t计算矩阵的伪逆 rank\t计算矩阵的秩 rref\t将矩阵化成行最简形 cell\t创建单元数组 celldisp\t显示单元数组 cellplot\t显示单元数组的图形表示 num2cell\t将数值阵列转化为异质阵列 deal\t匹配输入和输出列表 iscell\t判断是否为元胞类型\n4. 逻辑语句 if a=1; if a\u0026lt;2 disp('a\u0026lt;2'); end a=1; if a\u0026lt;2 disp('a\u0026lt;2'); elseif a\u0026lt;3 disp('a\u0026lt;3'); else disp('a\u0026gt;=2'); end  while a=0; while a\u0026lt;20 disp(a); a=a+1; end  for for a = 10:20 disp(a); end for a = 1.0: -0.1: 0.0 disp(a); end for a = [24,18,17,23,28] disp(a); end  5.向量矩阵的索引与操作 ##向量\nr=[1 2 3 4]; a=r(1);%a=1 b=r(1:2);%a=[1 2] c=r(:);%c=[1;2;3;4] %追加 a=[1 2]; b=[3 4]; c=[a,b];%c=[1 2 3 4] c=[a;b];%c=[1 2;3 4] dot(a, b);%点乘  ##矩阵\nm=[1 2 3;4 5 6;7 8 9]; a=m(1,2);%2 a=m(:,1:2);%前两列 m(:,1)=[];%删除第一列 mm=[m,m];%矩阵组合 randm=rand(3, 5);%0-1均匀分布的随机3*5矩阵  #6.函数\n%add_two_num.m function y=add_two_num(a,b) y=a+b; %隐函数 power = @(x, n) x.^n; result1 = power(7, 3)  注意函数名与m文件名要一致，每个m文件只含有一个主函数，可有若干子函数辅助主函数（包含在主函数内部） #6.数据导入导出 保存工作区：save命令 导入工作区：load命令 load xxx.mat 导入JSON ：json2data=loadjson(\u0026lsquo;xxx.json\u0026rsquo;) #7快捷键 【Ctrl+C】在命令窗口输入，使得运行的程序停下来 【Ctrl+R】注释（对多行有效） 【Ctrl+T】去掉注释（对多行有效） 【F5】 运行程序 【选中代码+F9】执行选中的代码\n","date":1518321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518321600,"objectID":"58dcb86f1e1290a10d812a7081f4322e","permalink":"https://leidawt.github.io/en/post/matlab%E5%9F%BA%E7%A1%80%E5%A4%87%E5%BF%98/","publishdate":"2018-02-11T12:00:00+08:00","relpermalink":"/en/post/matlab%E5%9F%BA%E7%A1%80%E5%A4%87%E5%BF%98/","section":"post","summary":"1.特殊运算符 .* 数组乘法 （逐位相乘 .的作用，下同） .^ 数组求幂运算符 \\ 矩阵左除 / 矩阵右除 .\\ 阵列左除 ./ 阵列右除 % 注释标志 2.矩阵、向量的创建 r =","tags":[],"title":"MATLAB基础备忘","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"####通常直接拍摄书籍纸张总是灰乎乎的，应用opencv自适应阈值二值化可以很方便的将照片转化为清晰的二值化照片，打印出来也不会是黑的，代码如下：\n#! python2 # -*- coding:utf-8 -*- import numpy as np import cv2 import sys import os def handle(imgDir): name = imgDir.split(\u0026quot;.\u0026quot;)[0] fmt = imgDir.split(\u0026quot;.\u0026quot;)[1] img = cv2.imread(imgDir, 0) # img = cv2.medianBlur(img, 5) # 中值滤波 # 自适应阈值二值化 img = cv2.adaptiveThreshold( img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 5, 2) img = cv2.medianBlur(img, 5) # 中值滤波 #cv2.imshow(\u0026quot;img\u0026quot;, img) cv2.imwrite(name + \u0026quot;_cv.\u0026quot; + fmt, img) print imgDir + \u0026quot; process successfully!\u0026quot; for arg in sys.argv: print arg if arg.split(\u0026quot;.\u0026quot;)[1] not in [\u0026quot;py\u0026quot;, \u0026quot;exe\u0026quot;]: try: handle(str(arg)) except: print \u0026quot;process \u0026quot; + arg + \u0026quot; failed!\u0026quot; os.system('pause')  ####效果 \r\r\r\r\r\r####可直接拖拽想要转换的图来启动，会把转换的图片加_cv后缀保存到原地址处，通过pyinstaller打包成exe就可以跨平台了\n","date":1516593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516593600,"objectID":"1e2095a94023fa520d2bbcdf0245e7bd","permalink":"https://leidawt.github.io/en/post/%E5%9B%BE%E7%89%87%E4%BA%8C%E5%80%BC%E5%8C%96%E5%B7%A5%E5%85%B7/","publishdate":"2018-01-22T12:00:00+08:00","relpermalink":"/en/post/%E5%9B%BE%E7%89%87%E4%BA%8C%E5%80%BC%E5%8C%96%E5%B7%A5%E5%85%B7/","section":"post","summary":"####通常直接拍摄书籍纸张总是灰乎乎的，应用opencv自适应阈值二值化可以很方便的将照片转化为清晰的二值化照片，打印出来也不会是黑的，代","tags":[],"title":"图片二值化工具","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"首先安装json解码工具箱 在此页面 https://cn.mathworks.com/matlabcentral/fileexchange/33381-jsonlab--a-toolbox-to-encode-decode-json-files 下载工具箱zip 解压复制到{安装路径}\\MATLAB\\R2015b\\toolbox下 matlab下添加路径\naddpath('{安装路径}\\MATLAB\\R2015b\\toolbox\\jsonlab-1.5')  使用 json2data=loadjson('xxx.json')  即可导入工作区，解析为struct类型\n","date":1513483200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513483200,"objectID":"f25d63f467224efffecc1aa2ac9f900d","permalink":"https://leidawt.github.io/en/post/matlab%E5%AF%BC%E5%85%A5json%E6%95%B0%E6%8D%AE/","publishdate":"2017-12-17T12:00:00+08:00","relpermalink":"/en/post/matlab%E5%AF%BC%E5%85%A5json%E6%95%B0%E6%8D%AE/","section":"post","summary":"首先安装json解码工具箱 在此页面 https://cn.mathworks.com/matlabcentral/fileexchange/33381-jsonlab--a-toolbox-to-encode-decode-json-files 下载工具箱zip 解压复制到{安装路径}\\MATLAB\\R2015b\\toolbox下 matlab下添加路","tags":[],"title":"MATLAB导入json数据","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"推荐一款神奇的gif录屏小软件，开源免费 安装后打开即可录屏 支持win,macos 下载：https://www.cockos.com/licecap/ github:https://github.com/justinfrankel/licecap\n","date":1513396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513396800,"objectID":"4ddf6e7e5e9f5d2a703d19ee3fd7d001","permalink":"https://leidawt.github.io/en/post/%E4%B8%80%E6%AC%BEgif%E5%BD%95%E5%B1%8F%E5%B0%8F%E8%BD%AF%E4%BB%B6/","publishdate":"2017-12-16T12:00:00+08:00","relpermalink":"/en/post/%E4%B8%80%E6%AC%BEgif%E5%BD%95%E5%B1%8F%E5%B0%8F%E8%BD%AF%E4%BB%B6/","section":"post","summary":"推荐一款神奇的gif录屏小软件，开源免费 安装后打开即可录屏 支持win,macos 下载：https://www.cockos.com/lice","tags":[],"title":"一款GIF录屏小软件","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"####argparse是一个按照UNIX规范从命令行读取对程序传参的python模块常用使用方式如下\ndemo.py #! python2 # -*- coding:utf-8 -*- import argparse # 引入 parser = argparse.ArgumentParser() # 初始化解析器 # 下面列举常用的参数形式 # 添加参数解析 -a 解析为二进制，激活为True, 默认False，添加帮助指导 “a binary arg” parser.add_argument('-a', action=\u0026quot;store_true\u0026quot;, default=False, help=\u0026quot;a binary arg\u0026quot;) # 添加参数解析 -b 解析为字符串 parser.add_argument('-b', help=\u0026quot;a str arg\u0026quot;) # 添加参数解析 -b 解析为int parser.add_argument('-c', type=int, help=\u0026quot;a int arg\u0026quot;) # 添加参数解析 -b 解析为float parser.add_argument('-d', type=float, help=\u0026quot;a float arg\u0026quot;) args = vars(parser.parse_args())# 从命令行读参数，解析到args print args[\u0026quot;a\u0026quot;] print args[\u0026quot;b\u0026quot;] print args[\u0026quot;c\u0026quot;] print args[\u0026quot;d\u0026quot;]  使用时 python demo.py -a -b hello -c 123 -d 1.23  结果 True hello 123 1.23  推荐阅读 http://blog.xiayf.cn/2013/03/30/argparse/\nhttps://docs.python.org/2/howto/argparse.html#introducing-optional-arguments\n","date":1513137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513137600,"objectID":"15d822aae8dffb16d3ff7a2c570bc77f","permalink":"https://leidawt.github.io/en/post/python-argparse%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90%E5%99%A8%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/","publishdate":"2017-12-13T12:00:00+08:00","relpermalink":"/en/post/python-argparse%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90%E5%99%A8%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/","section":"post","summary":"####argparse是一个按照UNIX规范从命令行读取对程序传参的python模块常用使用方式如下 demo.py #! python2 # -*- coding:utf-8 -*- import argparse # 引入 parser = argparse.ArgumentParser() # 初始化","tags":[],"title":"python argparse参数解析器使用笔记","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"MATLAB离散控制系统仿真常用操作 1.离散传递函数构建 通过离散化连续时间传递函数得到 连续时间传递函数s函数用tf构建：\nGc=tf([1],[1 1 0]);%参数为分子分母降幂排列S的系数  对其离散化：通常在对象前面加上一个零阶保持器\r\r\r\rans_zoh=c2d(Gc,1,'zoh')%添加零阶保持器离散化T=1 ans_imp=c2d(Gc,1,'imp')%或直接离散化T=1 %后者等效 ilaplace(1/(s^2+s))%拉普拉斯反变换 compose(ans,1*t)%T=1采样，t-\u0026gt;T*t ztrans(ans)%z变换 pretty(vpa(collect(ans)))%整理显示  2.性能分析 rlocus(ans_zoh);%根轨迹 bode(ans_zoh);%伯德图 nyquist(ans_zoh)%奈奎斯特图  查看系统根轨迹 可从根轨迹明显看到差异，非理想采样的加入降低了稳定性 \r\r\r奈奎斯特图： \r\r\r伯德图： 红色：连续 蓝色：经zero保持器采样 黄色：经理想采样 \r\r\r3.simulink 先按照连续时间方法搭建系统 然后应用离散化工具，选择离散化方法，采样时间对模型离散化 \r\r\r\r\r\r\r\r\r仿真得到不同采样周期下的情况如下 T=1: \r\r\rT=0.1； \r\r\r","date":1512705600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512705600,"objectID":"191aef05ab05b58c632104280ad4761a","permalink":"https://leidawt.github.io/en/post/matlab%E7%A6%BB%E6%95%A3%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/","publishdate":"2017-12-08T12:00:00+08:00","relpermalink":"/en/post/matlab%E7%A6%BB%E6%95%A3%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F/","section":"post","summary":"MATLAB离散控制系统仿真常用操作 1.离散传递函数构建 通过离散化连续时间传递函数得到 连续时间传递函数s函数用tf构建： Gc=tf([1],[1 1 0]);%参数为","tags":[],"title":"MATLAB离散控制系统","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"在stm32f1系列使用dsp库 ##获取dsp库 在keil mdk 版本中，dsp库集成与runtime environment之中，可以在keil安装目录找到，通常路径： C:\\Keil_v5\\ARM\\PACK\\ARM\\CMSIS\\4.5.0\\CMSIS\\DSP_Lib 或者从官网获取：CMSIS-DSP Library for Cortex-M, SC000, and SC300 Pack: ARM::CMSIS, http://www.keil.com/pack/ARM.CMSIS.4.5.0.pack\ndsp库内容 \r\r\r\rdsp库包含常用数学运算，复数，矩阵，三角函数，还有重要的fir滤波器和FFT，非常实用\n使用dsp库 1.1使用runtime environment 包管理器时引入 只需勾选dsp \r\r\r1.2不使用runtime environment 包管理器时引入 此时引入dsp lib 通常因为自己加入了cmX.h内核文件导致错误，因为runtime environment会自动处理依赖，添加内核，这时只需要将内核头文件的文件夹从include path 中移除即可 ###2.添加全局宏定义 添加内核定义：在此处添加 ARM_MATH_CM3 宏定义，其他内核按需修改可为CM0 ~ CM4 \r\r\r3.头文件 最后引入\n#include \u0026quot;arm_math.h\u0026quot;  便可以引用了\n文档与例程 帮助文件位于 C:\\Keil_v5\\ARM\\PACK\\ARM\\CMSIS\\4.5.0\\CMSIS\\Documentation\\RTX\\html\\index.html\n此文件夹Examples目录 C:\\Keil_v5\\ARM\\PACK\\ARM\\CMSIS\\4.5.0\\CMSIS\\DSP_Lib 中有大量官方例程可供参考\n","date":1512532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512532800,"objectID":"9cdbe9eb8afa0fd3b36e4fe8a16bb7bb","permalink":"https://leidawt.github.io/en/post/%E5%9C%A8stm32f1%E7%B3%BB%E5%88%97%E4%BD%BF%E7%94%A8dsp%E5%BA%93/","publishdate":"2017-12-06T12:00:00+08:00","relpermalink":"/en/post/%E5%9C%A8stm32f1%E7%B3%BB%E5%88%97%E4%BD%BF%E7%94%A8dsp%E5%BA%93/","section":"post","summary":"在stm32f1系列使用dsp库 ##获取dsp库 在keil mdk 版本中，dsp库集成与runtime environment之中，可以在keil安","tags":[],"title":"在stm32f1系列使用dsp库","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"这是一个用python脚本通过sftp快速上传文件夹到树莓派的脚本 可以直接拖拽，会自动上传到树莓派./Desktop/文件夹下 按需修改代码中地址和登录密码即可使用\n安装依赖 pip instal paramiko\nautoftp.py #! python3 # -*- coding:utf-8 -*- import paramiko import sys import os import os.path socks=('192.168.2.100',22)#此处写树莓派的地址 testssh=paramiko.Transport(socks) testssh.connect(username='pi',password='raspberry')#ssh账号密码 sftptest=paramiko.SFTPClient.from_transport(testssh) rootdir=sys.argv[1] rootnamelen=len(rootdir)-len(rootdir.split('\\\\')[-1]) if os.path.isdir(rootdir) and not rootdir[rootnamelen:] in sftptest.listdir(\u0026quot;./Desktop/\u0026quot;): tempdir=(\u0026quot;./Desktop/\u0026quot;+rootdir[rootnamelen:]).replace('\\\\','/') print(\u0026quot;mkdir: \u0026quot;,tempdir) sftptest.mkdir(tempdir) for parent,dirnames,filenames in os.walk(rootdir): for dirname in dirnames: print (\u0026quot;parent is:\u0026quot; + parent+ \u0026quot; dirname is:\u0026quot; + dirname) tempdir=(\u0026quot;./Desktop/\u0026quot;+parent[rootnamelen:]+'/'+dirname).replace('\\\\','/') print(\u0026quot;mkdir: \u0026quot;,tempdir) sftptest.mkdir(tempdir) for filename in filenames: print (\u0026quot;Upload File: parent is:\u0026quot; + parent +\u0026quot; filename is:\u0026quot; + filename) sftptest.put(parent+'\\\\'+filename,(\u0026quot;./Desktop/\u0026quot;+parent[rootnamelen:]+'/'+filename).replace('\\\\','/')) if not os.path.isdir(rootdir): sftptest.put(rootdir,\u0026quot;./Desktop/\u0026quot;+rootdir.split('\\\\')[-1]) print(\u0026quot;upload \u0026quot;,rootdir.split('\\\\')[-1],\u0026quot;succeed\u0026quot;) else: print(\u0026quot;dir is already exist\u0026quot;) sftptest.close() testssh.close() print(\u0026quot;done, sftp closed\u0026quot;)  下面是一个win 下的bat脚本实现拖拽功能 注意将目录换为autoftp.py所在目录\necho %1 E: cd E:XXX\\XXX py -3 autoftp.py %1 pause  然后直接将目标文件夹拖到这个bat上就可以实现自动上传了！ ","date":1512532800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512532800,"objectID":"8256f146787f01e1bf23367453545c8d","permalink":"https://leidawt.github.io/en/post/%E8%87%AA%E5%8A%A8%E4%B8%8A%E4%BC%A0%E5%88%B0%E6%A0%91%E8%8E%93%E6%B4%BE/","publishdate":"2017-12-06T12:00:00+08:00","relpermalink":"/en/post/%E8%87%AA%E5%8A%A8%E4%B8%8A%E4%BC%A0%E5%88%B0%E6%A0%91%E8%8E%93%E6%B4%BE/","section":"post","summary":"这是一个用python脚本通过sftp快速上传文件夹到树莓派的脚本 可以直接拖拽，会自动上传到树莓派./Desktop/文件夹下 按需修改代码中","tags":[],"title":"自动上传到树莓派","type":"post"},{"authors":["Xiaohan Wang"],"categories":[],"content":"动机 鉴于CSDN博客的自动化备份工具基本都挂了，为了简单的备份csdn的markdown博客，编写此小工具，主要用于下载markdown博客里的外链插图到本地，同时做一下链接替换的工作。 原理如下：\n 使用CSDN编辑器导出博文的markdown \r\r\r 将导出的markdown中的插图下载下来本地化，并进行url替换。 即将原文的 \r\r\r替换为需要的格式，比如hugo博客一般是如下格式： \r\r\r  实现 实现很简单，就是正则匹配替换并使用urlretrieve下载图片。核心python脚本如下： blog_transformer.py\n#! py -3 #!/usr/bin/env python3 import re from urllib.request import urlretrieve import argparse import os import codecs class BlogTransformer: def __init__(self, mode='hugo-academic'): \u0026quot;\u0026quot;\u0026quot;__init__ init class Args: mode (str, optional): REPLACE mode select. Defaults to 'hugo-academic'. \u0026quot;\u0026quot;\u0026quot; self.MD_FILE = '' self.SAVE_PATH = '' self.REPLACE_MODE = '' if mode == 'hugo-academic': # https://sourcethemes.com/academic/docs/writing-markdown-latex/#images # use format required by hugo # \r\r\rself.REPLACE_MODE = '{{\r\r\r\r}}' if mode == 'local-img-url': # use standard markdown format self.REPLACE_MODE = '[]({0})' def _pic_download(self, url, path, file_name): \u0026quot;\u0026quot;\u0026quot;_pic_download download picture Args: url (str): url path (str): save path for the picture file_name (str): name for the picture Returns: (save_name, file_type): - \u0026quot;\u0026quot;\u0026quot; file_type = '' if '.png' in url: file_type = '.png' elif '.jpg' in url: file_type = '.jpg' elif '.gif' in url: file_type = '.gif' else: # for some url without expicity file_type, use .png file_type = '.png' save_name = os.path.join(path, file_name+file_type) print('saving to {}'.format(save_name)) urlretrieve(url, save_name) return save_name, file_type def run(self, md_file, save_path='', save=True): \u0026quot;\u0026quot;\u0026quot;run download every pic url and modified the markdown Args: md_file (str): input markdown string save_path (str, optional): path to save the modified markdown, when set to default, use the same path as imput markdown file. Defaults to ''. save (bool, optional): Save the modified markdown?. Defaults to True. Raises: ValueError: raise when there are errors in matching pic url, i.e. meet invalid regex match Returns: str: modified markdown \u0026quot;\u0026quot;\u0026quot; self.MD_FILE = md_file if save_path == '': # use the same dir as md_file self.SAVE_PATH = os.path.dirname(os.path.abspath(self.MD_FILE)) else: self.SAVE_PATH = save_path # read markdown file into str try: with open(self.MD_FILE, 'r') as f: md = f.read() except: # for \u0026quot;utf-8 with dom\u0026quot; format with open(self.MD_FILE, 'r', encoding='utf-8-sig') as f: md = f.read() # findall all ![xxx](xxx) commands url_commands = re.findall(r\u0026quot;!\\[[\\s\\S]*?\\]\\(.+?\\)\u0026quot;, md) # download each img and change the ![xxx](xxx) commands to new REPLACE_MODE format for id, each in enumerate(url_commands): id = str(id) url = re.findall(r\u0026quot;!\\[[\\s\\S]*?\\]\\((.+?)\\)\u0026quot;, each) if url is not []: url = url[0] print(\u0026quot;Downloading: {}\u0026quot;.format(url)) else: raise ValueError('Err in matching url in {}'.format(each)) save_name, file_type = self._pic_download( url, self.SAVE_PATH, id) print('Done') replace_str = self.REPLACE_MODE.format(id+file_type) print('Replacing {} to {}'.format(each, replace_str)) md = md.replace(each, replace_str) # save the new markdown file print('#'*80) print('Saving modifieded markdown file into {}'.format( os.path.join(self.SAVE_PATH, '_'+self.MD_FILE))) if save: with codecs.open(os.path.join(self.SAVE_PATH, '_'+self.MD_FILE), \u0026quot;w\u0026quot;, \u0026quot;utf-8\u0026quot;) as f: f.write(md) print('Done!') return md if __name__ == \u0026quot;__main__\u0026quot;: parser = argparse.ArgumentParser() # init parser.add_argument('-f', help=\u0026quot;input markdown file\u0026quot;) parser.add_argument('-d', help=\u0026quot;path to save pictures\u0026quot;, default='') parser.add_argument('-m', help=\u0026quot;replace mode\u0026quot;, default='hugo-academic') args = vars(parser.parse_args()) print('MD_FILE = {}'.format(args['f'])) print('SAVE_PATH = {}'.format(args['d'])) print('REPLACE_MODE = {}'.format(args['m'])) print('#'*80) bt = BlogTransformer(args['m']) bt.run(args['f'], args['d'])  完整代码可见\rgithub\r安装使用 安装 git clone前述\r仓库\r，将路径加入环境变量以便bash或命令行能找到。\n使用 一般，执行下面命令即可，效果是在输入文件YorMarkdown.md的文件夹下导出替换过url的_YorMarkdown.md文件，并下载所有插图到本地，从0开始顺序编号。\nblog_transformer.py -f YorMarkdown.md  默认替换为hugo模式的url，亦可使用原生markdown格式：\nblog_transformer.py -f YorMarkdown.md -m local-img-url  另一个脚本pipline.py是个更进一步的自动化工具\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"47c5004f5f1761173cb470b50fa2688e","permalink":"https://leidawt.github.io/en/post/markdown%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%B0%8F%E5%B7%A5%E5%85%B7/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/post/markdown%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%B0%8F%E5%B7%A5%E5%85%B7/","section":"post","summary":"动机 鉴于CSDN博客的自动化备份工具基本都挂了，为了简单的备份csdn的markdown博客，编写此小工具，主要用于下载markdown博客","tags":[],"title":"markdown博客迁移小工具","type":"post"}]